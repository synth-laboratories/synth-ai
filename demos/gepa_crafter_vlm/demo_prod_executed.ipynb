{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {
    "papermill": {
     "duration": 0.003712,
     "end_time": "2025-12-30T19:30:11.761427",
     "exception": false,
     "start_time": "2025-12-30T19:30:11.757715",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Crafter VLM GEPA Demo\n",
    "\n",
    "This demo runs GEPA prompt optimization for a Crafter vision-language agent that uses image-only observations.\n",
    "\n",
    "**What this demo does:**\n",
    "1. Creates a local task app for the Crafter VLM agent\n",
    "2. Runs GEPA prompt optimization to find the best system prompt\n",
    "3. Extracts the optimized prompt from results\n",
    "4. Runs eval jobs comparing baseline vs optimized prompts\n",
    "5. Displays comparison results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:30:11.769164Z",
     "iopub.status.busy": "2025-12-30T19:30:11.768911Z",
     "iopub.status.idle": "2025-12-30T19:30:11.775661Z",
     "shell.execute_reply": "2025-12-30T19:30:11.774842Z"
    },
    "papermill": {
     "duration": 0.011517,
     "end_time": "2025-12-30T19:30:11.776797",
     "exception": false,
     "start_time": "2025-12-30T19:30:11.765280",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (can be overridden by papermill)\n",
    "BACKEND_URL = \"https://api.usesynth.ai\"  # Default to production\n",
    "API_KEY = None  # Will be set based on environment\n",
    "POLICY_MODEL = \"gpt-4.1-nano\"  # VLM model for the agent\n",
    "VERIFIER_MODEL = \"gpt-5-nano\"  # Model for verification (must be in allowed list)\n",
    "ROLLOUT_BUDGET = 30  # Total rollout budget\n",
    "NUM_GENERATIONS = 2  # Number of GEPA generations\n",
    "USE_TUNNEL = True  # Whether to use cloudflared tunnels (required for prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1999c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:30:11.783380Z",
     "iopub.status.busy": "2025-12-30T19:30:11.783123Z",
     "iopub.status.idle": "2025-12-30T19:30:11.786224Z",
     "shell.execute_reply": "2025-12-30T19:30:11.785566Z"
    },
    "papermill": {
     "duration": 0.007324,
     "end_time": "2025-12-30T19:30:11.786913",
     "exception": false,
     "start_time": "2025-12-30T19:30:11.779589",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BACKEND_URL = \"https://api.usesynth.ai\"\n",
    "POLICY_MODEL = \"gpt-4.1-nano\"\n",
    "VERIFIER_MODEL = \"gpt-5-nano\"\n",
    "ROLLOUT_BUDGET = 6\n",
    "NUM_GENERATIONS = 1\n",
    "USE_TUNNEL = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:30:11.792126Z",
     "iopub.status.busy": "2025-12-30T19:30:11.791931Z",
     "iopub.status.idle": "2025-12-30T19:30:13.101981Z",
     "shell.execute_reply": "2025-12-30T19:30:13.101641Z"
    },
    "papermill": {
     "duration": 1.313332,
     "end_time": "2025-12-30T19:30:13.102663",
     "exception": false,
     "start_time": "2025-12-30T19:30:11.789331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshpurtell/Documents/GitHub/synth-ai/synth_ai/sdk/task/contracts.py:34: UserWarning: Field name \"schema\" in \"StructuredOutputConfig\" shadows an attribute in parent \"BaseModel\"\n",
      "  class StructuredOutputConfig(BaseModel):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[celery_app] EXPERIMENT_QUEUE_DB_PATH not set, will use default path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[celery_app] Using default database path: /Users/joshpurtell/.synth_ai/experiment_queue.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[celery_app] Initializing with database: /Users/joshpurtell/.synth_ai/experiment_queue.db (broker: redis://localhost:6379/0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Imports and Setup\n",
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import httpx\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path('.').resolve().parent.parent))\n",
    "\n",
    "from synth_ai.sdk.api.eval import EvalJob, EvalJobConfig\n",
    "from synth_ai.sdk.api.train.prompt_learning import PromptLearningJob\n",
    "from synth_ai.sdk.learning.prompt_learning_client import PromptLearningClient\n",
    "from synth_ai.sdk.learning.rl import mint_environment_api_key, setup_environment_api_key\n",
    "from synth_ai.sdk.localapi import LocalAPIConfig, create_local_api\n",
    "from synth_ai.sdk.task import TaskInfo, run_server_background\n",
    "from synth_ai.sdk.task.contracts import RolloutMetrics, RolloutRequest, RolloutResponse\n",
    "from synth_ai.sdk.tunnels import wait_for_health_check\n",
    "from synth_ai.sdk.tunnels.tunneled_api import TunneledLocalAPI, TunnelBackend\n",
    "\n",
    "from crafter_logic import (\n",
    "    ACTION_STRING_TO_INT,\n",
    "    CRAFTER_ALLOWED_ACTIONS,\n",
    "    CrafterEnvironmentWrapper,\n",
    "    CrafterScorer,\n",
    "    CrafterVLMReActPolicy,\n",
    "    normalize_action_name,\n",
    ")\n",
    "\n",
    "print('Imports loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:30:13.105504Z",
     "iopub.status.busy": "2025-12-30T19:30:13.105320Z",
     "iopub.status.idle": "2025-12-30T19:30:13.107309Z",
     "shell.execute_reply": "2025-12-30T19:30:13.106947Z"
    },
    "papermill": {
     "duration": 0.00392,
     "end_time": "2025-12-30T19:30:13.107767",
     "exception": false,
     "start_time": "2025-12-30T19:30:13.103847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend: https://api.usesynth.ai\n",
      "Local API Ports: 8001, 8002\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Configuration\n",
    "\n",
    "SYNTH_API_BASE = 'https://api.usesynth.ai'\n",
    "LOCAL_API_PORT = 8001\n",
    "OPTIMIZED_LOCAL_API_PORT = 8002\n",
    "\n",
    "print(f'Backend: {SYNTH_API_BASE}')\n",
    "print(f'Local API Ports: {LOCAL_API_PORT}, {OPTIMIZED_LOCAL_API_PORT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:30:13.112996Z",
     "iopub.status.busy": "2025-12-30T19:30:13.112910Z",
     "iopub.status.idle": "2025-12-30T19:30:13.833900Z",
     "shell.execute_reply": "2025-12-30T19:30:13.833062Z"
    },
    "papermill": {
     "duration": 0.726284,
     "end_time": "2025-12-30T19:30:13.835070",
     "exception": false,
     "start_time": "2025-12-30T19:30:13.108786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using API Key: sk_live_ace8b968-a52...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend health: {'status': 'ok', 'database': 'connected', 'details': {}}\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Get API Key and Check Backend Health\n",
    "\n",
    "if API_KEY:\n",
    "    SYNTH_API_KEY = API_KEY\n",
    "else:\n",
    "    SYNTH_API_KEY = os.environ.get('SYNTH_API_KEY', '').strip()\n",
    "\n",
    "if not SYNTH_API_KEY:\n",
    "    raise RuntimeError('SYNTH_API_KEY not set. Please set it in environment or pass as parameter.')\n",
    "\n",
    "print(f'Using API Key: {SYNTH_API_KEY[:20]}...')\n",
    "\n",
    "# Check backend health\n",
    "r = httpx.get(f'{SYNTH_API_BASE}/health', timeout=30)\n",
    "if r.status_code == 200:\n",
    "    print(f'Backend health: {r.json()}')\n",
    "else:\n",
    "    raise RuntimeError(f'Backend not healthy: status {r.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:30:13.847359Z",
     "iopub.status.busy": "2025-12-30T19:30:13.847129Z",
     "iopub.status.idle": "2025-12-30T19:30:13.860758Z",
     "shell.execute_reply": "2025-12-30T19:30:13.860371Z"
    },
    "papermill": {
     "duration": 0.019346,
     "end_time": "2025-12-30T19:30:13.861143",
     "exception": false,
     "start_time": "2025-12-30T19:30:13.841797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local API factory defined\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Local API Factory\n",
    "\n",
    "APP_ID = \"crafter_vlm\"\n",
    "APP_NAME = \"Crafter VLM ReAct Agent\"\n",
    "TOOL_NAME = \"crafter_interact\"\n",
    "\n",
    "def create_crafter_vlm_local_api(system_prompt: str, env_api_key: str):\n",
    "    \"\"\"Factory to create a Crafter VLM task app with a specific system prompt.\"\"\"\n",
    "    # Import inside factory to ensure availability in closure\n",
    "    from crafter_logic import (\n",
    "        ACTION_STRING_TO_INT,\n",
    "        CRAFTER_ALLOWED_ACTIONS,\n",
    "        CrafterEnvironmentWrapper,\n",
    "        CrafterScorer,\n",
    "        CrafterVLMReActPolicy,\n",
    "        normalize_action_name,\n",
    "    )\n",
    "    \n",
    "    os.environ['ENVIRONMENT_API_KEY'] = env_api_key\n",
    "\n",
    "    async def run_rollout(request: RolloutRequest, fastapi_request) -> RolloutResponse:\n",
    "        policy_config = request.policy.config or {}\n",
    "        seed = request.env.seed or 0\n",
    "        env_config = request.env.config or {}\n",
    "        max_steps = int(env_config.get('max_steps_per_episode', 200))\n",
    "        max_turns = int(env_config.get('max_turns', 50))\n",
    "\n",
    "        env = CrafterEnvironmentWrapper(seed=seed, max_steps=max_steps)\n",
    "        observation = await env.reset()\n",
    "\n",
    "        policy = CrafterVLMReActPolicy(\n",
    "            system_prompt=system_prompt,\n",
    "            use_vision=True,\n",
    "            image_only_mode=True,\n",
    "        )\n",
    "\n",
    "        # Route OpenAI calls through Synth's inference proxy for trace reconstruction\n",
    "        inference_url = policy_config.get('inference_url', '')\n",
    "        if inference_url:\n",
    "            os.environ['OPENAI_BASE_URL'] = inference_url\n",
    "        \n",
    "        # Use policy_config api_key (from Synth proxy) or fall back to OPENAI_API_KEY env var\n",
    "        api_key = policy_config.get('api_key') or os.environ.get('OPENAI_API_KEY')\n",
    "        if not api_key:\n",
    "            raise ValueError(\"No API key available: policy_config['api_key'] and OPENAI_API_KEY env var are both empty\")\n",
    "        client = AsyncOpenAI(api_key=api_key)\n",
    "\n",
    "        history: List[Dict[str, Any]] = []\n",
    "        episode_rewards: List[float] = []\n",
    "\n",
    "        for turn in range(max_turns):\n",
    "            messages = policy.build_messages(observation, history)\n",
    "            \n",
    "            response = await client.chat.completions.create(\n",
    "                model=policy_config.get('model', POLICY_MODEL),\n",
    "                messages=messages,\n",
    "                tools=policy.tools,\n",
    "                tool_choice='required',\n",
    "                max_completion_tokens=policy_config.get('max_completion_tokens', 512),\n",
    "            )\n",
    "            \n",
    "            message = response.choices[0].message\n",
    "            response_text = message.content or ''\n",
    "            tool_calls = [\n",
    "                {'id': tc.id, 'type': 'function', 'function': {'name': tc.function.name, 'arguments': tc.function.arguments}}\n",
    "                for tc in (message.tool_calls or [])\n",
    "            ]\n",
    "\n",
    "            next_observation = observation\n",
    "            tool_responses: List[Dict[str, Any]] = []\n",
    "            \n",
    "            if tool_calls:\n",
    "                for tc in tool_calls:\n",
    "                    tool_call_id = tc['id']\n",
    "                    tool_name = tc['function']['name']\n",
    "                    actions_list: List[str] = []\n",
    "                    \n",
    "                    if tool_name == TOOL_NAME:\n",
    "                        try:\n",
    "                            args = json.loads(tc['function']['arguments'])\n",
    "                            raw_actions = args.get('actions_list', [])\n",
    "                            actions_list = [str(a) for a in raw_actions if str(a).strip()][:5]\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    \n",
    "                    if not actions_list:\n",
    "                        actions_list = ['noop']\n",
    "\n",
    "                    normalized_actions = []\n",
    "                    action_results = []\n",
    "\n",
    "                    for action_str in actions_list:\n",
    "                        normalized = normalize_action_name(action_str) or 'noop'\n",
    "                        normalized_actions.append(normalized)\n",
    "                        action = ACTION_STRING_TO_INT.get(normalized, 0)\n",
    "                        next_observation = await env.step(action)\n",
    "                        reward = next_observation.get('reward', 0.0)\n",
    "                        episode_rewards.append(float(reward))\n",
    "                        action_results.append({\n",
    "                            'action': normalized,\n",
    "                            'reward': reward,\n",
    "                            'terminated': next_observation.get('terminated'),\n",
    "                            'truncated': next_observation.get('truncated'),\n",
    "                        })\n",
    "                        if next_observation.get('terminated') or next_observation.get('truncated'):\n",
    "                            break\n",
    "\n",
    "                    tool_responses.append({'tool_call_id': tool_call_id, 'actions': normalized_actions, 'results': action_results})\n",
    "                    if next_observation.get('terminated') or next_observation.get('truncated'):\n",
    "                        break\n",
    "            else:\n",
    "                next_observation = await env.step(0)\n",
    "                episode_rewards.append(float(next_observation.get('reward', 0.0)))\n",
    "\n",
    "            history.append({'role': 'assistant', 'content': response_text, 'tool_calls': tool_calls})\n",
    "            for resp in tool_responses:\n",
    "                history.append({\n",
    "                    'role': 'tool',\n",
    "                    'tool_call_id': resp['tool_call_id'],\n",
    "                    'content': json.dumps({'actions': resp['actions'], 'results': resp['results']}),\n",
    "                })\n",
    "\n",
    "            observation = next_observation\n",
    "            if observation.get('terminated') or observation.get('truncated'):\n",
    "                break\n",
    "\n",
    "        score, details = CrafterScorer.score_episode(observation, len(episode_rewards), max_steps)\n",
    "\n",
    "        return RolloutResponse(\n",
    "            run_id=request.run_id,\n",
    "            metrics=RolloutMetrics(outcome_reward=score, details=details),\n",
    "            trace=None,  # Synth reconstructs from inference proxy\n",
    "            trace_correlation_id=policy_config.get('trace_correlation_id'),\n",
    "        )\n",
    "\n",
    "    def provide_taskset_description():\n",
    "        return {'splits': ['train', 'test']}\n",
    "\n",
    "    def provide_task_instances(seeds):\n",
    "        for seed in seeds:\n",
    "            yield TaskInfo(\n",
    "                task={'id': APP_ID, 'name': APP_NAME},\n",
    "                dataset={'id': APP_ID, 'split': 'train', 'index': seed},\n",
    "                inference={'tool': TOOL_NAME},\n",
    "                limits={'max_turns': 50},\n",
    "                task_metadata={'seed': seed},\n",
    "            )\n",
    "\n",
    "    return create_local_api(LocalAPIConfig(\n",
    "        app_id=APP_ID,\n",
    "        name=APP_NAME,\n",
    "        description=f'{APP_NAME} local API for VLM agent with image-only observations.',\n",
    "        provide_taskset_description=provide_taskset_description,\n",
    "        provide_task_instances=provide_task_instances,\n",
    "        rollout=run_rollout,\n",
    "        cors_origins=['*'],\n",
    "    ))\n",
    "\n",
    "\n",
    "print('Local API factory defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:30:13.863914Z",
     "iopub.status.busy": "2025-12-30T19:30:13.863829Z",
     "iopub.status.idle": "2025-12-30T19:30:13.866683Z",
     "shell.execute_reply": "2025-12-30T19:30:13.866366Z"
    },
    "papermill": {
     "duration": 0.004785,
     "end_time": "2025-12-30T19:30:13.867119",
     "exception": false,
     "start_time": "2025-12-30T19:30:13.862334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEPA job runner defined\n"
     ]
    }
   ],
   "source": [
    "# Step 6: GEPA Job Runner\n",
    "\n",
    "def run_gepa_job(\n",
    "    *,\n",
    "    api_key: str,\n",
    "    local_api_url: str,\n",
    "    local_api_key: str,\n",
    "    baseline_system_prompt: str,\n",
    "):\n",
    "    \"\"\"Run a GEPA prompt optimization job.\"\"\"\n",
    "    config_body = {\n",
    "        'prompt_learning': {\n",
    "            'algorithm': 'gepa',\n",
    "            'task_app_url': local_api_url,\n",
    "            'task_app_api_key': local_api_key,\n",
    "            'env_name': 'crafter',\n",
    "            'initial_prompt': {\n",
    "                'messages': [{'role': 'system', 'order': 0, 'pattern': baseline_system_prompt}],\n",
    "                'wildcards': {},\n",
    "            },\n",
    "            'policy': {\n",
    "                'inference_mode': 'synth_hosted',\n",
    "                'model': POLICY_MODEL,\n",
    "                'provider': 'openai',\n",
    "                'temperature': 0.0,\n",
    "                'max_completion_tokens': 512,\n",
    "            },\n",
    "            'gepa': {\n",
    "                'env_name': 'crafter',\n",
    "                'evaluation': {'seeds': list(range(30)), 'validation_seeds': list(range(50, 56))},\n",
    "                'rollout': {'budget': ROLLOUT_BUDGET, 'max_concurrent': 3, 'minibatch_size': 3},\n",
    "                'mutation': {'rate': 0.3},\n",
    "                'population': {'initial_size': 3, 'num_generations': NUM_GENERATIONS, 'children_per_generation': 2},\n",
    "                'archive': {'size': 5, 'pareto_set_size': 10},\n",
    "                'token': {'max_limit': 4000, 'counting_model': 'gpt-4', 'max_spend_usd': 50.0},\n",
    "            },\n",
    "            'env': {\n",
    "                'max_turns': 20,  # Cap VLM calls per rollout\n",
    "                'max_steps_per_episode': 200,\n",
    "            },\n",
    "            'verifier': {\n",
    "                'enabled': False,\n",
    "                'reward_source': 'task_app',\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    job = PromptLearningJob.from_dict(\n",
    "        config_dict=config_body,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        api_key=api_key,\n",
    "        task_app_api_key=local_api_key,\n",
    "        skip_health_check=True,\n",
    "    )\n",
    "    job_id = job.submit()\n",
    "    print(f'GEPA job created: {job_id}')\n",
    "    \n",
    "    result = job.poll_until_complete(timeout=3600.0, interval=3.0, progress=True)\n",
    "    print(f'GEPA job finished: {result.status.value}')\n",
    "    return result\n",
    "\n",
    "\n",
    "print('GEPA job runner defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:30:13.869744Z",
     "iopub.status.busy": "2025-12-30T19:30:13.869669Z",
     "iopub.status.idle": "2025-12-30T19:30:13.871987Z",
     "shell.execute_reply": "2025-12-30T19:30:13.871643Z"
    },
    "papermill": {
     "duration": 0.004097,
     "end_time": "2025-12-30T19:30:13.872378",
     "exception": false,
     "start_time": "2025-12-30T19:30:13.868281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval job runner defined\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Eval Job Runner\n",
    "\n",
    "EVAL_MODEL = \"gpt-4o-mini\"  # Use real OpenAI model for eval (eval doesn't support synth_hosted)\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', '')  # Get OpenAI key for eval jobs\n",
    "\n",
    "def run_eval_job(*, local_api_url: str, local_api_key: str, seeds: list[int], mode: str):\n",
    "    \"\"\"Run an eval job and wait for completion.\"\"\"\n",
    "    config = EvalJobConfig(\n",
    "        task_app_url=local_api_url,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        api_key=SYNTH_API_KEY,\n",
    "        task_app_api_key=local_api_key,\n",
    "        env_name='crafter',\n",
    "        seeds=seeds,\n",
    "        policy_config={\n",
    "            'model': EVAL_MODEL,\n",
    "            'provider': 'openai',\n",
    "            'api_key': OPENAI_API_KEY,  # Pass OpenAI key to task app\n",
    "        },\n",
    "        env_config={\n",
    "            'max_steps_per_episode': 200,\n",
    "            'max_turns': 20,\n",
    "        },\n",
    "        concurrency=5,\n",
    "    )\n",
    "    job = EvalJob(config)\n",
    "    job_id = job.submit()\n",
    "    print(f'  {mode} eval job: {job_id}')\n",
    "    return job.poll_until_complete(timeout=600.0, interval=2.0, progress=True)\n",
    "\n",
    "\n",
    "print('Eval job runner defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:30:13.874894Z",
     "iopub.status.busy": "2025-12-30T19:30:13.874817Z",
     "iopub.status.idle": "2025-12-30T19:30:15.122667Z",
     "shell.execute_reply": "2025-12-30T19:30:15.121946Z"
    },
    "papermill": {
     "duration": 1.250525,
     "end_time": "2025-12-30T19:30:15.123989",
     "exception": false,
     "start_time": "2025-12-30T19:30:13.873464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minted: b8ab02fef61f...9565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[env-keys] public_key: b64_len=44 sha256=e173cb5664e240ba1b0d97a36000b0c4c15f42506a227a82cd2b2c00be119c57 head=LYqK7q2klATZWxRq tail=bgj7pMsckDLn1j8=\n",
      "[env-keys] plaintext: len=64 preview=b8ab02……9565 has_ws=False\n",
      "[env-keys] ciphertext: b64_len=152 sha256=d671581b1db8ee7e4b1b2dc4dbafed30364e5aa9e08f91f5d788ae76520c8223 head=X+wZu5nAVKraPPJw tail=8XNPObZedsKSsg==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment API key uploaded\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Setup Environment API Key\n",
    "\n",
    "environment_api_key = mint_environment_api_key()\n",
    "os.environ['ENVIRONMENT_API_KEY'] = environment_api_key\n",
    "print(f'Minted: {environment_api_key[:12]}...{environment_api_key[-4:]}')\n",
    "\n",
    "try:\n",
    "    setup_environment_api_key(SYNTH_API_BASE, SYNTH_API_KEY, token=environment_api_key)\n",
    "    print('Environment API key uploaded')\n",
    "except Exception as exc:\n",
    "    print(f'Warning: failed to upload ENVIRONMENT_API_KEY: {exc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:30:15.133241Z",
     "iopub.status.busy": "2025-12-30T19:30:15.132874Z",
     "iopub.status.idle": "2025-12-30T19:30:15.138501Z",
     "shell.execute_reply": "2025-12-30T19:30:15.137810Z"
    },
    "papermill": {
     "duration": 0.011425,
     "end_time": "2025-12-30T19:30:15.139827",
     "exception": false,
     "start_time": "2025-12-30T19:30:15.128402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline prompt:\n",
      "You are an agent playing Crafter, a survival crafting game. Your goal is to survive and unlock achievements by exploring, crafting, and building. You can see the game state through images. Analyze eac...\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Define Baseline Prompt\n",
    "\n",
    "allowed_actions = ', '.join(CRAFTER_ALLOWED_ACTIONS)\n",
    "baseline_prompt = (\n",
    "    'You are an agent playing Crafter, a survival crafting game. '\n",
    "    'Your goal is to survive and unlock achievements by exploring, crafting, and building. '\n",
    "    'You can see the game state through images. Analyze each image carefully to understand '\n",
    "    'your surroundings, inventory, health, and available resources. '\n",
    "    'Use the crafter_interact tool to execute actions. '\n",
    "    \"Key mechanics: use 'do' only when adjacent to a resource (tree, stone, cow, plant); \"\n",
    "    'it does nothing on grass or water. '\n",
    "    'Craft progression: wood -> table -> wood_pickaxe -> stone -> stone_pickaxe -> iron tools. '\n",
    "    'Sleep when energy is low to restore and unlock wake_up. '\n",
    "    f'Available actions: {allowed_actions}. '\n",
    "    'Only use these action names and return 2-5 actions per decision. '\n",
    "    'Strategy: move toward trees to collect wood; place a table once you have wood; '\n",
    "    'craft a wood pickaxe, then collect stone and craft a stone pickaxe; '\n",
    "    'progress toward iron tools and combat when safe.'\n",
    ")\n",
    "\n",
    "print('Baseline prompt:')\n",
    "print(baseline_prompt[:200] + '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-12-30T19:30:15.143316",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 10: Start Baseline Local API\n",
    "\n",
    "baseline_app = create_crafter_vlm_local_api(baseline_prompt, environment_api_key)\n",
    "run_server_background(baseline_app, port=LOCAL_API_PORT)\n",
    "await wait_for_health_check('127.0.0.1', LOCAL_API_PORT, environment_api_key, timeout=60.0)\n",
    "\n",
    "if USE_TUNNEL:\n",
    "    # Create tunnel to expose local API to the internet\n",
    "    print(f'Creating tunnel for port {LOCAL_API_PORT}...')\n",
    "    baseline_tunnel = await TunneledLocalAPI.create(\n",
    "        local_port=LOCAL_API_PORT,\n",
    "        backend=TunnelBackend.CloudflareManagedTunnel,\n",
    "        api_key=SYNTH_API_KEY,\n",
    "        env_api_key=environment_api_key,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        progress=True,\n",
    "    )\n",
    "    BASELINE_LOCAL_API_URL = baseline_tunnel.url\n",
    "else:\n",
    "    BASELINE_LOCAL_API_URL = f'http://localhost:{LOCAL_API_PORT}'\n",
    "\n",
    "print(f'Baseline local API URL: {BASELINE_LOCAL_API_URL}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 11: Run GEPA Optimization\n",
    "\n",
    "print('Starting GEPA optimization...')\n",
    "print(f'  Rollout budget: {ROLLOUT_BUDGET}')\n",
    "print(f'  Generations: {NUM_GENERATIONS}')\n",
    "\n",
    "job_result = run_gepa_job(\n",
    "    api_key=SYNTH_API_KEY,\n",
    "    local_api_url=BASELINE_LOCAL_API_URL,\n",
    "    local_api_key=environment_api_key,\n",
    "    baseline_system_prompt=baseline_prompt,\n",
    ")\n",
    "\n",
    "print(f'\\nGEPA Status: {job_result.status.value}')\n",
    "if job_result.succeeded:\n",
    "    print('GEPA optimization succeeded!')\n",
    "else:\n",
    "    print(f'GEPA failed: {job_result.error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 12: Extract Optimized Prompt\n",
    "\n",
    "def extract_system_prompt(best_prompt: Optional[Dict[str, Any]]) -> Optional[str]:\n",
    "    \"\"\"Extract system prompt from prompt learning results.\"\"\"\n",
    "    if not best_prompt:\n",
    "        return None\n",
    "    for msg in best_prompt.get('messages', []):\n",
    "        if msg.get('role') == 'system':\n",
    "            return msg.get('pattern') or msg.get('content')\n",
    "    for sec in best_prompt.get('sections', []):\n",
    "        if sec.get('role') == 'system':\n",
    "            return sec.get('content')\n",
    "    return None\n",
    "\n",
    "optimized_prompt = None\n",
    "\n",
    "if job_result.succeeded:\n",
    "    pl_client = PromptLearningClient(SYNTH_API_BASE, SYNTH_API_KEY)\n",
    "    prompt_results = await pl_client.get_prompts(job_result.job_id)\n",
    "    optimized_prompt = extract_system_prompt(prompt_results.best_prompt)\n",
    "    \n",
    "    if optimized_prompt:\n",
    "        print('=' * 60)\n",
    "        print('OPTIMIZED PROMPT')\n",
    "        print('=' * 60)\n",
    "        print(optimized_prompt[:800] + '...' if len(optimized_prompt) > 800 else optimized_prompt)\n",
    "        print('=' * 60)\n",
    "        \n",
    "        # Save to results directory\n",
    "        results_dir = Path('results')\n",
    "        results_dir.mkdir(exist_ok=True)\n",
    "        with open(results_dir / 'optimized_prompt.txt', 'w') as f:\n",
    "            f.write(optimized_prompt)\n",
    "        print(f'\\nSaved optimized prompt to: {results_dir / \"optimized_prompt.txt\"}')\n",
    "    else:\n",
    "        print('Failed to extract optimized prompt from results')\n",
    "else:\n",
    "    print('Skipping prompt extraction (GEPA did not succeed)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 13: Run Evaluation (Baseline vs Optimized)\n",
    "\n",
    "EVAL_SEEDS = list(range(100, 120))  # 20 held-out test samples\n",
    "\n",
    "if optimized_prompt:\n",
    "    # Start optimized local API\n",
    "    optimized_app = create_crafter_vlm_local_api(optimized_prompt, environment_api_key)\n",
    "    run_server_background(optimized_app, port=OPTIMIZED_LOCAL_API_PORT)\n",
    "    await wait_for_health_check('127.0.0.1', OPTIMIZED_LOCAL_API_PORT, environment_api_key, timeout=60.0)\n",
    "    \n",
    "    if USE_TUNNEL:\n",
    "        # Create tunnel for optimized API\n",
    "        print(f'Creating tunnel for port {OPTIMIZED_LOCAL_API_PORT}...')\n",
    "        optimized_tunnel = await TunneledLocalAPI.create(\n",
    "            local_port=OPTIMIZED_LOCAL_API_PORT,\n",
    "            backend=TunnelBackend.CloudflareManagedTunnel,\n",
    "            api_key=SYNTH_API_KEY,\n",
    "            env_api_key=environment_api_key,\n",
    "            backend_url=SYNTH_API_BASE,\n",
    "            progress=True,\n",
    "        )\n",
    "        OPTIMIZED_LOCAL_API_URL = optimized_tunnel.url\n",
    "    else:\n",
    "        OPTIMIZED_LOCAL_API_URL = f'http://localhost:{OPTIMIZED_LOCAL_API_PORT}'\n",
    "    \n",
    "    print(f'Optimized local API URL: {OPTIMIZED_LOCAL_API_URL}')\n",
    "    print(f'\\nRunning evaluation on {len(EVAL_SEEDS)} seeds...')\n",
    "\n",
    "    # Run baseline eval\n",
    "    print('\\nRunning BASELINE eval...')\n",
    "    baseline_eval = run_eval_job(\n",
    "        local_api_url=BASELINE_LOCAL_API_URL,\n",
    "        local_api_key=environment_api_key,\n",
    "        seeds=EVAL_SEEDS,\n",
    "        mode='baseline',\n",
    "    )\n",
    "\n",
    "    # Run optimized eval\n",
    "    print('\\nRunning OPTIMIZED eval...')\n",
    "    optimized_eval = run_eval_job(\n",
    "        local_api_url=OPTIMIZED_LOCAL_API_URL,\n",
    "        local_api_key=environment_api_key,\n",
    "        seeds=EVAL_SEEDS,\n",
    "        mode='optimized',\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print('\\n' + '=' * 60)\n",
    "    print('EVALUATION RESULTS')\n",
    "    print('=' * 60)\n",
    "    print(f'Baseline: {baseline_eval.raw}')\n",
    "    print(f'Optimized: {optimized_eval.raw}')\n",
    "    \n",
    "    # Save results\n",
    "    results_dir = Path('results')\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    with open(results_dir / 'eval_results.json', 'w') as f:\n",
    "        json.dump({'baseline': baseline_eval.raw, 'optimized': optimized_eval.raw}, f, indent=2)\n",
    "    print(f'\\nSaved eval results to: {results_dir / \"eval_results.json\"}')\n",
    "else:\n",
    "    print('Skipping evaluation (no optimized prompt)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 14: Done\n",
    "print('Demo complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "/Users/joshpurtell/Documents/GitHub/synth-ai/demos/gepa_crafter_vlm/demo_prod.ipynb",
   "output_path": "/Users/joshpurtell/Documents/GitHub/synth-ai/demos/gepa_crafter_vlm/demo_prod_executed.ipynb",
   "parameters": {
    "BACKEND_URL": "https://api.usesynth.ai",
    "VERIFIER_MODEL": "gpt-5-nano",
    "NUM_GENERATIONS": 1,
    "POLICY_MODEL": "gpt-4.1-nano",
    "ROLLOUT_BUDGET": 6,
    "USE_TUNNEL": true
   },
   "start_time": "2025-12-30T19:30:11.136568",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}