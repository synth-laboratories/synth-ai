{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Crafter VLM GEPA Demo\n",
    "\n",
    "This demo runs GEPA prompt optimization for a Crafter vision-language agent that uses image-only observations.\n",
    "\n",
    "**What this demo does:**\n",
    "1. Creates a local task app for the Crafter VLM agent\n",
    "2. Runs GEPA prompt optimization to find the best system prompt\n",
    "3. Extracts the optimized prompt from results\n",
    "4. Runs eval jobs comparing baseline vs optimized prompts\n",
    "5. Displays comparison results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (can be overridden by papermill)\n",
    "API_KEY = None  # Will be set based on environment\n",
    "POLICY_MODEL = \"gpt-4.1-nano\"  # VLM model for the agent\n",
    "VERIFIER_MODEL = \"gpt-5-nano\"  # Model for verification (must be in allowed list)\n",
    "ROLLOUT_BUDGET = 30  # Total rollout budget\n",
    "NUM_GENERATIONS = 2  # Number of GEPA generations\n",
    "USE_TUNNEL = True  # Whether to use cloudflared tunnels (required for prod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Imports and Setup\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import httpx\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "from synth_ai.core.urls import BACKEND_URL_BASE, backend_health_url, backend_me_url\n",
    "from synth_ai.sdk.auth import get_or_mint_synth_api_key\n",
    "from synth_ai.sdk.api.eval import EvalJob, EvalJobConfig\n",
    "from synth_ai.sdk.api.train.prompt_learning import PromptLearningJob\n",
    "from synth_ai.sdk.learning.prompt_learning_client import PromptLearningClient\n",
    "from synth_ai.sdk.learning.rl import mint_environment_api_key, setup_environment_api_key\n",
    "from synth_ai.sdk.localapi import LocalAPIConfig, create_local_api\n",
    "from synth_ai.sdk.task import TaskInfo, run_server_background\n",
    "from synth_ai.sdk.task.contracts import RolloutMetrics, RolloutRequest, RolloutResponse\n",
    "from synth_ai.sdk.tunnels import wait_for_health_check\n",
    "from synth_ai.sdk.tunnels.tunneled_api import TunneledLocalAPI, TunnelBackend\n",
    "\n",
    "from crafter_logic import (\n",
    "    ACTION_STRING_TO_INT,\n",
    "    CRAFTER_ALLOWED_ACTIONS,\n",
    "    CrafterEnvironmentWrapper,\n",
    "    CrafterScorer,\n",
    "    CrafterVLMReActPolicy,\n",
    "    normalize_action_name,\n",
    ")\n",
    "\n",
    "print(\"Imports loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Configuration\n",
    "\n",
    "SYNTH_API_BASE = BACKEND_URL_BASE\n",
    "LOCAL_API_PORT = 8001\n",
    "OPTIMIZED_LOCAL_API_PORT = 8002\n",
    "\n",
    "print(f\"Backend: {SYNTH_API_BASE}\")\n",
    "print(f\"Local API Ports: {LOCAL_API_PORT}, {OPTIMIZED_LOCAL_API_PORT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Get API Key and Check Backend Health\n",
    "\n",
    "def _validate_api_key(api_key: str) -> bool:\n",
    "    if not api_key:\n",
    "        return False\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    try:\n",
    "        resp = httpx.get(backend_me_url(SYNTH_API_BASE), headers=headers, timeout=10)\n",
    "    except Exception:\n",
    "        return False\n",
    "    return resp.status_code == 200\n",
    "\n",
    "if API_KEY:\n",
    "    SYNTH_API_KEY = API_KEY\n",
    "else:\n",
    "    SYNTH_API_KEY = get_or_mint_synth_api_key(\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        validator=_validate_api_key,\n",
    "    )\n",
    "\n",
    "if not SYNTH_API_KEY:\n",
    "    raise RuntimeError(\"SYNTH_API_KEY not set. Please set it in environment or pass as parameter.\")\n",
    "\n",
    "os.environ[\"SYNTH_API_KEY\"] = SYNTH_API_KEY\n",
    "print(f\"Using API Key: {SYNTH_API_KEY[:20]}...\")\n",
    "\n",
    "# Check backend health\n",
    "r = httpx.get(backend_health_url(SYNTH_API_BASE), timeout=30)\n",
    "if r.status_code == 200:\n",
    "    print(f\"Backend health: {r.json()}\")\n",
    "else:\n",
    "    raise RuntimeError(f\"Backend not healthy: status {r.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Local API Factory\n",
    "\n",
    "APP_ID = \"crafter_vlm\"\n",
    "APP_NAME = \"Crafter VLM ReAct Agent\"\n",
    "TOOL_NAME = \"crafter_interact\"\n",
    "\n",
    "\n",
    "def create_crafter_vlm_local_api(system_prompt: str):\n",
    "    \"\"\"Factory to create a Crafter VLM task app with a specific system prompt.\"\"\"\n",
    "    # Import inside factory to ensure availability in closure\n",
    "    from crafter_logic import (\n",
    "        ACTION_STRING_TO_INT,\n",
    "        CRAFTER_ALLOWED_ACTIONS,\n",
    "        CrafterEnvironmentWrapper,\n",
    "        CrafterScorer,\n",
    "        CrafterVLMReActPolicy,\n",
    "        normalize_action_name,\n",
    "    )\n",
    "\n",
    "    async def run_rollout(request: RolloutRequest, fastapi_request) -> RolloutResponse:\n",
    "        policy_config = request.policy.config or {}\n",
    "        seed = request.env.seed or 0\n",
    "        env_config = request.env.config or {}\n",
    "        max_steps = int(env_config.get(\"max_steps_per_episode\", 200))\n",
    "        max_turns = int(env_config.get(\"max_turns\", 50))\n",
    "\n",
    "        env = CrafterEnvironmentWrapper(seed=seed, max_steps=max_steps)\n",
    "        observation = await env.reset()\n",
    "\n",
    "        policy = CrafterVLMReActPolicy(\n",
    "            system_prompt=system_prompt,\n",
    "            use_vision=True,\n",
    "            image_only_mode=True,\n",
    "        )\n",
    "\n",
    "        # Route OpenAI calls through Synth's inference proxy for trace reconstruction\n",
    "        inference_url = policy_config.get(\"inference_url\", \"\")\n",
    "        if inference_url:\n",
    "            os.environ[\"OPENAI_BASE_URL\"] = inference_url\n",
    "\n",
    "        # Use policy_config api_key (from Synth proxy) or fall back to OPENAI_API_KEY env var\n",
    "        api_key = policy_config.get(\"api_key\") or os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\n",
    "                \"No API key available: policy_config['api_key'] and OPENAI_API_KEY env var are both empty\"\n",
    "            )\n",
    "        client = AsyncOpenAI(api_key=api_key)\n",
    "\n",
    "        history: List[Dict[str, Any]] = []\n",
    "        episode_rewards: List[float] = []\n",
    "\n",
    "        for turn in range(max_turns):\n",
    "            messages = policy.build_messages(observation, history)\n",
    "\n",
    "            response = await client.chat.completions.create(\n",
    "                model=policy_config.get(\"model\", POLICY_MODEL),\n",
    "                messages=messages,\n",
    "                tools=policy.tools,\n",
    "                tool_choice=\"required\",\n",
    "                max_completion_tokens=policy_config.get(\"max_completion_tokens\", 512),\n",
    "            )\n",
    "\n",
    "            message = response.choices[0].message\n",
    "            response_text = message.content or \"\"\n",
    "            tool_calls = [\n",
    "                {\n",
    "                    \"id\": tc.id,\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\"name\": tc.function.name, \"arguments\": tc.function.arguments},\n",
    "                }\n",
    "                for tc in (message.tool_calls or [])\n",
    "            ]\n",
    "\n",
    "            next_observation = observation\n",
    "            tool_responses: List[Dict[str, Any]] = []\n",
    "\n",
    "            if tool_calls:\n",
    "                for tc in tool_calls:\n",
    "                    tool_call_id = tc[\"id\"]\n",
    "                    tool_name = tc[\"function\"][\"name\"]\n",
    "                    actions_list: List[str] = []\n",
    "\n",
    "                    if tool_name == TOOL_NAME:\n",
    "                        try:\n",
    "                            args = json.loads(tc[\"function\"][\"arguments\"])\n",
    "                            raw_actions = args.get(\"actions_list\", [])\n",
    "                            actions_list = [str(a) for a in raw_actions if str(a).strip()][:5]\n",
    "                        except Exception:\n",
    "                            pass\n",
    "\n",
    "                    if not actions_list:\n",
    "                        actions_list = [\"noop\"]\n",
    "\n",
    "                    normalized_actions = []\n",
    "                    action_results = []\n",
    "\n",
    "                    for action_str in actions_list:\n",
    "                        normalized = normalize_action_name(action_str) or \"noop\"\n",
    "                        normalized_actions.append(normalized)\n",
    "                        action = ACTION_STRING_TO_INT.get(normalized, 0)\n",
    "                        next_observation = await env.step(action)\n",
    "                        reward = next_observation.get(\"reward\", 0.0)\n",
    "                        episode_rewards.append(float(reward))\n",
    "                        action_results.append(\n",
    "                            {\n",
    "                                \"action\": normalized,\n",
    "                                \"reward\": reward,\n",
    "                                \"terminated\": next_observation.get(\"terminated\"),\n",
    "                                \"truncated\": next_observation.get(\"truncated\"),\n",
    "                            }\n",
    "                        )\n",
    "                        if next_observation.get(\"terminated\") or next_observation.get(\"truncated\"):\n",
    "                            break\n",
    "\n",
    "                    tool_responses.append(\n",
    "                        {\n",
    "                            \"tool_call_id\": tool_call_id,\n",
    "                            \"actions\": normalized_actions,\n",
    "                            \"results\": action_results,\n",
    "                        }\n",
    "                    )\n",
    "                    if next_observation.get(\"terminated\") or next_observation.get(\"truncated\"):\n",
    "                        break\n",
    "            else:\n",
    "                next_observation = await env.step(0)\n",
    "                episode_rewards.append(float(next_observation.get(\"reward\", 0.0)))\n",
    "\n",
    "            history.append(\n",
    "                {\"role\": \"assistant\", \"content\": response_text, \"tool_calls\": tool_calls}\n",
    "            )\n",
    "            for resp in tool_responses:\n",
    "                history.append(\n",
    "                    {\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": resp[\"tool_call_id\"],\n",
    "                        \"content\": json.dumps(\n",
    "                            {\"actions\": resp[\"actions\"], \"results\": resp[\"results\"]}\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            observation = next_observation\n",
    "            if observation.get(\"terminated\") or observation.get(\"truncated\"):\n",
    "                break\n",
    "\n",
    "        score, details = CrafterScorer.score_episode(observation, len(episode_rewards), max_steps)\n",
    "\n",
    "        return RolloutResponse(\n",
    "            run_id=request.run_id,\n",
    "            metrics=RolloutMetrics(outcome_reward=score, details=details),\n",
    "            trace=None,  # Synth reconstructs from inference proxy\n",
    "            trace_correlation_id=policy_config.get(\"trace_correlation_id\"),\n",
    "        )\n",
    "\n",
    "    def provide_taskset_description():\n",
    "        return {\"splits\": [\"train\", \"test\"]}\n",
    "\n",
    "    def provide_task_instances(seeds):\n",
    "        for seed in seeds:\n",
    "            yield TaskInfo(\n",
    "                task={\"id\": APP_ID, \"name\": APP_NAME},\n",
    "                dataset={\"id\": APP_ID, \"split\": \"train\", \"index\": seed},\n",
    "                inference={\"tool\": TOOL_NAME},\n",
    "                limits={\"max_turns\": 50},\n",
    "                task_metadata={\"seed\": seed},\n",
    "            )\n",
    "\n",
    "    return create_local_api(\n",
    "        LocalAPIConfig(\n",
    "            app_id=APP_ID,\n",
    "            name=APP_NAME,\n",
    "            description=f\"{APP_NAME} local API for VLM agent with image-only observations.\",\n",
    "            provide_taskset_description=provide_taskset_description,\n",
    "            provide_task_instances=provide_task_instances,\n",
    "            rollout=run_rollout,\n",
    "            cors_origins=[\"*\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Local API factory defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: GEPA Job Runner\n",
    "\n",
    "\n",
    "def run_gepa_job(\n",
    "    *,\n",
    "    api_key: str,\n",
    "    local_api_url: str,\n",
    "    local_api_key: str,\n",
    "    baseline_system_prompt: str,\n",
    "):\n",
    "    \"\"\"Run a GEPA prompt optimization job.\"\"\"\n",
    "    config_body = {\n",
    "        \"prompt_learning\": {\n",
    "            \"algorithm\": \"gepa\",\n",
    "            \"task_app_url\": local_api_url,\n",
    "            \"env_name\": \"crafter\",\n",
    "            \"initial_prompt\": {\n",
    "                \"messages\": [{\"role\": \"system\", \"order\": 0, \"pattern\": baseline_system_prompt}],\n",
    "                \"wildcards\": {},\n",
    "            },\n",
    "            \"policy\": {\n",
    "                \"inference_mode\": \"synth_hosted\",\n",
    "                \"model\": POLICY_MODEL,\n",
    "                \"provider\": \"openai\",\n",
    "                \"temperature\": 0.0,\n",
    "                \"max_completion_tokens\": 512,\n",
    "            },\n",
    "            \"gepa\": {\n",
    "                \"env_name\": \"crafter\",\n",
    "                \"evaluation\": {\"seeds\": list(range(30)), \"validation_seeds\": list(range(50, 56))},\n",
    "                \"rollout\": {\"budget\": ROLLOUT_BUDGET, \"max_concurrent\": 3, \"minibatch_size\": 3},\n",
    "                \"mutation\": {\"rate\": 0.3},\n",
    "                \"population\": {\n",
    "                    \"initial_size\": 3,\n",
    "                    \"num_generations\": NUM_GENERATIONS,\n",
    "                    \"children_per_generation\": 2,\n",
    "                },\n",
    "                \"archive\": {\"size\": 5, \"pareto_set_size\": 10},\n",
    "                \"token\": {\"max_limit\": 4000, \"counting_model\": \"gpt-4\", \"max_spend_usd\": 50.0},\n",
    "            },\n",
    "            \"env\": {\n",
    "                \"max_turns\": 20,  # Cap VLM calls per rollout\n",
    "                \"max_steps_per_episode\": 200,\n",
    "            },\n",
    "            \"verifier\": {\n",
    "                \"enabled\": False,\n",
    "                \"reward_source\": \"task_app\",\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    job = PromptLearningJob.from_dict(\n",
    "        config_dict=config_body,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        api_key=api_key,\n",
    "        skip_health_check=True,\n",
    "    )\n",
    "    job_id = job.submit()\n",
    "    print(f\"GEPA job created: {job_id}\")\n",
    "\n",
    "    result = job.poll_until_complete(timeout=3600.0, interval=3.0, progress=True)\n",
    "    print(f\"GEPA job finished: {result.status.value}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"GEPA job runner defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Eval Job Runner\n",
    "\n",
    "EVAL_MODEL = \"gpt-4o-mini\"  # Use real OpenAI model for eval (eval doesn't support synth_hosted)\n",
    "\n",
    "\n",
    "def run_eval_job(*, local_api_url: str, seeds: list[int], mode: str):\n",
    "    \"\"\"Run an eval job and wait for completion.\"\"\"\n",
    "    config = EvalJobConfig(\n",
    "        task_app_url=local_api_url,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        api_key=SYNTH_API_KEY,\n",
    "        env_name=\"crafter\",\n",
    "        seeds=seeds,\n",
    "        policy_config={\n",
    "            \"model\": EVAL_MODEL,\n",
    "            \"provider\": \"openai\",\n",
    "            \"api_key\": OPENAI_API_KEY,  # Pass OpenAI key to task app\n",
    "        },\n",
    "        env_config={\n",
    "            \"max_steps_per_episode\": 200,\n",
    "            \"max_turns\": 20,\n",
    "        },\n",
    "        concurrency=5,\n",
    "    )\n",
    "    job = EvalJob(config)\n",
    "    job_id = job.submit()\n",
    "    print(f\"  {mode} eval job: {job_id}\")\n",
    "    return job.poll_until_complete(timeout=600.0, interval=2.0, progress=True)\n",
    "\n",
    "\n",
    "print(\"Eval job runner defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENT_API_KEY = mint_environment_api_key()\n",
    "print(f\"Env key ready: {ENVIRONMENT_API_KEY[:12]}...{ENVIRONMENT_API_KEY[-4:]}\")\n",
    "try:\n",
    "    result = setup_environment_api_key(SYNTH_API_BASE, SYNTH_API_KEY, token=ENVIRONMENT_API_KEY)\n",
    "    print(f\"Uploaded env key: {result}\")\n",
    "except Exception as exc:\n",
    "    print(f\"Env key upload failed (continuing locally): {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Define Baseline Prompt\n",
    "\n",
    "allowed_actions = \", \".join(CRAFTER_ALLOWED_ACTIONS)\n",
    "baseline_prompt = (\n",
    "    \"You are an agent playing Crafter, a survival crafting game. \"\n",
    "    \"Your goal is to survive and unlock achievements by exploring, crafting, and building. \"\n",
    "    \"You can see the game state through images. Analyze each image carefully to understand \"\n",
    "    \"your surroundings, inventory, health, and available resources. \"\n",
    "    \"Use the crafter_interact tool to execute actions. \"\n",
    "    \"Key mechanics: use 'do' only when adjacent to a resource (tree, stone, cow, plant); \"\n",
    "    \"it does nothing on grass or water. \"\n",
    "    \"Craft progression: wood -> table -> wood_pickaxe -> stone -> stone_pickaxe -> iron tools. \"\n",
    "    \"Sleep when energy is low to restore and unlock wake_up. \"\n",
    "    f\"Available actions: {allowed_actions}. \"\n",
    "    \"Only use these action names and return 2-5 actions per decision. \"\n",
    "    \"Strategy: move toward trees to collect wood; place a table once you have wood; \"\n",
    "    \"craft a wood pickaxe, then collect stone and craft a stone pickaxe; \"\n",
    "    \"progress toward iron tools and combat when safe.\"\n",
    ")\n",
    "\n",
    "print(\"Baseline prompt:\")\n",
    "print(baseline_prompt[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Start Baseline Local API\n",
    "\n",
    "baseline_app = create_crafter_vlm_local_api(baseline_prompt)\n",
    "run_server_background(baseline_app, port=LOCAL_API_PORT)\n",
    "await wait_for_health_check(\"127.0.0.1\", LOCAL_API_PORT, environment_api_key, timeout=60.0)\n",
    "\n",
    "if USE_TUNNEL:\n",
    "    # Create tunnel to expose local API to the internet\n",
    "    print(f\"Creating tunnel for port {LOCAL_API_PORT}...\")\n",
    "    baseline_tunnel = await TunneledLocalAPI.create(\n",
    "        local_port=LOCAL_API_PORT,\n",
    "        backend=TunnelBackend.CloudflareManagedTunnel,\n",
    "        api_key=SYNTH_API_KEY,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        progress=True,\n",
    "    )\n",
    "    BASELINE_LOCAL_API_URL = baseline_tunnel.url\n",
    "else:\n",
    "    BASELINE_LOCAL_API_URL = f\"http://localhost:{LOCAL_API_PORT}\"\n",
    "\n",
    "print(f\"Baseline local API URL: {BASELINE_LOCAL_API_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Run GEPA Optimization\n",
    "\n",
    "print(\"Starting GEPA optimization...\")\n",
    "print(f\"  Rollout budget: {ROLLOUT_BUDGET}\")\n",
    "print(f\"  Generations: {NUM_GENERATIONS}\")\n",
    "\n",
    "job_result = run_gepa_job(\n",
    "    api_key=SYNTH_API_KEY,\n",
    "    local_api_url=BASELINE_LOCAL_API_URL,\n",
    "    baseline_system_prompt=baseline_prompt,\n",
    ")\n",
    "\n",
    "print(f\"\\nGEPA Status: {job_result.status.value}\")\n",
    "if job_result.succeeded:\n",
    "    print(\"GEPA optimization succeeded!\")\n",
    "else:\n",
    "    print(f\"GEPA failed: {job_result.error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Extract Optimized Prompt\n",
    "\n",
    "\n",
    "def extract_system_prompt(best_prompt: Optional[Dict[str, Any]]) -> Optional[str]:\n",
    "    \"\"\"Extract system prompt from prompt learning results.\"\"\"\n",
    "    if not best_prompt:\n",
    "        return None\n",
    "    for msg in best_prompt.get(\"messages\", []):\n",
    "        if msg.get(\"role\") == \"system\":\n",
    "            return msg.get(\"pattern\") or msg.get(\"content\")\n",
    "    for sec in best_prompt.get(\"sections\", []):\n",
    "        if sec.get(\"role\") == \"system\":\n",
    "            return sec.get(\"content\")\n",
    "    return None\n",
    "\n",
    "\n",
    "optimized_prompt = None\n",
    "\n",
    "if job_result.succeeded:\n",
    "    pl_client = PromptLearningClient(SYNTH_API_BASE, SYNTH_API_KEY)\n",
    "    prompt_results = await pl_client.get_prompts(job_result.job_id)\n",
    "    optimized_prompt = extract_system_prompt(prompt_results.best_prompt)\n",
    "\n",
    "    if optimized_prompt:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"OPTIMIZED PROMPT\")\n",
    "        print(\"=\" * 60)\n",
    "        print(optimized_prompt[:800] + \"...\" if len(optimized_prompt) > 800 else optimized_prompt)\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Save to results directory\n",
    "        results_dir = Path(\"results\")\n",
    "        results_dir.mkdir(exist_ok=True)\n",
    "        with open(results_dir / \"optimized_prompt.txt\", \"w\") as f:\n",
    "            f.write(optimized_prompt)\n",
    "        print(f\"\\nSaved optimized prompt to: {results_dir / 'optimized_prompt.txt'}\")\n",
    "    else:\n",
    "        print(\"Failed to extract optimized prompt from results\")\n",
    "else:\n",
    "    print(\"Skipping prompt extraction (GEPA did not succeed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Run Evaluation (Baseline vs Optimized)\n",
    "\n",
    "EVAL_SEEDS = list(range(100, 120))  # 20 held-out test samples\n",
    "\n",
    "if optimized_prompt:\n",
    "    # Start optimized local API\n",
    "    optimized_app = create_crafter_vlm_local_api(optimized_prompt)\n",
    "    run_server_background(optimized_app, port=OPTIMIZED_LOCAL_API_PORT)\n",
    "    await wait_for_health_check(\n",
    "        \"127.0.0.1\", OPTIMIZED_LOCAL_API_PORT, environment_api_key, timeout=60.0\n",
    "    )\n",
    "\n",
    "    if USE_TUNNEL:\n",
    "        # Create tunnel for optimized API\n",
    "        print(f\"Creating tunnel for port {OPTIMIZED_LOCAL_API_PORT}...\")\n",
    "        optimized_tunnel = await TunneledLocalAPI.create(\n",
    "            local_port=OPTIMIZED_LOCAL_API_PORT,\n",
    "            backend=TunnelBackend.CloudflareManagedTunnel,\n",
    "            api_key=SYNTH_API_KEY,\n",
    "            backend_url=SYNTH_API_BASE,\n",
    "            progress=True,\n",
    "        )\n",
    "        OPTIMIZED_LOCAL_API_URL = optimized_tunnel.url\n",
    "    else:\n",
    "        OPTIMIZED_LOCAL_API_URL = f\"http://localhost:{OPTIMIZED_LOCAL_API_PORT}\"\n",
    "\n",
    "    print(f\"Optimized local API URL: {OPTIMIZED_LOCAL_API_URL}\")\n",
    "    print(f\"\\nRunning evaluation on {len(EVAL_SEEDS)} seeds...\")\n",
    "\n",
    "    # Run baseline eval\n",
    "    print(\"\\nRunning BASELINE eval...\")\n",
    "    baseline_eval = run_eval_job(\n",
    "        local_api_url=BASELINE_LOCAL_API_URL,\n",
    "        seeds=EVAL_SEEDS,\n",
    "        mode=\"baseline\",\n",
    "    )\n",
    "\n",
    "    # Run optimized eval\n",
    "    print(\"\\nRunning OPTIMIZED eval...\")\n",
    "    optimized_eval = run_eval_job(\n",
    "        local_api_url=OPTIMIZED_LOCAL_API_URL,\n",
    "        seeds=EVAL_SEEDS,\n",
    "        mode=\"optimized\",\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Baseline: {baseline_eval.raw}\")\n",
    "    print(f\"Optimized: {optimized_eval.raw}\")\n",
    "\n",
    "    # Save results\n",
    "    results_dir = Path(\"results\")\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    with open(results_dir / \"eval_results.json\", \"w\") as f:\n",
    "        json.dump({\"baseline\": baseline_eval.raw, \"optimized\": optimized_eval.raw}, f, indent=2)\n",
    "    print(f\"\\nSaved eval results to: {results_dir / 'eval_results.json'}\")\n",
    "else:\n",
    "    print(\"Skipping evaluation (no optimized prompt)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14: Done\n",
    "print(\"Demo complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
