{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Synth GEPA Demo - Style Matching\n",
    "\n",
    "This notebook demonstrates **style-matching prompt optimization** using Synth's GEPA algorithm.\n",
    "\n",
    "We use a small, fixed dataset of essay prompts and gold examples defined directly in this notebook.\n",
    "The task app scores outputs using a zero-shot contrastive verifier, then we **optimize a verifier graph**\n",
    "(via Graph Evolve) and compare results across baseline vs optimized verifiers.\n",
    "\n",
    "**Run in Google Colab:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/synth-laboratories/synth-ai/blob/main/demos/style_matching/style_matching_prompt_optimization.ipynb)\n",
    "\n",
    "**Structure:**\n",
    "1. **Setup** - Install dependencies and configure API keys\n",
    "2. **Dataset + Task App** - Define a fixed dataset and a local task app\n",
    "3. **Optimize Verifier** - Evolve a verifier graph using graph-evolve jobs\n",
    "4. **Local Server + Tunnel** - Expose the task app to the Synth backend\n",
    "5. **Optimize Prompts** - Run GEPA with baseline vs optimized verifiers\n",
    "6. **Heldout Evaluation** - Score four combinations on a heldout set using eval jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Install dependencies (run this first on Colab)\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab - installing dependencies...\")\n",
    "    !pip install -q synth-ai httpx fastapi uvicorn nest_asyncio\n",
    "\n",
    "    # Install cloudflared\n",
    "    !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O /usr/local/bin/cloudflared\n",
    "    !chmod +x /usr/local/bin/cloudflared\n",
    "    !cloudflared --version\n",
    "\n",
    "    print(\"Dependencies installed!\")\n",
    "else:\n",
    "    print(\"Not in Colab - assuming dependencies are already installed\")\n",
    "    print(\"Required: pip install synth-ai httpx fastapi uvicorn nest_asyncio\")\n",
    "    print(\"Required: brew install cloudflare/cloudflare/cloudflared (macOS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Configure all imports, API keys, and environment keys in one place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setup - All imports, config, and API keys\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import httpx\n",
    "import nest_asyncio\n",
    "from fastapi import FastAPI, Header, HTTPException, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from synth_ai.core.urls import BACKEND_URL_BASE, backend_health_url, backend_me_url, join_url\n",
    "from synth_ai.sdk.auth import get_or_mint_synth_api_key\n",
    "from synth_ai.sdk.api.eval import EvalJob, EvalJobConfig\n",
    "from synth_ai.sdk.api.train.prompt_learning import PromptLearningJob\n",
    "from synth_ai.sdk.learning.prompt_learning_client import PromptLearningClient\n",
    "from synth_ai.sdk.learning.rl import mint_environment_api_key, setup_environment_api_key\n",
    "from synth_ai.sdk.task import run_server_background\n",
    "from synth_ai.sdk.tunnels import (\n",
    "    TunneledLocalAPI,\n",
    "    TunnelBackend,\n",
    "    kill_port,\n",
    "    wait_for_health_check,\n",
    "    cleanup_all,\n",
    "    find_available_port,\n",
    "    is_port_available,\n",
    ")\n",
    "\n",
    "from synth_ai.sdk.api.train.graph_optimization import (\n",
    "    GraphOptimizationClient,\n",
    "    GraphOptimizationConfig,\n",
    ")\n",
    "from synth_ai.sdk.api.train.graph_optimization_config import (\n",
    "    EvolutionConfig,\n",
    "    SeedsConfig,\n",
    "    LimitsConfig,\n",
    "    ProposerConfig,\n",
    ")\n",
    "\n",
    "USE_LOCAL_TASK_APP = True\n",
    "SYNTH_API_BASE = BACKEND_URL_BASE\n",
    "\n",
    "def _get_org_id() -> str:\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "    urls = [backend_me_url(SYNTH_API_BASE), join_url(SYNTH_API_BASE, \"/me\")]\n",
    "    for url in urls:\n",
    "        resp = httpx.get(url, headers=headers, timeout=30)\n",
    "        if resp.status_code == 404:\n",
    "            continue\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        org_id = data.get(\"org_id\") or data.get(\"orgId\")\n",
    "        if org_id:\n",
    "            return str(org_id)\n",
    "    raise RuntimeError(\"Unable to resolve org_id from /api/v1/me or /me\")\n",
    "\n",
    "\n",
    "LOCAL_TASK_PORT = 8132\n",
    "\n",
    "\n",
    "def _validate_api_key(api_key: str) -> bool:\n",
    "    if not api_key:\n",
    "        return False\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    try:\n",
    "        resp = httpx.get(backend_me_url(SYNTH_API_BASE), headers=headers, timeout=10)\n",
    "    except Exception:\n",
    "        return False\n",
    "    return resp.status_code == 200\n",
    "\n",
    "\n",
    "print(f\"Backend: {SYNTH_API_BASE}\")\n",
    "\n",
    "# Check backend health\n",
    "r = httpx.get(backend_health_url(SYNTH_API_BASE), timeout=30)\n",
    "if r.status_code != 200:\n",
    "    raise RuntimeError(f\"Backend not healthy: status {r.status_code}\")\n",
    "print(f\"Backend health: {r.json()}\")\n",
    "\n",
    "# Get API Key (use env var or mint demo key)\n",
    "API_KEY = get_or_mint_synth_api_key(backend_url=SYNTH_API_BASE, validator=_validate_api_key)\n",
    "print(\"\")\n",
    "print(\"Using SYNTH_API_KEY:\")\n",
    "print(f\"{API_KEY[:20]}...\")\n",
    "\n",
    "os.environ[\"SYNTH_API_KEY\"] = API_KEY\n",
    "\n",
    "# Mint and upload environment key (for local API authentication)\n",
    "ENVIRONMENT_API_KEY = mint_environment_api_key()\n",
    "print(\"\")\n",
    "print(\"Minted env key:\")\n",
    "print(f\"{ENVIRONMENT_API_KEY[:12]}...{ENVIRONMENT_API_KEY[-4:]}\")\n",
    "\n",
    "result = setup_environment_api_key(SYNTH_API_BASE, API_KEY, token=ENVIRONMENT_API_KEY)\n",
    "print(f\"Uploaded env key: {result}\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 50)\n",
    "print(\"SETUP COMPLETE\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Step 2: Dataset + Task App\n",
    "\n",
    "We define a fixed dataset of essay prompts and gold examples. The task app generates essays using the\n",
    "candidate prompt and scores outputs with the zero-shot contrastive verifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Fixed dataset + task app definition\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a thoughtful essayist with a direct, builder-first voice. \"\n",
    "    \"Write crisp, opinionated essays with concrete examples, minimal fluff, \"\n",
    "    \"and a short, memorable closing line. Aim for ~500 words (roughly 450-650).\"\n",
    ")\n",
    "\n",
    "USER_PROMPT_TEMPLATE = (\n",
    "    \"Title: {title}\\n\"\n",
    "    \"Outline:\\n{outline}\\n\\n\"\n",
    "    \"Notes:\\n{notes}\\n\\n\"\n",
    "    \"Target length: ~500 words.\\n\\n\"\n",
    "    \"Write the essay now.\"\n",
    ")\n",
    "\n",
    "TASKS: List[Dict[str, Any]] = [\n",
    "    {\n",
    "        \"id\": \"outline_speed\",\n",
    "        \"input\": {\n",
    "            \"title\": \"Shipping Fast Without Burning Out\",\n",
    "            \"outline\": (\n",
    "                \"1. Why speed compounds learning\\n\"\n",
    "                \"2. The tradeoff between velocity and quality\\n\"\n",
    "                \"3. How to keep teams aligned under pressure\\n\"\n",
    "                \"4. Practical rituals that preserve momentum\\n\"\n",
    "                \"5. Closing: pace as a competitive advantage\"\n",
    "            ),\n",
    "            \"notes\": [\"short feedback loops\", \"protect maker time\", \"use small bets\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"outline_focus\",\n",
    "        \"input\": {\n",
    "            \"title\": \"Focus Beats Optionality\",\n",
    "            \"outline\": (\n",
    "                \"1. Optionality feels safe but slows decisions\\n\"\n",
    "                \"2. Focus creates a compounding advantage\\n\"\n",
    "                \"3. The cost of context switching in small teams\\n\"\n",
    "                \"4. Saying no as a leadership skill\\n\"\n",
    "                \"5. Closing: clarity is leverage\"\n",
    "            ),\n",
    "            \"notes\": [\"pick one wedge\", \"eliminate parallel bets\", \"repeat a simple story\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"outline_learning\",\n",
    "        \"input\": {\n",
    "            \"title\": \"Learning in Public\",\n",
    "            \"outline\": (\n",
    "                \"1. Writing as a forcing function\\n\"\n",
    "                \"2. How sharing drafts accelerates feedback\\n\"\n",
    "                \"3. The credibility flywheel\\n\"\n",
    "                \"4. Risks of over-sharing (and how to avoid them)\\n\"\n",
    "                \"5. Closing: publish to clarify\"\n",
    "            ),\n",
    "            \"notes\": [\n",
    "                \"ship drafts, not polished essays\",\n",
    "                \"ask for specific feedback\",\n",
    "                \"be concrete about failures\",\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"outline_quality\",\n",
    "        \"input\": {\n",
    "            \"title\": \"Quality as a Constraint\",\n",
    "            \"outline\": (\n",
    "                \"1. The myth that quality slows you down\\n\"\n",
    "                \"2. Cheap fixes vs durable systems\\n\"\n",
    "                \"3. When to accept rough edges\\n\"\n",
    "                \"4. How to build taste in a team\\n\"\n",
    "                \"5. Closing: quality is a habit\"\n",
    "            ),\n",
    "            \"notes\": [\"use guardrails\", \"invest in tooling\", \"make quality visible\"],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def _make_trace(user_text: str, assistant_text: str) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"session_id\": \"trace\",\n",
    "        \"session_time_steps\": [\n",
    "            {\n",
    "                \"step_id\": \"1\",\n",
    "                \"step_index\": 0,\n",
    "                \"events\": [\n",
    "                    {\n",
    "                        \"event_type\": \"runtime\",\n",
    "                        \"event_id\": 1,\n",
    "                        \"type\": \"user_message\",\n",
    "                        \"content\": user_text,\n",
    "                    },\n",
    "                    {\n",
    "                        \"event_type\": \"runtime\",\n",
    "                        \"event_id\": 2,\n",
    "                        \"type\": \"assistant_message\",\n",
    "                        \"content\": assistant_text,\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "GOLD_EXAMPLES: List[Dict[str, Any]] = [\n",
    "    {\n",
    "        \"summary\": \"Direct, builder tone with concrete examples\",\n",
    "        \"gold_score\": 0.95,\n",
    "        \"gold_reasoning\": \"Short sentences, decisive stance, concrete advice, crisp closing line.\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about shipping fast.\",\n",
    "            \"Speed is a learning engine. Ship a small bet, watch users react, then sharpen the next move. Protect maker time, keep scope tight, and treat every release as feedback, not theater. Momentum beats motivation.\",\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"summary\": \"Opinionated essay with a sharp closing line\",\n",
    "        \"gold_score\": 0.92,\n",
    "        \"gold_reasoning\": \"Clear thesis, direct claims, practical advice, and a memorable end.\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about focus.\",\n",
    "            \"Optionality feels safe, but it dilutes learning. Pick one wedge, cut parallel bets, and ship. Small teams win by saying no early and often. Clarity is leverage.\",\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"summary\": \"Concrete, tactical quality guidance with guardrails\",\n",
    "        \"gold_score\": 0.96,\n",
    "        \"gold_reasoning\": \"Direct stance, concrete guardrails, and a crisp closing line.\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about quality as a constraint.\",\n",
    "            \"Quality is the guardrail that keeps speed from turning into chaos. Ship small, test the riskiest paths, and make rollback cheap. Fix root causes once, then automate the prevention. Quality is a habit, not a milestone.\",\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "BASELINE_VERIFIER_JOB_ID = \"zero_shot_verifier_contrastive_single\"\n",
    "VERIFIER_MODEL = \"gpt-4.1-nano\"\n",
    "VERIFIER_JOB_ID = BASELINE_VERIFIER_JOB_ID\n",
    "OPTIMIZED_VERIFIER_JOB_ID = None\n",
    "\n",
    "ROLLOUT_LOG: List[Dict[str, Any]] = []\n",
    "\n",
    "\n",
    "def _verify_api_key(x_api_key: Optional[str]) -> bool:\n",
    "    if not ENVIRONMENT_API_KEY:\n",
    "        return True\n",
    "    return x_api_key == ENVIRONMENT_API_KEY\n",
    "\n",
    "\n",
    "def _format_notes(notes: List[str]) -> str:\n",
    "    if not notes:\n",
    "        return \"- (none)\"\n",
    "    return \"\\n\".join(f\"- {note}\" for note in notes)\n",
    "\n",
    "\n",
    "def _safe_format(text: str, values: Dict[str, Any]) -> str:\n",
    "    class _DefaultDict(dict):\n",
    "        def __missing__(self, key: str) -> str:\n",
    "            return \"\"\n",
    "\n",
    "    return text.format_map(_DefaultDict(values))\n",
    "\n",
    "\n",
    "def _render_messages_from_sections(\n",
    "    sections: List[Dict[str, Any]], values: Dict[str, Any]\n",
    ") -> List[Dict[str, str]]:\n",
    "    rendered = []\n",
    "    for section in sorted(sections, key=lambda s: s.get(\"order\", 0)):\n",
    "        role = section.get(\"role\", \"user\")\n",
    "        content = section.get(\"content\") or section.get(\"pattern\") or \"\"\n",
    "        if content:\n",
    "            rendered.append({\"role\": str(role), \"content\": _safe_format(str(content), values)})\n",
    "    return rendered\n",
    "\n",
    "\n",
    "def _build_messages(\n",
    "    task_input: Dict[str, Any], prompt_sections: Optional[List[Dict[str, Any]]] = None\n",
    ") -> List[Dict[str, str]]:\n",
    "    notes_text = _format_notes(task_input.get(\"notes\", []))\n",
    "    values = {\n",
    "        \"title\": task_input.get(\"title\", \"\"),\n",
    "        \"outline\": task_input.get(\"outline\", \"\"),\n",
    "        \"notes\": notes_text,\n",
    "    }\n",
    "    if prompt_sections:\n",
    "        return _render_messages_from_sections(prompt_sections, values)\n",
    "\n",
    "    user_prompt = USER_PROMPT_TEMPLATE.format(\n",
    "        title=values[\"title\"],\n",
    "        outline=values[\"outline\"],\n",
    "        notes=values[\"notes\"],\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "\n",
    "def _build_inference_url(inference_url: str) -> str:\n",
    "    if \"?\" in inference_url:\n",
    "        base, query = inference_url.split(\"?\", 1)\n",
    "        return f\"{join_url(base, '/chat/completions')}?{query}\"\n",
    "    return join_url(inference_url, \"/chat/completions\")\n",
    "\n",
    "\n",
    "def _extract_completion(data: Dict[str, Any]) -> str:\n",
    "    try:\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"] or \"\"\n",
    "    except (KeyError, IndexError, TypeError):\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _extract_verifier_score(result: Dict[str, Any]) -> float:\n",
    "    output = result.get(\"output\", result)\n",
    "    if isinstance(output, dict):\n",
    "        outcome_review = output.get(\"outcome_review\")\n",
    "        if isinstance(outcome_review, dict) and isinstance(\n",
    "            outcome_review.get(\"total\"), (int, float)\n",
    "        ):\n",
    "            return float(outcome_review[\"total\"])\n",
    "        event_reviews = output.get(\"event_reviews\")\n",
    "        if isinstance(event_reviews, list) and event_reviews:\n",
    "            totals = [rev.get(\"total\") for rev in event_reviews if isinstance(rev, dict)]\n",
    "            totals = [t for t in totals if isinstance(t, (int, float))]\n",
    "            if totals:\n",
    "                return float(sum(totals) / len(totals))\n",
    "        if isinstance(output.get(\"total\"), (int, float)):\n",
    "            return float(output[\"total\"])\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "async def _call_policy_llm(messages: List[Dict[str, str]], policy_config: Dict[str, Any]) -> str:\n",
    "    inference_url = policy_config.get(\"inference_url\")\n",
    "    if not inference_url:\n",
    "        raise RuntimeError(\"policy.config.inference_url is required\")\n",
    "\n",
    "    url = _build_inference_url(inference_url)\n",
    "    model = policy_config.get(\"model\", \"gpt-4.1-nano\")\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    api_key = policy_config.get(\"api_key\")\n",
    "    if api_key:\n",
    "        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "    elif ENVIRONMENT_API_KEY:\n",
    "        headers[\"X-API-Key\"] = ENVIRONMENT_API_KEY\n",
    "        headers[\"Authorization\"] = f\"Bearer {ENVIRONMENT_API_KEY}\"\n",
    "\n",
    "    payload = {\"model\": model, \"messages\": messages}\n",
    "    payload[\"temperature\"] = float(policy_config.get(\"temperature\", 0.7))\n",
    "    payload[\"max_tokens\"] = int(policy_config.get(\"max_completion_tokens\", 1200))\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=120.0) as client:\n",
    "        response = await client.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return _extract_completion(response.json())\n",
    "\n",
    "\n",
    "async def _score_with_verifier(\n",
    "    session_trace: Dict[str, Any], verifier_job_id: Optional[str] = None\n",
    ") -> float:\n",
    "    job_id = verifier_job_id or VERIFIER_JOB_ID\n",
    "    payload = {\n",
    "        \"job_id\": job_id,\n",
    "        \"input\": {\n",
    "            \"trace\": session_trace,\n",
    "            \"gold_examples\": GOLD_EXAMPLES,\n",
    "            \"candidate_score\": 0.5,\n",
    "            \"candidate_reasoning\": \"Auto-evaluated from style-matching task app\",\n",
    "            \"options\": {\"model\": VERIFIER_MODEL},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=120.0) as client:\n",
    "        response = await client.post(\n",
    "            join_url(SYNTH_API_BASE, \"/api/graphs/completions\"),\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(\n",
    "                f\"Verifier failed: HTTP {response.status_code} {response.text[:500]}\"\n",
    "            )\n",
    "        result = response.json()\n",
    "\n",
    "    return _extract_verifier_score(result)\n",
    "\n",
    "\n",
    "app = FastAPI(title=\"Style Matching Task App\")\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health() -> Dict[str, str]:\n",
    "    return {\"status\": \"ok\", \"task_app\": \"style_matching\"}\n",
    "\n",
    "\n",
    "@app.get(\"/task_info\")\n",
    "async def task_info() -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"taskset\": {\n",
    "            \"name\": \"style_matching\",\n",
    "            \"description\": \"Style matching demo task app\",\n",
    "            \"size\": len(TASKS),\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "@app.get(\"/tasks\")\n",
    "async def list_tasks() -> Dict[str, Any]:\n",
    "    return {\"tasks\": TASKS, \"gold_examples\": GOLD_EXAMPLES}\n",
    "\n",
    "\n",
    "@app.get(\"/rollouts\")\n",
    "async def list_rollouts(x_api_key: Optional[str] = Header(None)) -> Dict[str, Any]:\n",
    "    if not _verify_api_key(x_api_key):\n",
    "        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n",
    "    return {\"rollouts\": ROLLOUT_LOG}\n",
    "\n",
    "\n",
    "@app.post(\"/rollout\")\n",
    "async def rollout(request: Request, x_api_key: Optional[str] = Header(None)) -> Dict[str, Any]:\n",
    "    if not _verify_api_key(x_api_key):\n",
    "        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n",
    "\n",
    "    try:\n",
    "        data = await request.json()\n",
    "    except Exception:\n",
    "        raise HTTPException(status_code=400, detail=\"Invalid JSON\") from None\n",
    "\n",
    "    run_id = data.get(\"run_id\")\n",
    "    env = data.get(\"env\", {})\n",
    "    policy = data.get(\"policy\", {})\n",
    "    policy_config = policy.get(\"config\", {})\n",
    "\n",
    "    trace_correlation_id = policy_config.get(\"trace_correlation_id\")\n",
    "\n",
    "    env_config = env.get(\"config\", {}) or {}\n",
    "    verifier_job_id = env_config.get(\"verifier_job_id\") or VERIFIER_JOB_ID\n",
    "    prompt_sections = env_config.get(\"prompt_sections\")\n",
    "\n",
    "    seed = int(env.get(\"seed\", 0))\n",
    "    task = TASKS[seed % len(TASKS)]\n",
    "    task_input = task[\"input\"]\n",
    "\n",
    "    messages = _build_messages(task_input, prompt_sections=prompt_sections)\n",
    "\n",
    "    try:\n",
    "        essay = await _call_policy_llm(messages, policy_config)\n",
    "    except Exception as exc:\n",
    "        essay = f\"[error: {exc}]\"\n",
    "\n",
    "    session_trace = {\n",
    "        \"session_id\": f\"style-matching-{task['id']}\",\n",
    "        \"session_time_steps\": [\n",
    "            {\n",
    "                \"step_id\": \"1\",\n",
    "                \"step_index\": 0,\n",
    "                \"events\": [\n",
    "                    {\n",
    "                        \"event_type\": \"runtime\",\n",
    "                        \"event_id\": 1,\n",
    "                        \"type\": \"user_message\",\n",
    "                        \"content\": messages[-1][\"content\"],\n",
    "                    },\n",
    "                    {\n",
    "                        \"event_type\": \"runtime\",\n",
    "                        \"event_id\": 2,\n",
    "                        \"type\": \"assistant_message\",\n",
    "                        \"content\": essay,\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    score = await _score_with_verifier(session_trace, verifier_job_id=verifier_job_id)\n",
    "\n",
    "    ROLLOUT_LOG.append(\n",
    "        {\n",
    "            \"run_id\": run_id,\n",
    "            \"seed\": seed,\n",
    "            \"task_id\": task[\"id\"],\n",
    "            \"title\": task_input.get(\"title\", \"\"),\n",
    "            \"essay\": essay,\n",
    "            \"score\": score,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"metrics\": {\n",
    "            \"mean_return\": score,\n",
    "            \"outcome_score\": score,\n",
    "            \"num_steps\": 1,\n",
    "            \"details\": {\"verifier_score\": score},\n",
    "        },\n",
    "        \"trajectories\": [\n",
    "            {\"steps\": [{\"observation\": task_input, \"action\": {\"essay\": essay}, \"reward\": score}]}\n",
    "        ],\n",
    "        \"metadata\": {\"task_id\": task[\"id\"]},\n",
    "        \"trace_correlation_id\": trace_correlation_id or \"\",\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Step 3: Optimize Verifier Graph\n",
    "\n",
    "We use Graph Evolve to optimize a verifier graph on a small, fixed calibration set.\n",
    "This graph will serve as an alternative verifier in the GEPA loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Optimize a verifier graph with Graph Evolve\n",
    "\n",
    "VERIFIER_EXAMPLES = [\n",
    "    {\n",
    "        \"task_id\": \"good_speed\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about shipping fast.\",\n",
    "            \"Speed compounds learning. Ship small bets, learn fast, keep scope tight, and protect maker time.\",\n",
    "        ),\n",
    "        \"score\": 0.95,\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"good_focus\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about focus.\",\n",
    "            \"Optionality dilutes learning. Pick one wedge, cut parallel bets, and repeat a simple story.\",\n",
    "        ),\n",
    "        \"score\": 0.92,\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"good_quality\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about quality as a constraint.\",\n",
    "            \"Quality is a guardrail. Ship small, test risky paths, and make rollback cheap.\",\n",
    "        ),\n",
    "        \"score\": 0.93,\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"good_learning\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about learning in public.\",\n",
    "            \"Publish drafts to accelerate feedback, build credibility, and clarify thinking.\",\n",
    "        ),\n",
    "        \"score\": 0.91,\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"bad_rambling\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about focus.\",\n",
    "            \"Focus is important because focus is important. You should focus on focusing and focus on focus.\",\n",
    "        ),\n",
    "        \"score\": 0.10,\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"bad_vague\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about quality.\",\n",
    "            \"Quality is good. Teams should be good and do good things to make quality good.\",\n",
    "        ),\n",
    "        \"score\": 0.05,\n",
    "    },\n",
    "]\n",
    "\n",
    "VERIFIER_TRAIN_SEEDS = list(range(4))\n",
    "VERIFIER_VAL_SEEDS = list(range(4, len(VERIFIER_EXAMPLES)))\n",
    "\n",
    "verifier_dataset = {\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_id\": example[\"task_id\"],\n",
    "            \"input\": {\"trace\": example[\"trace\"], \"gold_examples\": GOLD_EXAMPLES},\n",
    "        }\n",
    "        for example in VERIFIER_EXAMPLES\n",
    "    ],\n",
    "    \"gold_outputs\": [\n",
    "        {\"task_id\": example[\"task_id\"], \"output\": {}, \"score\": example[\"score\"]}\n",
    "        for example in VERIFIER_EXAMPLES\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"name\": \"style_matching_verifier\",\n",
    "        \"task_description\": (\n",
    "            \"Score style-matching quality using strict verifier-style outputs. Return event_reviews, \"\n",
    "            \"outcome_review, event_totals; use a 1.0 baseline with deductions for discrepancies \"\n",
    "            \"(generic outputs < 0.3).\"\n",
    "        ),\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"trace\": {\"type\": \"object\"}, \"gold_examples\": {\"type\": \"array\"}},\n",
    "            \"required\": [\"trace\", \"gold_examples\"],\n",
    "        },\n",
    "        \"output_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"event_reviews\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"criteria\": {\"type\": \"object\"},\n",
    "                            \"total\": {\"type\": \"number\"},\n",
    "                            \"summary\": {\"type\": \"string\"},\n",
    "                        },\n",
    "                        \"required\": [\"criteria\", \"total\"],\n",
    "                    },\n",
    "                },\n",
    "                \"outcome_review\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"criteria\": {\"type\": \"object\"},\n",
    "                        \"total\": {\"type\": \"number\"},\n",
    "                        \"summary\": {\"type\": \"string\"},\n",
    "                    },\n",
    "                    \"required\": [\"criteria\", \"total\"],\n",
    "                },\n",
    "                \"event_totals\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}},\n",
    "                \"score\": {\"type\": \"number\"},\n",
    "            },\n",
    "            \"required\": [\"event_reviews\", \"outcome_review\", \"event_totals\"],\n",
    "        },\n",
    "        \"output_config\": {\n",
    "            \"format\": \"json\",\n",
    "            \"strict\": True,\n",
    "            \"extract_from\": [\"(root)\"],\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"event_reviews\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"criteria\": {\"type\": \"object\"},\n",
    "                                \"total\": {\"type\": \"number\"},\n",
    "                                \"summary\": {\"type\": \"string\"},\n",
    "                            },\n",
    "                            \"required\": [\"criteria\", \"total\"],\n",
    "                        },\n",
    "                    },\n",
    "                    \"outcome_review\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"criteria\": {\"type\": \"object\"},\n",
    "                            \"total\": {\"type\": \"number\"},\n",
    "                            \"summary\": {\"type\": \"string\"},\n",
    "                        },\n",
    "                        \"required\": [\"criteria\", \"total\"],\n",
    "                    },\n",
    "                    \"event_totals\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}},\n",
    "                    \"score\": {\"type\": \"number\"},\n",
    "                },\n",
    "                \"required\": [\"event_reviews\", \"outcome_review\", \"event_totals\"],\n",
    "            },\n",
    "        },\n",
    "        \"domain\": \"text\",\n",
    "    },\n",
    "}\n",
    "\n",
    "verifier_config = GraphOptimizationConfig(\n",
    "    algorithm=\"graph_evolve\",\n",
    "    dataset_name=\"style_matching_verifier\",\n",
    "    graph_type=\"verifier\",\n",
    "    graph_structure=\"single_prompt\",\n",
    "    topology_guidance=(\n",
    "        \"Single-node VerifierGraph. Use one DagNode (e.g., judge_style) with template_transform. \"\n",
    "        \"Set output_mapping to copy event_reviews, outcome_review, event_totals, score to root. \"\n",
    "        \"Include verdict_weights and aggregation_policy: weighted_average.\"\n",
    "    ),\n",
    "    allowed_policy_models=[\"gpt-4.1-nano\", \"gpt-4o-mini\"],\n",
    "    evolution=EvolutionConfig(num_generations=3, children_per_generation=2),\n",
    "    proposer=ProposerConfig(model=\"gpt-4.1\", temperature=0.0),\n",
    "    seeds=SeedsConfig(train=VERIFIER_TRAIN_SEEDS, validation=VERIFIER_VAL_SEEDS),\n",
    "    limits=LimitsConfig(max_spend_usd=5.0, timeout_seconds=3600),\n",
    "    verifier_mode=\"contrastive\",\n",
    "    verifier_model=VERIFIER_MODEL,\n",
    "    dataset=verifier_dataset,\n",
    "    output_schema=verifier_dataset[\"metadata\"][\"output_schema\"],\n",
    "    output_config=verifier_dataset[\"metadata\"][\"output_config\"],\n",
    "    task_description=(\n",
    "        \"Score style-matching quality using strict verifier-style outputs. Return event_reviews, \"\n",
    "        \"outcome_review, event_totals; use a 1.0 baseline with deductions for discrepancies \"\n",
    "        \"(generic outputs < 0.3).\"\n",
    "    ),\n",
    "    problem_spec=(\n",
    "        \"You are generating a VerifierGraph. The final output MUST be a JSON object with: \"\n",
    "        \"event_reviews (list of per-event review objects with criteria, total, summary), \"\n",
    "        \"outcome_review (object with criteria, total, summary), and event_totals (list of numbers). \"\n",
    "        \"Include a top-level score if helpful, but the verifier contract is based on outcome_review.total \"\n",
    "        \"and event_totals. Make totals floats in [0,1]. Scoring policy must be strict: start at 1.0 and \"\n",
    "        \"deduct for every discrepancy vs gold examples. Generic/standard outputs should score below 0.3. \"\n",
    "        \"Deduction guide: obvious/giveaway discrepancy deduct 0.15-0.3, major discrepancy 0.08-0.15, \"\n",
    "        \"minor 0.02-0.08.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "async def run_verifier_optimization() -> tuple[str, Dict[str, Any]]:\n",
    "    async with GraphOptimizationClient(SYNTH_API_BASE, API_KEY) as client:\n",
    "        job_id = await client.start_job(verifier_config)\n",
    "        print(f\"Graph evolve job: {job_id}\")\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=90.0) as http:\n",
    "        headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "        for _ in range(900):\n",
    "            try:\n",
    "                status_resp = await http.get(\n",
    "                    join_url(SYNTH_API_BASE, f\"/graph-evolve/jobs/{job_id}/status\"),\n",
    "                    headers=headers,\n",
    "                )\n",
    "                if status_resp.status_code == 404:\n",
    "                    await asyncio.sleep(2.0)\n",
    "                    continue\n",
    "                status_resp.raise_for_status()\n",
    "                status = status_resp.json().get(\"status\")\n",
    "                if status in {\"completed\", \"failed\", \"cancelled\"}:\n",
    "                    result_resp = await http.get(\n",
    "                        join_url(SYNTH_API_BASE, f\"/graph-evolve/jobs/{job_id}/result\"),\n",
    "                        headers=headers,\n",
    "                    )\n",
    "                    result_resp.raise_for_status()\n",
    "                    return job_id, result_resp.json()\n",
    "            except httpx.HTTPStatusError:\n",
    "                await asyncio.sleep(2.0)\n",
    "                continue\n",
    "            await asyncio.sleep(2.0)\n",
    "\n",
    "    raise RuntimeError(f\"Graph evolve job {job_id} did not complete in time\")\n",
    "\n",
    "\n",
    "async def save_verifier_graph(job_id: str, org_id: str) -> str:\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"name\": f\"style-matching-verifier-{job_id[:8]}\",\n",
    "        \"org_id\": org_id,\n",
    "        \"kind\": \"verifier\",\n",
    "    }\n",
    "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "        resp = await client.post(\n",
    "            join_url(SYNTH_API_BASE, f\"/graph-evolve/jobs/{job_id}/save-graph\"),\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "        )\n",
    "        if resp.status_code >= 400:\n",
    "            raise RuntimeError(f\"save-graph failed ({resp.status_code}): {resp.text[:500]}\")\n",
    "        data = resp.json()\n",
    "    graph_id = data.get(\"graph_id\") or data.get(\"id\")\n",
    "    if not graph_id:\n",
    "        raise RuntimeError(f\"save-graph did not return graph_id: {data}\")\n",
    "    return str(graph_id)\n",
    "\n",
    "\n",
    "ORG_ID = _get_org_id()\n",
    "print(f\"Resolved org_id: {ORG_ID}\")\n",
    "\n",
    "OPTIMIZED_VERIFIER_JOB_ID, OPTIMIZED_VERIFIER_RESULT = await run_verifier_optimization()\n",
    "status = OPTIMIZED_VERIFIER_RESULT.get(\"status\")\n",
    "if status != \"completed\":\n",
    "    error_msg = OPTIMIZED_VERIFIER_RESULT.get(\"error\") or \"unknown error\"\n",
    "    raise RuntimeError(f\"Graph evolve job failed with status={status}: {error_msg}\")\n",
    "\n",
    "OPTIMIZED_VERIFIER_GRAPH_ID = await save_verifier_graph(OPTIMIZED_VERIFIER_JOB_ID, ORG_ID)\n",
    "print(\"\\nVerifier optimization complete\")\n",
    "print(f\"Optimized verifier job id: {OPTIMIZED_VERIFIER_JOB_ID}\")\n",
    "print(f\"Optimized verifier graph id: {OPTIMIZED_VERIFIER_GRAPH_ID}\")\n",
    "print(f\"Best score: {OPTIMIZED_VERIFIER_RESULT.get('best_score')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Step 4: Start Task App + Tunnel\n",
    "\n",
    "Run the task app locally, then create a tunnel so the Synth backend can call it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Start task app and create tunnel\n",
    "\n",
    "import asyncio\n",
    "import socket\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "def wait_for_system_dns(hostname: str, timeout: float = 90.0, interval: float = 3.0) -> None:\n",
    "    deadline = time.time() + timeout\n",
    "    last_exc = None\n",
    "    while time.time() < deadline:\n",
    "        try:\n",
    "            socket.gethostbyname(hostname)\n",
    "            return\n",
    "        except Exception as exc:\n",
    "            last_exc = exc\n",
    "            time.sleep(interval)\n",
    "    raise RuntimeError(f\"System DNS did not resolve {hostname} within {timeout}s: {last_exc}\")\n",
    "\n",
    "\n",
    "def _task_app_healthcheck(host: str, port: int) -> bool:\n",
    "    try:\n",
    "        resp = httpx.get(\n",
    "            f\"http://{host}:{port}/health\",\n",
    "            headers={\"X-API-Key\": ENVIRONMENT_API_KEY},\n",
    "            timeout=5,\n",
    "        )\n",
    "        return resp.status_code == 200\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _wait_for_task_app(host: str, port: int, timeout: float = 30.0) -> None:\n",
    "    deadline = time.time() + timeout\n",
    "    while time.time() < deadline:\n",
    "        if _task_app_healthcheck(host, port):\n",
    "            return\n",
    "        time.sleep(1.0)\n",
    "    raise RuntimeError(f\"Task app health check failed after {timeout}s\")\n",
    "\n",
    "\n",
    "task_app_thread = None\n",
    "_task_app_lock = threading.Lock()\n",
    "TASK_APP_URL = \"\"\n",
    "\n",
    "\n",
    "def _start_task_app() -> None:\n",
    "    global LOCAL_TASK_PORT\n",
    "    global task_app_thread\n",
    "\n",
    "    kill_port(LOCAL_TASK_PORT)\n",
    "    if not is_port_available(LOCAL_TASK_PORT):\n",
    "        LOCAL_TASK_PORT = find_available_port(LOCAL_TASK_PORT + 1)\n",
    "        print(f\"Port in use; switched to {LOCAL_TASK_PORT}\")\n",
    "\n",
    "    task_app_thread = run_server_background(app, LOCAL_TASK_PORT, host=LOCAL_TASK_HOST)\n",
    "    _wait_for_task_app(LOCAL_TASK_HOST, LOCAL_TASK_PORT, timeout=30.0)\n",
    "\n",
    "\n",
    "def _start_task_app_monitor(interval: float = 5.0) -> threading.Thread:\n",
    "    def _monitor() -> None:\n",
    "        while True:\n",
    "            time.sleep(interval)\n",
    "            with _task_app_lock:\n",
    "                if _task_app_healthcheck(LOCAL_TASK_HOST, LOCAL_TASK_PORT):\n",
    "                    continue\n",
    "                print(\"Task app health check failed; restarting...\")\n",
    "                try:\n",
    "                    _start_task_app()\n",
    "                except Exception as exc:\n",
    "                    print(f\"Task app restart failed: {exc}\")\n",
    "\n",
    "    thread = threading.Thread(target=_monitor, daemon=True, name=\"task-app-monitor\")\n",
    "    thread.start()\n",
    "    return thread\n",
    "\n",
    "\n",
    "print(f\"Starting task app on port {LOCAL_TASK_PORT}...\")\n",
    "with _task_app_lock:\n",
    "    _start_task_app()\n",
    "print(\"Task app ready!\")\n",
    "monitor_thread = _start_task_app_monitor()\n",
    "\n",
    "if USE_LOCAL_TASK_APP:\n",
    "    print(\"Using localhost task app URL (no tunnel)\")\n",
    "    TASK_APP_URL = f\"http://{LOCAL_TASK_HOST}:{LOCAL_TASK_PORT}\"\n",
    "    tunnel = None\n",
    "else:\n",
    "    print(\"\")\n",
    "    print(\"Provisioning Cloudflare tunnel...\")\n",
    "    try:\n",
    "        tunnel = await TunneledLocalAPI.create(\n",
    "            local_port=LOCAL_TASK_PORT,\n",
    "            backend=TunnelBackend.CloudflareManagedTunnel,\n",
    "            api_key=API_KEY,\n",
    "            env_api_key=ENVIRONMENT_API_KEY,\n",
    "            backend_url=SYNTH_API_BASE,\n",
    "            reason=\"style_matching_notebook\",\n",
    "            progress=True,\n",
    "        )\n",
    "        print(f\"Waiting for system DNS to resolve {tunnel.hostname}...\")\n",
    "        await wait_for_system_dns(tunnel.hostname)\n",
    "    except Exception as exc:\n",
    "        print(f\"Managed tunnel failed or DNS unresolved ({exc}). Falling back to quick tunnel...\")\n",
    "        tunnel = await TunneledLocalAPI.create(\n",
    "            local_port=LOCAL_TASK_PORT,\n",
    "            backend=TunnelBackend.CloudflareQuickTunnel,\n",
    "            env_api_key=ENVIRONMENT_API_KEY,\n",
    "            progress=True,\n",
    "        )\n",
    "\n",
    "if tunnel is not None:\n",
    "    TASK_APP_URL = tunnel.url\n",
    "print(f\"Task app URL: {TASK_APP_URL}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step 5: Run GEPA With Baseline vs Optimized Verifiers\n",
    "\n",
    "We run GEPA twice: once with the baseline verifier and once with the optimized verifier graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Run GEPA (baseline vs optimized verifier)\n",
    "\n",
    "POLICY_MODEL = \"gpt-4.1-nano\"\n",
    "\n",
    "\n",
    "def _make_gepa_config(task_app_url: str) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"prompt_learning\": {\n",
    "            \"algorithm\": \"gepa\",\n",
    "            \"task_app_url\": task_app_url,\n",
    "            \"task_app_api_key\": ENVIRONMENT_API_KEY,\n",
    "            \"env_name\": \"style-matching\",\n",
    "            \"initial_prompt\": {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"order\": 0, \"pattern\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"order\": 1, \"pattern\": USER_PROMPT_TEMPLATE},\n",
    "                ],\n",
    "                \"wildcards\": {\"title\": \"REQUIRED\", \"outline\": \"REQUIRED\", \"notes\": \"REQUIRED\"},\n",
    "            },\n",
    "            \"policy\": {\n",
    "                \"inference_mode\": \"synth_hosted\",\n",
    "                \"model\": POLICY_MODEL,\n",
    "                \"provider\": \"openai\",\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_completion_tokens\": 1200,\n",
    "            },\n",
    "            \"gepa\": {\n",
    "                \"env_name\": \"style-matching\",\n",
    "                \"evaluation\": {\"seeds\": list(range(13)), \"validation_seeds\": list(range(13, 17))},\n",
    "                \"rollout\": {\"budget\": 48, \"max_concurrent\": 3, \"minibatch_size\": 3},\n",
    "                \"mutation\": {\"rate\": 0.3},\n",
    "                \"population\": {\"initial_size\": 3, \"num_generations\": 3, \"children_per_generation\": 2},\n",
    "                \"archive\": {\"size\": 5, \"pareto_set_size\": 10},\n",
    "                \"token\": {\"counting_model\": \"gpt-4\"},\n",
    "            },\n",
    "            \"verifier\": {\"enabled\": False, \"reward_source\": \"task_app\"},\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "async def run_gepa_with_verifier(label: str, verifier_job_id: str):\n",
    "    global VERIFIER_JOB_ID\n",
    "    VERIFIER_JOB_ID = verifier_job_id\n",
    "    print(f\"\\nRunning GEPA ({label}) with verifier: {verifier_job_id}\")\n",
    "\n",
    "    config_body = _make_gepa_config(TASK_APP_URL)\n",
    "    job = PromptLearningJob.from_dict(\n",
    "        config_dict=config_body,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        task_app_api_key=ENVIRONMENT_API_KEY,\n",
    "    )\n",
    "\n",
    "    job_id = job.submit()\n",
    "    print(f\"GEPA job id ({label}): {job_id}\")\n",
    "\n",
    "    result = job.poll_until_complete(timeout=3600.0, interval=3.0, progress=True)\n",
    "    print(f\"GEPA finished ({label}): {result.status.value}\")\n",
    "    if result.failed:\n",
    "        error_msg = result.error\n",
    "        if not error_msg:\n",
    "            try:\n",
    "                client = PromptLearningClient(SYNTH_API_BASE, API_KEY)\n",
    "                events = await client.get_events(job_id, limit=5000)\n",
    "            except Exception as exc:\n",
    "                error_msg = f\"Failed to fetch job events: {exc}\"\n",
    "            else:\n",
    "                for event in reversed(events):\n",
    "                    event_type = str(event.get(\"type\", \"\"))\n",
    "                    if event_type in {\"prompt.learning.failed\", \"job.failed\"}:\n",
    "                        error_msg = event.get(\"message\") or event.get(\"data\")\n",
    "                        break\n",
    "        raise RuntimeError(f\"GEPA job failed ({label}): {error_msg or 'unknown error'}\")\n",
    "    return job_id, result\n",
    "\n",
    "\n",
    "baseline_gepa_job_id, baseline_gepa_result = await run_gepa_with_verifier(\n",
    "    \"baseline\", BASELINE_VERIFIER_JOB_ID\n",
    ")\n",
    "\n",
    "optimized_gepa_job_id, optimized_gepa_result = await run_gepa_with_verifier(\n",
    "    \"optimized\", OPTIMIZED_VERIFIER_JOB_ID\n",
    ")\n",
    "\n",
    "pl_client = PromptLearningClient(SYNTH_API_BASE, API_KEY)\n",
    "\n",
    "baseline_prompts = await pl_client.get_prompts(baseline_gepa_job_id)\n",
    "optimized_prompts = await pl_client.get_prompts(optimized_gepa_job_id)\n",
    "\n",
    "\n",
    "def _select_prompt(result):\n",
    "    best = result.best_prompt\n",
    "    if best:\n",
    "        return best\n",
    "    top = result.top_prompts or []\n",
    "    if top:\n",
    "        return top[0]\n",
    "    raise RuntimeError(\"No prompts returned from GEPA job\")\n",
    "\n",
    "\n",
    "baseline_prompt_obj = _select_prompt(baseline_prompts)\n",
    "optimized_prompt_obj = _select_prompt(optimized_prompts)\n",
    "\n",
    "print(\"\\nBaseline best score:\", baseline_prompts.best_score)\n",
    "print(\"Optimized best score:\", optimized_prompts.best_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Step 6: Heldout Evaluation (4 Final Scores)\n",
    "\n",
    "We evaluate both best prompts on a heldout set using **eval jobs** and both verifiers:\n",
    "\n",
    "1. Baseline prompt + baseline verifier\n",
    "2. Baseline prompt + optimized verifier\n",
    "3. Optimized prompt + baseline verifier\n",
    "4. Optimized prompt + optimized verifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Heldout evaluation via eval jobs\n",
    "\n",
    "HELDOUT_SEEDS = [20, 21, 22, 23]\n",
    "EVAL_POLICY_MODEL = \"gpt-4.1-nano\"\n",
    "\n",
    "\n",
    "def _extract_prompt_sections(prompt_obj: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    template = prompt_obj.get(\"template\") or {}\n",
    "    sections = template.get(\"sections\")\n",
    "    if isinstance(sections, list) and sections:\n",
    "        return sections\n",
    "    messages = prompt_obj.get(\"messages\")\n",
    "    if isinstance(messages, list) and messages:\n",
    "        return messages\n",
    "    return []\n",
    "\n",
    "\n",
    "async def run_eval_job(\n",
    "    label: str, prompt_sections: List[Dict[str, Any]], verifier_job_id: str\n",
    ") -> float:\n",
    "    config = EvalJobConfig(\n",
    "        task_app_url=TASK_APP_URL,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        api_key=API_KEY,\n",
    "        task_app_api_key=ENVIRONMENT_API_KEY,\n",
    "        env_name=\"style-matching\",\n",
    "        seeds=HELDOUT_SEEDS,\n",
    "        policy_config={\"model\": EVAL_POLICY_MODEL, \"provider\": \"openai\"},\n",
    "        env_config={\n",
    "            \"prompt_sections\": prompt_sections,\n",
    "            \"verifier_job_id\": verifier_job_id,\n",
    "        },\n",
    "        concurrency=3,\n",
    "    )\n",
    "\n",
    "    job = EvalJob(config)\n",
    "    job_id = job.submit()\n",
    "    print(f\"Eval job ({label}): {job_id}\")\n",
    "\n",
    "    result = job.poll_until_complete(timeout=1800.0, interval=3.0, progress=True)\n",
    "    if not result.succeeded:\n",
    "        raise RuntimeError(f\"Eval job failed ({label}): {result.error}\")\n",
    "    return result.mean_score or 0.0\n",
    "\n",
    "\n",
    "baseline_sections = _extract_prompt_sections(baseline_prompt_obj)\n",
    "optimized_sections = _extract_prompt_sections(optimized_prompt_obj)\n",
    "\n",
    "results = {}\n",
    "results[\"baseline_prompt__baseline_verifier\"] = await run_eval_job(\n",
    "    \"baseline_prompt__baseline_verifier\", baseline_sections, BASELINE_VERIFIER_JOB_ID\n",
    ")\n",
    "results[\"baseline_prompt__optimized_verifier\"] = await run_eval_job(\n",
    "    \"baseline_prompt__optimized_verifier\", baseline_sections, OPTIMIZED_VERIFIER_JOB_ID\n",
    ")\n",
    "results[\"optimized_prompt__baseline_verifier\"] = await run_eval_job(\n",
    "    \"optimized_prompt__baseline_verifier\", optimized_sections, BASELINE_VERIFIER_JOB_ID\n",
    ")\n",
    "results[\"optimized_prompt__optimized_verifier\"] = await run_eval_job(\n",
    "    \"optimized_prompt__optimized_verifier\", optimized_sections, OPTIMIZED_VERIFIER_JOB_ID\n",
    ")\n",
    "\n",
    "print(\"\\nHeldout results (mean verifier score):\")\n",
    "for k, v in results.items():\n",
    "    print(f\"- {k}: {v:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "print(\"Cleaning up tunnels...\")\n",
    "cleanup_all()\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
