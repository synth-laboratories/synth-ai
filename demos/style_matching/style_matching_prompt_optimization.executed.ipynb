{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e28c78",
   "metadata": {},
   "source": [
    "# Synth GEPA Demo - Style Matching\n",
    "\n",
    "This notebook demonstrates **style-matching prompt optimization** using Synth's GEPA algorithm.\n",
    "\n",
    "We use a small, fixed dataset of essay prompts and gold examples defined directly in this notebook.\n",
    "The task app scores outputs using a zero-shot contrastive verifier, then we **optimize a verifier graph**\n",
    "(via Graph Evolve) and compare results across baseline vs optimized verifiers.\n",
    "\n",
    "**Run in Google Colab:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/synth-laboratories/synth-ai/blob/main/demos/style_matching/style_matching_prompt_optimization.ipynb)\n",
    "\n",
    "**Structure:**\n",
    "1. **Setup** - Install dependencies and configure API keys\n",
    "2. **Dataset + Task App** - Define a fixed dataset and a local task app\n",
    "3. **Optimize Verifier** - Evolve a verifier graph using graph-evolve jobs\n",
    "4. **Local Server + Tunnel** - Expose the task app to the Synth backend\n",
    "5. **Optimize Prompts** - Run GEPA with baseline vs optimized verifiers\n",
    "6. **Heldout Evaluation** - Score four combinations on a heldout set using eval jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6feb6169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T07:42:40.155407Z",
     "iopub.status.busy": "2026-01-05T07:42:40.155112Z",
     "iopub.status.idle": "2026-01-05T07:42:40.162720Z",
     "shell.execute_reply": "2026-01-05T07:42:40.162263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in Colab - assuming dependencies are already installed\n",
      "Required: pip install synth-ai httpx fastapi uvicorn nest_asyncio\n",
      "Required: brew install cloudflare/cloudflare/cloudflared (macOS)\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Install dependencies (run this first on Colab)\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab - installing dependencies...\")\n",
    "    !pip install -q synth-ai httpx fastapi uvicorn nest_asyncio\n",
    "\n",
    "    # Install cloudflared\n",
    "    !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O /usr/local/bin/cloudflared\n",
    "    !chmod +x /usr/local/bin/cloudflared\n",
    "    !cloudflared --version\n",
    "\n",
    "    print(\"Dependencies installed!\")\n",
    "else:\n",
    "    print(\"Not in Colab - assuming dependencies are already installed\")\n",
    "    print(\"Required: pip install synth-ai httpx fastapi uvicorn nest_asyncio\")\n",
    "    print(\"Required: brew install cloudflare/cloudflare/cloudflared (macOS)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4664db4",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Configure all imports, API keys, and environment keys in one place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a59666a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T07:42:40.165349Z",
     "iopub.status.busy": "2026-01-05T07:42:40.165195Z",
     "iopub.status.idle": "2026-01-05T07:42:45.789113Z",
     "shell.execute_reply": "2026-01-05T07:42:45.788608Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshpurtell/Documents/GitHub/synth-ai/synth_ai/sdk/task/contracts.py:34: UserWarning: Field name \"schema\" in \"StructuredOutputConfig\" shadows an attribute in parent \"BaseModel\"\n",
      "  class StructuredOutputConfig(BaseModel):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[celery_app] EXPERIMENT_QUEUE_DB_PATH not set, will use default path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[celery_app] Using default database path: /Users/joshpurtell/.synth_ai/experiment_queue.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[celery_app] Initializing with database: /Users/joshpurtell/.synth_ai/experiment_queue.db (broker: redis://localhost:6379/0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend: http://localhost:8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend health: {'status': 'ok', 'database': 'connected', 'details': {}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using SYNTH_API_KEY:\n",
      "sk_live_ace8b968-a52...\n",
      "\n",
      "Minted env key:\n",
      "fe0fd73b799f...d7fa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded env key: {'stored': True, 'id': '387be31a-01c0-41a1-8aa6-3e12aa2d387d', 'name': 'ENVIRONMENT_API_KEY', 'updated_at': '2026-01-05T07:42:45.011851Z'}\n",
      "\n",
      "==================================================\n",
      "SETUP COMPLETE\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setup - All imports, config, and API keys\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import httpx\n",
    "import nest_asyncio\n",
    "from fastapi import FastAPI, Header, HTTPException, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from synth_ai.sdk.api.eval import EvalJob, EvalJobConfig\n",
    "from synth_ai.sdk.api.train.prompt_learning import PromptLearningJob\n",
    "from synth_ai.sdk.learning.prompt_learning_client import PromptLearningClient\n",
    "from synth_ai.sdk.learning.rl import mint_environment_api_key, setup_environment_api_key\n",
    "from synth_ai.sdk.task import run_server_background\n",
    "from synth_ai.sdk.tunnels import TunneledLocalAPI, TunnelBackend, kill_port, wait_for_health_check, cleanup_all\n",
    "\n",
    "from synth_ai.products.graph_evolve import GraphOptimizationClient, GraphOptimizationConfig\n",
    "from synth_ai.products.graph_evolve.config import EvolutionConfig, SeedsConfig, LimitsConfig, ProposerConfig\n",
    "\n",
    "def _load_env_file(path: str) -> None:\n",
    "    env_path = Path(path)\n",
    "    if not env_path.exists():\n",
    "        return\n",
    "    for line in env_path.read_text().splitlines():\n",
    "        if not line or line.lstrip().startswith(\"#\") or \"=\" not in line:\n",
    "            continue\n",
    "        key, value = line.split(\"=\", 1)\n",
    "        key = key.strip()\n",
    "        value = value.strip().strip(\"\\\"\").strip(\"\\'\")\n",
    "        if key and key not in os.environ:\n",
    "            os.environ[key] = value\n",
    "\n",
    "_load_env_file(\"synth-ai/.env\")\n",
    "\n",
    "USE_LOCAL_BACKEND = True\n",
    "SYNTH_API_BASE = \"http://localhost:8000\" if USE_LOCAL_BACKEND else \"https://api.usesynth.ai\"\n",
    "os.environ[\"BACKEND_BASE_URL\"] = SYNTH_API_BASE\n",
    "\n",
    "def _get_org_id() -> str:\n",
    "    \n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "    urls = [f\"{SYNTH_API_BASE}/api/v1/me\", f\"{SYNTH_API_BASE}/me\"]\n",
    "    for url in urls:\n",
    "        resp = httpx.get(url, headers=headers, timeout=30)\n",
    "        if resp.status_code == 404:\n",
    "            continue\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        org_id = data.get(\"org_id\") or data.get(\"orgId\")\n",
    "        if org_id:\n",
    "            return str(org_id)\n",
    "    raise RuntimeError(\"Unable to resolve org_id from /api/v1/me or /me\")\n",
    "LOCAL_TASK_PORT = 8132\n",
    "\n",
    "def _validate_api_key(api_key: str) -> bool:\n",
    "    if not api_key:\n",
    "        return False\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    try:\n",
    "        resp = httpx.get(f\"{SYNTH_API_BASE}/api/v1/me\", headers=headers, timeout=10)\n",
    "    except Exception:\n",
    "        return False\n",
    "    return resp.status_code == 200\n",
    "\n",
    "print(f\"Backend: {SYNTH_API_BASE}\")\n",
    "\n",
    "# Check backend health\n",
    "r = httpx.get(f\"{SYNTH_API_BASE}/health\", timeout=30)\n",
    "if r.status_code != 200:\n",
    "    raise RuntimeError(f\"Backend not healthy: status {r.status_code}\")\n",
    "print(f\"Backend health: {r.json()}\")\n",
    "\n",
    "# Get API Key (use env var or mint demo key)\n",
    "API_KEY = os.environ.get(\"SYNTH_API_KEY\", \"\").strip()\n",
    "if not API_KEY or not _validate_api_key(API_KEY):\n",
    "    print(\"\")\n",
    "    print(\"SYNTH_API_KEY missing or invalid for this backend; minting demo key...\")\n",
    "    resp = httpx.post(f\"{SYNTH_API_BASE}/api/demo/keys\", json={\"ttl_hours\": 4}, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    API_KEY = resp.json()[\"api_key\"]\n",
    "    print(f\"Demo API Key: {API_KEY[:25]}...\")\n",
    "else:\n",
    "    print(\"\")\n",
    "    print(\"Using SYNTH_API_KEY:\")\n",
    "    print(f\"{API_KEY[:20]}...\")\n",
    "\n",
    "# Set API key in environment for SDK to use\n",
    "os.environ['SYNTH_API_KEY'] = API_KEY\n",
    "\n",
    "# Mint and upload environment key (for local API authentication)\n",
    "ENVIRONMENT_API_KEY = mint_environment_api_key()\n",
    "print(\"\")\n",
    "print(\"Minted env key:\")\n",
    "print(f\"{ENVIRONMENT_API_KEY[:12]}...{ENVIRONMENT_API_KEY[-4:]}\")\n",
    "\n",
    "result = setup_environment_api_key(SYNTH_API_BASE, API_KEY, token=ENVIRONMENT_API_KEY)\n",
    "print(f\"Uploaded env key: {result}\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 50)\n",
    "print(\"SETUP COMPLETE\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a302a",
   "metadata": {},
   "source": [
    "## Step 2: Dataset + Task App\n",
    "\n",
    "We define a fixed dataset of essay prompts and gold examples. The task app generates essays using the\n",
    "candidate prompt and scores outputs with the zero-shot contrastive verifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9faaa16e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T07:42:45.792145Z",
     "iopub.status.busy": "2026-01-05T07:42:45.791815Z",
     "iopub.status.idle": "2026-01-05T07:42:45.829447Z",
     "shell.execute_reply": "2026-01-05T07:42:45.828955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task dataset size: 4\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Fixed dataset + task app definition\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a thoughtful essayist with a direct, builder-first voice. \"\n",
    "    \"Write crisp, opinionated essays with concrete examples, minimal fluff, \"\n",
    "    \"and a short, memorable closing line. Aim for ~500 words (roughly 450-650).\"\n",
    ")\n",
    "\n",
    "USER_PROMPT_TEMPLATE = (\n",
    "    \"Title: {title}\\n\"\n",
    "    \"Outline:\\n{outline}\\n\\n\"\n",
    "    \"Notes:\\n{notes}\\n\\n\"\n",
    "    \"Target length: ~500 words.\\n\\n\"\n",
    "    \"Write the essay now.\"\n",
    ")\n",
    "\n",
    "TASKS: List[Dict[str, Any]] = [\n",
    "    {\n",
    "        \"id\": \"outline_speed\",\n",
    "        \"input\": {\n",
    "            \"title\": \"Shipping Fast Without Burning Out\",\n",
    "            \"outline\": (\n",
    "                \"1. Why speed compounds learning\\n\"\n",
    "                \"2. The tradeoff between velocity and quality\\n\"\n",
    "                \"3. How to keep teams aligned under pressure\\n\"\n",
    "                \"4. Practical rituals that preserve momentum\\n\"\n",
    "                \"5. Closing: pace as a competitive advantage\"\n",
    "            ),\n",
    "            \"notes\": [\"short feedback loops\", \"protect maker time\", \"use small bets\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"outline_focus\",\n",
    "        \"input\": {\n",
    "            \"title\": \"Focus Beats Optionality\",\n",
    "            \"outline\": (\n",
    "                \"1. Optionality feels safe but slows decisions\\n\"\n",
    "                \"2. Focus creates a compounding advantage\\n\"\n",
    "                \"3. The cost of context switching in small teams\\n\"\n",
    "                \"4. Saying no as a leadership skill\\n\"\n",
    "                \"5. Closing: clarity is leverage\"\n",
    "            ),\n",
    "            \"notes\": [\"pick one wedge\", \"eliminate parallel bets\", \"repeat a simple story\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"outline_learning\",\n",
    "        \"input\": {\n",
    "            \"title\": \"Learning in Public\",\n",
    "            \"outline\": (\n",
    "                \"1. Writing as a forcing function\\n\"\n",
    "                \"2. How sharing drafts accelerates feedback\\n\"\n",
    "                \"3. The credibility flywheel\\n\"\n",
    "                \"4. Risks of over-sharing (and how to avoid them)\\n\"\n",
    "                \"5. Closing: publish to clarify\"\n",
    "            ),\n",
    "            \"notes\": [\"ship drafts, not polished essays\", \"ask for specific feedback\", \"be concrete about failures\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"outline_quality\",\n",
    "        \"input\": {\n",
    "            \"title\": \"Quality as a Constraint\",\n",
    "            \"outline\": (\n",
    "                \"1. The myth that quality slows you down\\n\"\n",
    "                \"2. Cheap fixes vs durable systems\\n\"\n",
    "                \"3. When to accept rough edges\\n\"\n",
    "                \"4. How to build taste in a team\\n\"\n",
    "                \"5. Closing: quality is a habit\"\n",
    "            ),\n",
    "            \"notes\": [\"use guardrails\", \"invest in tooling\", \"make quality visible\"],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "GOLD_EXAMPLES: List[Dict[str, Any]] = [\n",
    "    {\n",
    "        \"summary\": \"Direct, builder tone with concrete examples\",\n",
    "        \"gold_score\": 0.95,\n",
    "        \"gold_reasoning\": \"Short sentences, decisive stance, concrete advice, crisp closing line.\",\n",
    "        \"trace\": {\n",
    "            \"session_id\": \"gold-1\",\n",
    "            \"session_time_steps\": [\n",
    "                {\n",
    "                    \"step_id\": \"1\",\n",
    "                    \"step_index\": 0,\n",
    "                    \"events\": [\n",
    "                        {\"event_type\": \"runtime\", \"event_id\": 1, \"type\": \"user_message\", \"content\": \"Write about shipping fast.\"},\n",
    "                        {\"event_type\": \"runtime\", \"event_id\": 2, \"type\": \"assistant_message\", \"content\": (\n",
    "                            \"Speed is a learning engine. Ship a small bet, watch users react, then sharpen the next move. \"\n",
    "                            \"Protect maker time, keep scope tight, and treat every release as feedback, not theater. \"\n",
    "                            \"Momentum beats motivation.\"\n",
    "                        )},\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"summary\": \"Opinionated essay with a sharp closing line\",\n",
    "        \"gold_score\": 0.92,\n",
    "        \"gold_reasoning\": \"Clear thesis, direct claims, practical advice, and a memorable end.\",\n",
    "        \"trace\": {\n",
    "            \"session_id\": \"gold-2\",\n",
    "            \"session_time_steps\": [\n",
    "                {\n",
    "                    \"step_id\": \"1\",\n",
    "                    \"step_index\": 0,\n",
    "                    \"events\": [\n",
    "                        {\"event_type\": \"runtime\", \"event_id\": 1, \"type\": \"user_message\", \"content\": \"Write about focus.\"},\n",
    "                        {\"event_type\": \"runtime\", \"event_id\": 2, \"type\": \"assistant_message\", \"content\": (\n",
    "                            \"Optionality feels safe, but it dilutes learning. Pick one wedge, cut parallel bets, and ship. \"\n",
    "                            \"Small teams win by saying no early and often. Clarity is leverage.\"\n",
    "                        )},\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"summary\": \"Concrete, tactical quality guidance with guardrails\",\n",
    "        \"gold_score\": 0.96,\n",
    "        \"gold_reasoning\": \"Direct stance, concrete guardrails, and a crisp closing line.\",\n",
    "        \"trace\": {\n",
    "            \"session_id\": \"gold-3\",\n",
    "            \"session_time_steps\": [\n",
    "                {\n",
    "                    \"step_id\": \"1\",\n",
    "                    \"step_index\": 0,\n",
    "                    \"events\": [\n",
    "                        {\"event_type\": \"runtime\", \"event_id\": 1, \"type\": \"user_message\", \"content\": \"Write about quality as a constraint.\"},\n",
    "                        {\"event_type\": \"runtime\", \"event_id\": 2, \"type\": \"assistant_message\", \"content\": (\n",
    "                            \"Quality is the guardrail that keeps speed from turning into chaos. Ship small, test the riskiest paths, \"\n",
    "                            \"and make rollback cheap. Fix root causes once, then automate the prevention. \"\n",
    "                            \"Quality is a habit, not a milestone.\"\n",
    "                        )},\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# Baseline verifier (zero-shot contrastive)\n",
    "BASELINE_VERIFIER_JOB_ID = \"zero_shot_verifier_contrastive_single\"\n",
    "VERIFIER_JOB_ID = BASELINE_VERIFIER_JOB_ID\n",
    "VERIFIER_MODEL = \"gpt-4.1-nano\"\n",
    "\n",
    "ROLLOUT_LOG: List[Dict[str, Any]] = []\n",
    "\n",
    "\n",
    "def _verify_api_key(x_api_key: Optional[str]) -> bool:\n",
    "    if not ENVIRONMENT_API_KEY:\n",
    "        return True\n",
    "    return x_api_key == ENVIRONMENT_API_KEY\n",
    "\n",
    "\n",
    "def _format_notes(notes: List[str]) -> str:\n",
    "    if not notes:\n",
    "        return \"- (none)\"\n",
    "    return \"\\n\".join(f\"- {note}\" for note in notes)\n",
    "\n",
    "\n",
    "def _safe_format(text: str, values: Dict[str, Any]) -> str:\n",
    "    class _DefaultDict(dict):\n",
    "        def __missing__(self, key: str) -> str:\n",
    "            return \"\"\n",
    "    return text.format_map(_DefaultDict(values))\n",
    "\n",
    "\n",
    "def _render_messages_from_sections(sections: List[Dict[str, Any]], values: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "    rendered = []\n",
    "    for section in sorted(sections, key=lambda s: s.get(\"order\", 0)):\n",
    "        role = section.get(\"role\", \"user\")\n",
    "        content = section.get(\"content\") or section.get(\"pattern\") or \"\"\n",
    "        if content:\n",
    "            rendered.append({\"role\": str(role), \"content\": _safe_format(str(content), values)})\n",
    "    return rendered\n",
    "\n",
    "\n",
    "def _build_messages(task_input: Dict[str, Any], prompt_sections: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, str]]:\n",
    "    notes_text = _format_notes(task_input.get(\"notes\", []))\n",
    "    values = {\n",
    "        \"title\": task_input.get(\"title\", \"\"),\n",
    "        \"outline\": task_input.get(\"outline\", \"\"),\n",
    "        \"notes\": notes_text,\n",
    "    }\n",
    "    if prompt_sections:\n",
    "        return _render_messages_from_sections(prompt_sections, values)\n",
    "\n",
    "    user_prompt = USER_PROMPT_TEMPLATE.format(\n",
    "        title=values[\"title\"],\n",
    "        outline=values[\"outline\"],\n",
    "        notes=values[\"notes\"],\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "\n",
    "def _build_inference_url(inference_url: str) -> str:\n",
    "    if \"?\" in inference_url:\n",
    "        base, query = inference_url.split(\"?\", 1)\n",
    "        return f\"{base.rstrip('/')}/chat/completions?{query}\"\n",
    "    return f\"{inference_url.rstrip('/')}/chat/completions\"\n",
    "\n",
    "\n",
    "def _extract_completion(data: Dict[str, Any]) -> str:\n",
    "    try:\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"] or \"\"\n",
    "    except (KeyError, IndexError, TypeError):\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _extract_verifier_score(result: Dict[str, Any]) -> float:\n",
    "    output = result.get(\"output\", result)\n",
    "    if isinstance(output, dict):\n",
    "        outcome_review = output.get(\"outcome_review\")\n",
    "        if isinstance(outcome_review, dict) and isinstance(outcome_review.get(\"total\"), (int, float)):\n",
    "            return float(outcome_review[\"total\"])\n",
    "        event_reviews = output.get(\"event_reviews\")\n",
    "        if isinstance(event_reviews, list) and event_reviews:\n",
    "            totals = [rev.get(\"total\") for rev in event_reviews if isinstance(rev, dict)]\n",
    "            totals = [t for t in totals if isinstance(t, (int, float))]\n",
    "            if totals:\n",
    "                return float(sum(totals) / len(totals))\n",
    "        if isinstance(output.get(\"total\"), (int, float)):\n",
    "            return float(output[\"total\"])\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "async def _call_policy_llm(messages: List[Dict[str, str]], policy_config: Dict[str, Any]) -> str:\n",
    "    inference_url = policy_config.get(\"inference_url\")\n",
    "    if not inference_url:\n",
    "        raise RuntimeError(\"policy.config.inference_url is required\")\n",
    "\n",
    "    url = _build_inference_url(inference_url)\n",
    "    model = policy_config.get(\"model\", \"gpt-4.1-nano\")\n",
    "    model_lower = model.lower()\n",
    "    is_gpt5 = \"gpt-5\" in model_lower\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    api_key = policy_config.get(\"api_key\")\n",
    "    if api_key:\n",
    "        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "    elif ENVIRONMENT_API_KEY:\n",
    "        headers[\"X-API-Key\"] = ENVIRONMENT_API_KEY\n",
    "        headers[\"Authorization\"] = f\"Bearer {ENVIRONMENT_API_KEY}\"\n",
    "\n",
    "    payload = {\"model\": model, \"messages\": messages}\n",
    "    if is_gpt5:\n",
    "        payload[\"max_completion_tokens\"] = int(policy_config.get(\"max_completion_tokens\", 16000))\n",
    "        reasoning_effort = policy_config.get(\"reasoning_effort\")\n",
    "        if reasoning_effort:\n",
    "            payload[\"reasoning_effort\"] = reasoning_effort\n",
    "    else:\n",
    "        payload[\"temperature\"] = float(policy_config.get(\"temperature\", 0.7))\n",
    "        payload[\"max_tokens\"] = int(policy_config.get(\"max_completion_tokens\", 1200))\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=120.0) as client:\n",
    "        response = await client.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return _extract_completion(response.json())\n",
    "\n",
    "\n",
    "async def _score_with_verifier(session_trace: Dict[str, Any], verifier_job_id: Optional[str] = None) -> float:\n",
    "    job_id = verifier_job_id or VERIFIER_JOB_ID\n",
    "    payload = {\n",
    "        \"job_id\": job_id,\n",
    "        \"input\": {\n",
    "            \"trace\": session_trace,\n",
    "            \"gold_examples\": GOLD_EXAMPLES,\n",
    "            \"candidate_score\": 0.5,\n",
    "            \"candidate_reasoning\": \"Auto-evaluated from style-matching task app\",\n",
    "            \"options\": {\"model\": VERIFIER_MODEL},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=120.0) as client:\n",
    "        response = await client.post(\n",
    "            f\"{SYNTH_API_BASE.rstrip('/')}/api/graphs/completions\",\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(\n",
    "                f\"Verifier failed: HTTP {response.status_code} {response.text[:500]}\"\n",
    "            )\n",
    "        result = response.json()\n",
    "\n",
    "    return _extract_verifier_score(result)\n",
    "\n",
    "\n",
    "app = FastAPI(title=\"Style Matching Task App\")\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health() -> Dict[str, str]:\n",
    "    return {\"status\": \"ok\", \"task_app\": \"style_matching\"}\n",
    "\n",
    "\n",
    "\n",
    "@app.get(\"/task_info\")\n",
    "async def task_info() -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"taskset\": {\n",
    "            \"name\": \"style_matching\",\n",
    "            \"description\": \"Style matching demo task app\",\n",
    "            \"size\": len(TASKS),\n",
    "        }\n",
    "    }\n",
    "@app.get(\"/tasks\")\n",
    "async def list_tasks() -> Dict[str, Any]:\n",
    "    return {\"tasks\": TASKS, \"gold_examples\": GOLD_EXAMPLES}\n",
    "\n",
    "\n",
    "@app.get(\"/rollouts\")\n",
    "async def list_rollouts(x_api_key: Optional[str] = Header(None)) -> Dict[str, Any]:\n",
    "    if not _verify_api_key(x_api_key):\n",
    "        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n",
    "    return {\"rollouts\": ROLLOUT_LOG}\n",
    "\n",
    "\n",
    "@app.post(\"/rollout\")\n",
    "async def rollout(request: Request, x_api_key: Optional[str] = Header(None)) -> Dict[str, Any]:\n",
    "    if not _verify_api_key(x_api_key):\n",
    "        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n",
    "\n",
    "    try:\n",
    "        data = await request.json()\n",
    "    except Exception:\n",
    "        raise HTTPException(status_code=400, detail=\"Invalid JSON\")\n",
    "\n",
    "    run_id = data.get(\"run_id\")\n",
    "    env = data.get(\"env\", {})\n",
    "    policy = data.get(\"policy\", {})\n",
    "    policy_config = policy.get(\"config\", {})\n",
    "\n",
    "    trace_correlation_id = policy_config.get(\"trace_correlation_id\")\n",
    "\n",
    "    env_config = env.get(\"config\", {}) or {}\n",
    "    verifier_job_id = env_config.get(\"verifier_job_id\") or VERIFIER_JOB_ID\n",
    "    prompt_sections = env_config.get(\"prompt_sections\")\n",
    "\n",
    "    seed = int(env.get(\"seed\", 0))\n",
    "    task = TASKS[seed % len(TASKS)]\n",
    "    task_input = task[\"input\"]\n",
    "\n",
    "    messages = _build_messages(task_input, prompt_sections=prompt_sections)\n",
    "\n",
    "    try:\n",
    "        essay = await _call_policy_llm(messages, policy_config)\n",
    "    except Exception as exc:\n",
    "        essay = f\"[error: {exc}]\"\n",
    "\n",
    "    session_trace = {\n",
    "        \"session_id\": f\"style-matching-{task['id']}\",\n",
    "        \"session_time_steps\": [\n",
    "            {\n",
    "                \"step_id\": \"1\",\n",
    "                \"step_index\": 0,\n",
    "                \"events\": [\n",
    "                    {\n",
    "                        \"event_type\": \"runtime\",\n",
    "                        \"event_id\": 1,\n",
    "                        \"type\": \"user_message\",\n",
    "                        \"content\": messages[-1][\"content\"],\n",
    "                    },\n",
    "                    {\n",
    "                        \"event_type\": \"runtime\",\n",
    "                        \"event_id\": 2,\n",
    "                        \"type\": \"assistant_message\",\n",
    "                        \"content\": essay,\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    score = await _score_with_verifier(session_trace, verifier_job_id=verifier_job_id)\n",
    "\n",
    "    ROLLOUT_LOG.append(\n",
    "        {\n",
    "            \"run_id\": run_id,\n",
    "            \"seed\": seed,\n",
    "            \"task_id\": task[\"id\"],\n",
    "            \"title\": task_input.get(\"title\", \"\"),\n",
    "            \"essay\": essay,\n",
    "            \"score\": score,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"metrics\": {\n",
    "            \"mean_return\": score,\n",
    "            \"outcome_score\": score,\n",
    "            \"num_steps\": 1,\n",
    "            \"details\": {\"verifier_score\": score},\n",
    "        },\n",
    "        \"trajectories\": [\n",
    "            {\n",
    "                \"steps\": [\n",
    "                    {\"observation\": task_input, \"action\": {\"essay\": essay}, \"reward\": score}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"metadata\": {\"task_id\": task[\"id\"]},\n",
    "        \"trace_correlation_id\": trace_correlation_id or \"\",\n",
    "    }\n",
    "\n",
    "print(f\"Task dataset size: {len(TASKS)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7db7157",
   "metadata": {},
   "source": [
    "## Step 3: Optimize Verifier Graph\n",
    "\n",
    "We use Graph Evolve to optimize a verifier graph on a small, fixed calibration set.\n",
    "This graph will serve as an alternative verifier in the GEPA loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a3eb63b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T07:42:45.830859Z",
     "iopub.status.busy": "2026-01-05T07:42:45.830769Z",
     "iopub.status.idle": "2026-01-05T07:45:59.021956Z",
     "shell.execute_reply": "2026-01-05T07:45:59.020943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved org_id: e77ef3a8-677d-4ddd-92d6-0f114d6bbdaf\n",
      "Graph evolve job: graph_evolve_6b8f54a422b9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifier optimization complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized verifier job id: graph_evolve_6b8f54a422b9\n",
      "Optimized verifier graph id: 685e5e97-f252-4595-97ae-a4e14f140d84\n",
      "Best score: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Optimize a verifier graph with Graph Evolve\n",
    "\n",
    "\n",
    "def _make_trace(user_text: str, assistant_text: str) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"session_id\": \"trace\",\n",
    "        \"session_time_steps\": [\n",
    "            {\n",
    "                \"step_id\": \"1\",\n",
    "                \"step_index\": 0,\n",
    "                \"events\": [\n",
    "                    {\n",
    "                        \"event_type\": \"runtime\",\n",
    "                        \"event_id\": 1,\n",
    "                        \"type\": \"user_message\",\n",
    "                        \"content\": user_text,\n",
    "                    },\n",
    "                    {\n",
    "                        \"event_type\": \"runtime\",\n",
    "                        \"event_id\": 2,\n",
    "                        \"type\": \"assistant_message\",\n",
    "                        \"content\": assistant_text,\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "VERIFIER_EXAMPLES = [\n",
    "    {\n",
    "        \"task_id\": \"good_speed\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about shipping fast.\",\n",
    "            \"Speed compounds learning. Ship small bets, learn fast, keep scope tight, and protect maker time.\",\n",
    "        ),\n",
    "        \"score\": 0.95,\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"good_focus\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about focus.\",\n",
    "            \"Optionality dilutes learning. Pick one wedge, cut parallel bets, and repeat a simple story.\",\n",
    "        ),\n",
    "        \"score\": 0.92,\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"good_quality\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about quality as a constraint.\",\n",
    "            \"Quality is a guardrail. Ship small, test risky paths, and make rollback cheap.\",\n",
    "        ),\n",
    "        \"score\": 0.93,\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"good_learning\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about learning in public.\",\n",
    "            \"Publish drafts to accelerate feedback, build credibility, and clarify thinking.\",\n",
    "        ),\n",
    "        \"score\": 0.91,\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"bad_rambling\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about focus.\",\n",
    "            \"Focus is important because focus is important. You should focus on focusing and focus on focus.\",\n",
    "        ),\n",
    "        \"score\": 0.10,\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"bad_vague\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about quality.\",\n",
    "            \"Quality is good. Teams should be good and do good things to make quality good.\",\n",
    "        ),\n",
    "        \"score\": 0.05,\n",
    "    },\n",
    "]\n",
    "\n",
    "VERIFIER_TRAIN_SEEDS = list(range(4))\n",
    "VERIFIER_VAL_SEEDS = list(range(4, len(VERIFIER_EXAMPLES)))\n",
    "\n",
    "verifier_dataset = {\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_id\": example[\"task_id\"],\n",
    "            \"input\": {\"trace\": example[\"trace\"], \"gold_examples\": GOLD_EXAMPLES},\n",
    "        }\n",
    "        for example in VERIFIER_EXAMPLES\n",
    "    ],\n",
    "    \"gold_outputs\": [\n",
    "        {\"task_id\": example[\"task_id\"], \"output\": {}, \"score\": example[\"score\"]}\n",
    "        for example in VERIFIER_EXAMPLES\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"name\": \"style_matching_verifier\",\n",
    "        \"task_description\": \"Score essay outputs for style-matching quality (0-1).\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"trace\": {\"type\": \"object\"},\n",
    "                \"gold_examples\": {\"type\": \"array\"}\n",
    "            },\n",
    "            \"required\": [\"trace\", \"gold_examples\"]\n",
    "        },\n",
    "        \"output_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"score\": {\"type\": \"number\"}},\n",
    "            \"required\": [\"score\"]\n",
    "        },\n",
    "        \"domain\": \"text\",\n",
    "    },\n",
    "}\n",
    "\n",
    "verifier_config = GraphOptimizationConfig(\n",
    "    algorithm=\"graph_evolve\",\n",
    "    dataset_name=\"style_matching_verifier\",\n",
    "    graph_type=\"verifier\",\n",
    "    graph_structure=\"single_prompt\",\n",
    "    allowed_policy_models=[\"gpt-4.1-nano\", \"gpt-4o-mini\"],\n",
    "    evolution=EvolutionConfig(num_generations=3, children_per_generation=2),\n",
    "    proposer=ProposerConfig(model=\"gpt-4.1\"),\n",
    "    seeds=SeedsConfig(train=VERIFIER_TRAIN_SEEDS, validation=VERIFIER_VAL_SEEDS),\n",
    "    limits=LimitsConfig(max_spend_usd=5.0, timeout_seconds=3600),\n",
    "    verifier_mode=\"contrastive\",\n",
    "    verifier_model=VERIFIER_MODEL,\n",
    "    dataset=verifier_dataset,\n",
    "    task_description=\"Score session traces for style-matching quality.\",\n",
    ")\n",
    "\n",
    "\n",
    "async def run_verifier_optimization():\n",
    "    async with GraphOptimizationClient(SYNTH_API_BASE, API_KEY) as client:\n",
    "        job_id = await client.start_job(verifier_config)\n",
    "        print(f\"Graph evolve job: {job_id}\")\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=60.0) as http:\n",
    "        headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "        for _ in range(120):\n",
    "            try:\n",
    "                status_resp = await http.get(\n",
    "                    f\"{SYNTH_API_BASE}/graph-evolve/jobs/{job_id}/status\", headers=headers\n",
    "                )\n",
    "                if status_resp.status_code == 404:\n",
    "                    await asyncio.sleep(2.0)\n",
    "                    continue\n",
    "                status_resp.raise_for_status()\n",
    "                status = status_resp.json().get(\"status\")\n",
    "                if status in {\"completed\", \"failed\", \"cancelled\"}:\n",
    "                    result_resp = await http.get(\n",
    "                        f\"{SYNTH_API_BASE}/graph-evolve/jobs/{job_id}/result\", headers=headers\n",
    "                    )\n",
    "                    result_resp.raise_for_status()\n",
    "                    return job_id, result_resp.json()\n",
    "            except httpx.HTTPStatusError:\n",
    "                await asyncio.sleep(2.0)\n",
    "                continue\n",
    "            await asyncio.sleep(2.0)\n",
    "\n",
    "    raise RuntimeError(f\"Graph evolve job {job_id} did not complete in time\")\n",
    "\n",
    "\n",
    "\n",
    "ORG_ID = _get_org_id()\n",
    "print(f\"Resolved org_id: {ORG_ID}\")\n",
    "\n",
    "async def save_verifier_graph(job_id: str) -> str:\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"name\": f\"style-matching-verifier-{job_id[:8]}\",\n",
    "        \"org_id\": ORG_ID,\n",
    "        \"kind\": \"verifier\",\n",
    "    }\n",
    "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "        resp = await client.post(\n",
    "            f\"{SYNTH_API_BASE}/graph-evolve/jobs/{job_id}/save-graph\",\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "        )\n",
    "        if resp.status_code >= 400:\n",
    "            raise RuntimeError(f\"save-graph failed ({resp.status_code}): {resp.text[:500]}\")\n",
    "        data = resp.json()\n",
    "    graph_id = data.get(\"graph_id\") or data.get(\"id\")\n",
    "    if not graph_id:\n",
    "        raise RuntimeError(f\"save-graph did not return graph_id: {data}\")\n",
    "    return str(graph_id)\n",
    "OPTIMIZED_VERIFIER_JOB_ID, OPTIMIZED_VERIFIER_RESULT = await run_verifier_optimization()\n",
    "status = OPTIMIZED_VERIFIER_RESULT.get(\"status\")\n",
    "if status != \"completed\":\n",
    "    error_msg = OPTIMIZED_VERIFIER_RESULT.get(\"error\") or \"unknown error\"\n",
    "    raise RuntimeError(f\"Graph evolve job failed with status={status}: {error_msg}\")\n",
    "print(\"\\nVerifier optimization complete\")\n",
    "OPTIMIZED_VERIFIER_GRAPH_ID = await save_verifier_graph(OPTIMIZED_VERIFIER_JOB_ID)\n",
    "print(f\"Optimized verifier job id: {OPTIMIZED_VERIFIER_JOB_ID}\")\n",
    "print(f\"Optimized verifier graph id: {OPTIMIZED_VERIFIER_GRAPH_ID}\")\n",
    "print(f\"Best score: {OPTIMIZED_VERIFIER_RESULT.get('best_score')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80166d77",
   "metadata": {},
   "source": [
    "## Step 4: Start Task App + Tunnel\n",
    "\n",
    "Run the task app locally, then create a tunnel so the Synth backend can call it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6fc1fa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T07:45:59.028055Z",
     "iopub.status.busy": "2026-01-05T07:45:59.027795Z",
     "iopub.status.idle": "2026-01-05T07:45:59.594863Z",
     "shell.execute_reply": "2026-01-05T07:45:59.594258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task app on port 8132...\n",
      "Waiting for task app health check...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task app ready!\n",
      "Using localhost task app URL (no tunnel)\n",
      "Task app URL: http://localhost:8132\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Start task app and create tunnel\n",
    "\n",
    "import asyncio\n",
    "import socket\n",
    "import time\n",
    "\n",
    "\n",
    "async def wait_for_system_dns(hostname: str, timeout: float = 90.0, interval: float = 3.0) -> None:\n",
    "    deadline = time.time() + timeout\n",
    "    last_exc = None\n",
    "    while time.time() < deadline:\n",
    "        try:\n",
    "            socket.gethostbyname(hostname)\n",
    "            return\n",
    "        except Exception as exc:\n",
    "            last_exc = exc\n",
    "            await asyncio.sleep(interval)\n",
    "    raise RuntimeError(f\"System DNS did not resolve {hostname} within {timeout}s: {last_exc}\")\n",
    "\n",
    "\n",
    "print(f\"Starting task app on port {LOCAL_TASK_PORT}...\")\n",
    "kill_port(LOCAL_TASK_PORT)\n",
    "run_server_background(app, LOCAL_TASK_PORT)\n",
    "\n",
    "print(\"Waiting for task app health check...\")\n",
    "await wait_for_health_check(\"localhost\", LOCAL_TASK_PORT, ENVIRONMENT_API_KEY, timeout=30.0)\n",
    "print(\"Task app ready!\")\n",
    "\n",
    "if USE_LOCAL_BACKEND:\n",
    "    print(\"Using localhost task app URL (no tunnel)\")\n",
    "    tunnel = await TunneledLocalAPI.create(\n",
    "        local_port=LOCAL_TASK_PORT,\n",
    "        backend=TunnelBackend.Localhost,\n",
    "    )\n",
    "else:\n",
    "    print(\"\")\n",
    "    print(\"Provisioning Cloudflare tunnel...\")\n",
    "    try:\n",
    "        tunnel = await TunneledLocalAPI.create(\n",
    "            local_port=LOCAL_TASK_PORT,\n",
    "            backend=TunnelBackend.CloudflareManagedTunnel,\n",
    "            api_key=API_KEY,\n",
    "            env_api_key=ENVIRONMENT_API_KEY,\n",
    "            backend_url=SYNTH_API_BASE,\n",
    "            reason=\"style_matching_notebook\",\n",
    "            progress=True,\n",
    "        )\n",
    "        print(f\"Waiting for system DNS to resolve {tunnel.hostname}...\")\n",
    "        await wait_for_system_dns(tunnel.hostname)\n",
    "    except Exception as exc:\n",
    "        print(f\"Managed tunnel failed or DNS unresolved ({exc}). Falling back to quick tunnel...\")\n",
    "        tunnel = await TunneledLocalAPI.create(\n",
    "            local_port=LOCAL_TASK_PORT,\n",
    "            backend=TunnelBackend.CloudflareQuickTunnel,\n",
    "            env_api_key=ENVIRONMENT_API_KEY,\n",
    "            progress=True,\n",
    "        )\n",
    "\n",
    "TASK_APP_URL = tunnel.url\n",
    "print(f\"Task app URL: {TASK_APP_URL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59279a1d",
   "metadata": {},
   "source": [
    "## Step 5: Run GEPA With Baseline vs Optimized Verifiers\n",
    "\n",
    "We run GEPA twice: once with the baseline verifier and once with the optimized verifier graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7107309b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T07:45:59.597988Z",
     "iopub.status.busy": "2026-01-05T07:45:59.597802Z",
     "iopub.status.idle": "2026-01-05T07:51:25.517026Z",
     "shell.execute_reply": "2026-01-05T07:51:25.516523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running GEPA (baseline) with verifier: zero_shot_verifier_contrastive_single\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEPA job id (baseline): pl_16e03c16e54642ea\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00] queued | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:03] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:06] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:09] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:13] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:16] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:19] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:22] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:25] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:29] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:32] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:35] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:38] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:42] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:45] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:48] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:51] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:55] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:58] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:01] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:04] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:08] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:11] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:14] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:17] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:20] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:24] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:27] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:37] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:40] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:43] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:46] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:49] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:53] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:56] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:59] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:03] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:06] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:09] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:12] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:15] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:19] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:22] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:25] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:28] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:32] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:35] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:38] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:41] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:45] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:48] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:51] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:55] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:58] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:01] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:04] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:08] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:11] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:14] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:17] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:21] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:24] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:27] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:30] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:34] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:37] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:40] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:43] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:47] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:50] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:53] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:56] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:00] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:03] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:06] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:09] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:12] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:15] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:18] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:21] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:24] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:27] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:30] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:33] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:36] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:39] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:42] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:45] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:48] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:51] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:54] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:57] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:00] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:03] succeeded | score: 0.32\n",
      "GEPA finished (baseline): succeeded\n",
      "\n",
      "Running GEPA (optimized) with verifier: 685e5e97-f252-4595-97ae-a4e14f140d84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEPA job id (optimized): pl_864b98b20fa84a53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00] queued | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:03] running | score: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py\", line 410, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/fastapi/applications.py\", line 1135, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/applications.py\", line 107, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 115, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 101, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 355, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 243, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/7b/96hbyfld35zflr_plpgvrdxm0000gn/T/ipykernel_3581/1507150015.py\", line 393, in rollout\n",
      "    score = await _score_with_verifier(session_trace, verifier_job_id=verifier_job_id)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/7b/96hbyfld35zflr_plpgvrdxm0000gn/T/ipykernel_3581/1507150015.py\", line 292, in _score_with_verifier\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Verifier failed: HTTP 404 {\"detail\":\"Job not found: 685e5e97-f252-4595-97ae-a4e14f140d84\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:06] running | score: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py\", line 410, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/fastapi/applications.py\", line 1135, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/applications.py\", line 107, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 115, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 101, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 355, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 243, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/7b/96hbyfld35zflr_plpgvrdxm0000gn/T/ipykernel_3581/1507150015.py\", line 393, in rollout\n",
      "    score = await _score_with_verifier(session_trace, verifier_job_id=verifier_job_id)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/7b/96hbyfld35zflr_plpgvrdxm0000gn/T/ipykernel_3581/1507150015.py\", line 292, in _score_with_verifier\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Verifier failed: HTTP 404 {\"detail\":\"Job not found: 685e5e97-f252-4595-97ae-a4e14f140d84\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:09] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:13] running | score: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py\", line 410, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/fastapi/applications.py\", line 1135, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/applications.py\", line 107, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 115, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 101, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 355, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/synth-ai/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 243, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/7b/96hbyfld35zflr_plpgvrdxm0000gn/T/ipykernel_3581/1507150015.py\", line 393, in rollout\n",
      "    score = await _score_with_verifier(session_trace, verifier_job_id=verifier_job_id)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/7b/96hbyfld35zflr_plpgvrdxm0000gn/T/ipykernel_3581/1507150015.py\", line 292, in _score_with_verifier\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Verifier failed: HTTP 404 {\"detail\":\"Job not found: 685e5e97-f252-4595-97ae-a4e14f140d84\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:16] failed | score: --\n",
      "GEPA finished (optimized): failed\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "GEPA job failed (optimized): Async task failed: [Async task] PromptLearningJobError: Optimization phase failed after 14.1s: ValueError: Pattern validation failed: Rollout failed with status 500: Internal Server Error. Completed 0/48 rollouts.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m job_id, result\n\u001b[32m     76\u001b[39m baseline_gepa_job_id, baseline_gepa_result = \u001b[38;5;28;01mawait\u001b[39;00m run_gepa_with_verifier(\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbaseline\u001b[39m\u001b[33m\"\u001b[39m, BASELINE_VERIFIER_JOB_ID\n\u001b[32m     78\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m optimized_gepa_job_id, optimized_gepa_result = \u001b[38;5;28;01mawait\u001b[39;00m run_gepa_with_verifier(\n\u001b[32m     81\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moptimized\u001b[39m\u001b[33m\"\u001b[39m, OPTIMIZED_VERIFIER_GRAPH_ID\n\u001b[32m     82\u001b[39m )\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Fetch prompts\u001b[39;00m\n\u001b[32m     85\u001b[39m pl_client = PromptLearningClient(SYNTH_API_BASE, API_KEY)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mrun_gepa_with_verifier\u001b[39m\u001b[34m(label, verifier_job_id)\u001b[39m\n\u001b[32m     70\u001b[39m                     error_msg = event.get(\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m event.get(\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m                     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGEPA job failed (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msg\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33munknown error\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m job_id, result\n",
      "\u001b[31mRuntimeError\u001b[39m: GEPA job failed (optimized): Async task failed: [Async task] PromptLearningJobError: Optimization phase failed after 14.1s: ValueError: Pattern validation failed: Rollout failed with status 500: Internal Server Error. Completed 0/48 rollouts."
     ]
    }
   ],
   "source": [
    "# Step 5: Run GEPA (baseline vs optimized verifier)\n",
    "\n",
    "POLICY_MODEL = \"gpt-4.1-nano\"\n",
    "\n",
    "\n",
    "def _make_gepa_config(task_app_url: str) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"prompt_learning\": {\n",
    "            \"algorithm\": \"gepa\",\n",
    "            \"task_app_url\": task_app_url,\n",
    "            \"task_app_api_key\": ENVIRONMENT_API_KEY,\n",
    "            \"env_name\": \"style-matching\",\n",
    "            \"initial_prompt\": {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"order\": 0, \"pattern\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"order\": 1, \"pattern\": USER_PROMPT_TEMPLATE},\n",
    "                ],\n",
    "                \"wildcards\": {\"title\": \"REQUIRED\", \"outline\": \"REQUIRED\", \"notes\": \"REQUIRED\"},\n",
    "            },\n",
    "            \"policy\": {\n",
    "                \"inference_mode\": \"synth_hosted\",\n",
    "                \"model\": POLICY_MODEL,\n",
    "                \"provider\": \"openai\",\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_completion_tokens\": 1200,\n",
    "            },\n",
    "            \"gepa\": {\n",
    "                \"env_name\": \"style-matching\",\n",
    "                \"evaluation\": {\"seeds\": list(range(13)), \"validation_seeds\": list(range(13, 17))},\n",
    "                \"rollout\": {\"budget\": 48, \"max_concurrent\": 3, \"minibatch_size\": 3},\n",
    "                \"mutation\": {\"rate\": 0.3},\n",
    "                \"population\": {\"initial_size\": 3, \"num_generations\": 3, \"children_per_generation\": 2},\n",
    "                \"archive\": {\"size\": 5, \"pareto_set_size\": 10},\n",
    "                \"token\": {\"counting_model\": \"gpt-4\"},\n",
    "            },\n",
    "            \"verifier\": {\"enabled\": False, \"reward_source\": \"task_app\"},\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "async def run_gepa_with_verifier(label: str, verifier_job_id: str):\n",
    "    global VERIFIER_JOB_ID\n",
    "    VERIFIER_JOB_ID = verifier_job_id\n",
    "    print(f\"\\nRunning GEPA ({label}) with verifier: {verifier_job_id}\")\n",
    "\n",
    "    config_body = _make_gepa_config(TASK_APP_URL)\n",
    "    job = PromptLearningJob.from_dict(\n",
    "        config_dict=config_body,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        task_app_api_key=ENVIRONMENT_API_KEY,\n",
    "    )\n",
    "\n",
    "    job_id = job.submit()\n",
    "    print(f\"GEPA job id ({label}): {job_id}\")\n",
    "\n",
    "    result = job.poll_until_complete(timeout=3600.0, interval=3.0, progress=True)\n",
    "    print(f\"GEPA finished ({label}): {result.status.value}\")\n",
    "    if result.failed:\n",
    "        error_msg = result.error\n",
    "        if not error_msg:\n",
    "            try:\n",
    "                client = PromptLearningClient(SYNTH_API_BASE, API_KEY)\n",
    "                events = await client.get_events(job_id, limit=5000)\n",
    "            except Exception as exc:\n",
    "                error_msg = f\"Failed to fetch job events: {exc}\"\n",
    "            else:\n",
    "                for event in reversed(events):\n",
    "                    event_type = str(event.get(\"type\", \"\"))\n",
    "                    if event_type in {\"prompt.learning.failed\", \"mipro.job.failed\", \"job.failed\"}:\n",
    "                        error_msg = event.get(\"message\") or event.get(\"data\")\n",
    "                        break\n",
    "        raise RuntimeError(f\"GEPA job failed ({label}): {error_msg or 'unknown error'}\")\n",
    "    return job_id, result\n",
    "\n",
    "\n",
    "baseline_gepa_job_id, baseline_gepa_result = await run_gepa_with_verifier(\n",
    "    \"baseline\", BASELINE_VERIFIER_JOB_ID\n",
    ")\n",
    "\n",
    "optimized_gepa_job_id, optimized_gepa_result = await run_gepa_with_verifier(\n",
    "    \"optimized\", OPTIMIZED_VERIFIER_GRAPH_ID\n",
    ")\n",
    "\n",
    "# Fetch prompts\n",
    "pl_client = PromptLearningClient(SYNTH_API_BASE, API_KEY)\n",
    "\n",
    "baseline_prompts = await pl_client.get_prompts(baseline_gepa_job_id)\n",
    "optimized_prompts = await pl_client.get_prompts(optimized_gepa_job_id)\n",
    "\n",
    "\n",
    "def _select_prompt(result):\n",
    "    best = result.best_prompt\n",
    "    if best:\n",
    "        return best\n",
    "    top = result.top_prompts or []\n",
    "    if top:\n",
    "        return top[0]\n",
    "    raise RuntimeError(\"No prompts returned from GEPA job\")\n",
    "\n",
    "baseline_prompt_obj = _select_prompt(baseline_prompts)\n",
    "optimized_prompt_obj = _select_prompt(optimized_prompts)\n",
    "\n",
    "print(\"\\nBaseline best score:\", baseline_prompts.best_score)\n",
    "print(\"Optimized best score:\", optimized_prompts.best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748375f",
   "metadata": {},
   "source": [
    "## Step 6: Heldout Evaluation (4 Final Scores)\n",
    "\n",
    "We evaluate both best prompts on a heldout set using **eval jobs** and both verifiers:\n",
    "\n",
    "1. Baseline prompt + baseline verifier\n",
    "2. Baseline prompt + optimized verifier\n",
    "3. Optimized prompt + baseline verifier\n",
    "4. Optimized prompt + optimized verifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e0a8c12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T07:51:25.519346Z",
     "iopub.status.busy": "2026-01-05T07:51:25.519250Z",
     "iopub.status.idle": "2026-01-05T07:51:25.537993Z",
     "shell.execute_reply": "2026-01-05T07:51:25.537533Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline_prompt_obj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     40\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEval job failed (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.error\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result.mean_score \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m baseline_prompt = \u001b[43mbaseline_prompt_obj\u001b[49m\n\u001b[32m     45\u001b[39m optimized_prompt = optimized_prompt_obj\n\u001b[32m     47\u001b[39m baseline_sections = _extract_prompt_sections(baseline_prompt)\n",
      "\u001b[31mNameError\u001b[39m: name 'baseline_prompt_obj' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 6: Heldout evaluation via eval jobs\n",
    "\n",
    "HELDOUT_SEEDS = [20, 21, 22, 23]\n",
    "EVAL_POLICY_MODEL = \"gpt-4.1-nano\"\n",
    "\n",
    "\n",
    "def _extract_prompt_sections(prompt_obj: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    template = prompt_obj.get(\"template\") or {}\n",
    "    sections = template.get(\"sections\")\n",
    "    if isinstance(sections, list) and sections:\n",
    "        return sections\n",
    "    messages = prompt_obj.get(\"messages\")\n",
    "    if isinstance(messages, list) and messages:\n",
    "        return messages\n",
    "    return []\n",
    "\n",
    "\n",
    "async def run_eval_job(label: str, prompt_sections: List[Dict[str, Any]], verifier_job_id: str) -> float:\n",
    "    config = EvalJobConfig(\n",
    "        task_app_url=TASK_APP_URL,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        api_key=API_KEY,\n",
    "        task_app_api_key=ENVIRONMENT_API_KEY,\n",
    "        env_name=\"style-matching\",\n",
    "        seeds=HELDOUT_SEEDS,\n",
    "        policy_config={\"model\": EVAL_POLICY_MODEL, \"provider\": \"openai\"},\n",
    "        env_config={\n",
    "            \"prompt_sections\": prompt_sections,\n",
    "            \"verifier_job_id\": verifier_job_id,\n",
    "        },\n",
    "        concurrency=3,\n",
    "    )\n",
    "\n",
    "    job = EvalJob(config)\n",
    "    job_id = job.submit()\n",
    "    print(f\"Eval job ({label}): {job_id}\")\n",
    "\n",
    "    result = job.poll_until_complete(timeout=1800.0, interval=3.0, progress=True)\n",
    "    if not result.succeeded:\n",
    "        raise RuntimeError(f\"Eval job failed ({label}): {result.error}\")\n",
    "    return result.mean_score or 0.0\n",
    "\n",
    "\n",
    "baseline_prompt = baseline_prompt_obj\n",
    "optimized_prompt = optimized_prompt_obj\n",
    "\n",
    "baseline_sections = _extract_prompt_sections(baseline_prompt)\n",
    "optimized_sections = _extract_prompt_sections(optimized_prompt)\n",
    "\n",
    "results = {}\n",
    "results[\"baseline_prompt__baseline_verifier\"] = await run_eval_job(\n",
    "    \"baseline_prompt__baseline_verifier\", baseline_sections, BASELINE_VERIFIER_JOB_ID\n",
    ")\n",
    "results[\"baseline_prompt__optimized_verifier\"] = await run_eval_job(\n",
    "    \"baseline_prompt__optimized_verifier\", baseline_sections, OPTIMIZED_VERIFIER_JOB_ID\n",
    ")\n",
    "results[\"optimized_prompt__baseline_verifier\"] = await run_eval_job(\n",
    "    \"optimized_prompt__baseline_verifier\", optimized_sections, BASELINE_VERIFIER_JOB_ID\n",
    ")\n",
    "results[\"optimized_prompt__optimized_verifier\"] = await run_eval_job(\n",
    "    \"optimized_prompt__optimized_verifier\", optimized_sections, OPTIMIZED_VERIFIER_JOB_ID\n",
    ")\n",
    "\n",
    "print(\"\\nHeldout results (mean verifier score):\")\n",
    "for k, v in results.items():\n",
    "    print(f\"- {k}: {v:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca05453b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T07:51:25.539823Z",
     "iopub.status.busy": "2026-01-05T07:51:25.539692Z",
     "iopub.status.idle": "2026-01-05T07:51:25.541872Z",
     "shell.execute_reply": "2026-01-05T07:51:25.541408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up tunnels...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "print(\"Cleaning up tunnels...\")\n",
    "cleanup_all()\n",
    "print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
