{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c1c0601",
   "metadata": {},
   "source": [
    "# Synth GEPA Demo - Style Matching\n",
    "\n",
    "This notebook demonstrates **style-matching prompt optimization** using Synth's GEPA algorithm.\n",
    "\n",
    "We use a small, fixed dataset of essay prompts and gold examples defined directly in this notebook.\n",
    "The task app scores outputs using a zero-shot contrastive verifier, then we **optimize a verifier graph**\n",
    "(via Graph Evolve) and compare results across baseline vs optimized verifiers.\n",
    "\n",
    "**Run in Google Colab:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/synth-laboratories/synth-ai/blob/main/demos/style_matching/style_matching_prompt_optimization.ipynb)\n",
    "\n",
    "**Structure:**\n",
    "1. **Setup** - Install dependencies and configure API keys\n",
    "2. **Dataset + Task App** - Define a fixed dataset and a local task app\n",
    "3. **Optimize Verifier** - Evolve a verifier graph using graph-evolve jobs\n",
    "4. **Local Server + Tunnel** - Expose the task app to the Synth backend\n",
    "5. **Optimize Prompts** - Run GEPA with baseline vs optimized verifiers\n",
    "6. **Heldout Evaluation** - Score four combinations on a heldout set using eval jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "754bd12c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T20:01:48.612432Z",
     "iopub.status.busy": "2026-01-05T20:01:48.612368Z",
     "iopub.status.idle": "2026-01-05T20:01:48.617191Z",
     "shell.execute_reply": "2026-01-05T20:01:48.616942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in Colab - assuming dependencies are already installed\n",
      "Required: pip install synth-ai httpx fastapi uvicorn nest_asyncio\n",
      "Required: brew install cloudflare/cloudflare/cloudflared (macOS)\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Install dependencies (run this first on Colab)\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab - installing dependencies...\")\n",
    "    !pip install -q synth-ai httpx fastapi uvicorn nest_asyncio\n",
    "\n",
    "    # Install cloudflared\n",
    "    !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O /usr/local/bin/cloudflared\n",
    "    !chmod +x /usr/local/bin/cloudflared\n",
    "    !cloudflared --version\n",
    "\n",
    "    print(\"Dependencies installed!\")\n",
    "else:\n",
    "    print(\"Not in Colab - assuming dependencies are already installed\")\n",
    "    print(\"Required: pip install synth-ai httpx fastapi uvicorn nest_asyncio\")\n",
    "    print(\"Required: brew install cloudflare/cloudflare/cloudflared (macOS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b12c94",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Configure all imports, API keys, and environment keys in one place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a59666a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T20:01:48.618443Z",
     "iopub.status.busy": "2026-01-05T20:01:48.618386Z",
     "iopub.status.idle": "2026-01-05T20:01:52.755218Z",
     "shell.execute_reply": "2026-01-05T20:01:52.754772Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshpurtell/Documents/GitHub/synth-ai/synth_ai/sdk/task/contracts.py:34: UserWarning: Field name \"schema\" in \"StructuredOutputConfig\" shadows an attribute in parent \"BaseModel\"\n",
      "  class StructuredOutputConfig(BaseModel):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[celery_app] EXPERIMENT_QUEUE_DB_PATH not set, will use default path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[celery_app] Using default database path: /Users/joshpurtell/.synth_ai/experiment_queue.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[celery_app] Initializing with database: /Users/joshpurtell/.synth_ai/experiment_queue.db (broker: redis://localhost:6379/0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend: http://localhost:8000\n",
      "Backend health: {'status': 'ok', 'database': 'connected', 'details': {}}\n",
      "\n",
      "Using SYNTH_API_KEY:\n",
      "sk_live_ace8b968-a52...\n",
      "\n",
      "Minted env key:\n",
      "7e7aacb2c83e...0df3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded env key: {'stored': True, 'id': '08a0c070-3c1a-4a99-9386-50a67a65c3ce', 'name': 'ENVIRONMENT_API_KEY', 'updated_at': '2026-01-05T20:01:51.974189Z'}\n",
      "\n",
      "==================================================\n",
      "SETUP COMPLETE\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setup - All imports, config, and API keys\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import httpx\n",
    "import nest_asyncio\n",
    "from fastapi import FastAPI, Header, HTTPException, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from synth_ai.sdk.api.eval import EvalJob, EvalJobConfig\n",
    "from synth_ai.sdk.api.train.prompt_learning import PromptLearningJob\n",
    "from synth_ai.sdk.learning.prompt_learning_client import PromptLearningClient\n",
    "from synth_ai.sdk.learning.rl import mint_environment_api_key, setup_environment_api_key\n",
    "from synth_ai.sdk.task import run_server_background\n",
    "from synth_ai.sdk.tunnels import (\n",
    "    TunneledLocalAPI,\n",
    "    TunnelBackend,\n",
    "    kill_port,\n",
    "    wait_for_health_check,\n",
    "    cleanup_all,\n",
    "    find_available_port,\n",
    "    is_port_available,\n",
    ")\n",
    "\n",
    "from synth_ai.products.graph_evolve import GraphOptimizationClient, GraphOptimizationConfig\n",
    "from synth_ai.products.graph_evolve.config import (\n",
    "    EvolutionConfig,\n",
    "    SeedsConfig,\n",
    "    LimitsConfig,\n",
    "    ProposerConfig,\n",
    ")\n",
    "\n",
    "\n",
    "def _load_env_file(path: str, override: bool = True) -> None:\n",
    "    env_path = Path(path)\n",
    "    if not env_path.exists():\n",
    "        return\n",
    "    for line in env_path.read_text().splitlines():\n",
    "        if not line or line.lstrip().startswith(\"#\") or \"=\" not in line:\n",
    "            continue\n",
    "        key, value = line.split(\"=\", 1)\n",
    "        key = key.strip()\n",
    "        value = value.strip().strip('\"').strip(\"'\")\n",
    "        if key and (override or key not in os.environ):\n",
    "            os.environ[key] = value\n",
    "\n",
    "\n",
    "_load_env_file(\"synth-ai/.env\", override=True)\n",
    "\n",
    "USE_LOCAL_BACKEND = True\n",
    "SYNTH_API_BASE = \"http://localhost:8000\" if USE_LOCAL_BACKEND else \"https://api.usesynth.ai\"\n",
    "os.environ[\"BACKEND_BASE_URL\"] = SYNTH_API_BASE\n",
    "\n",
    "\n",
    "def _get_org_id() -> str:\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "    urls = [f\"{SYNTH_API_BASE}/api/v1/me\", f\"{SYNTH_API_BASE}/me\"]\n",
    "    for url in urls:\n",
    "        resp = httpx.get(url, headers=headers, timeout=30)\n",
    "        if resp.status_code == 404:\n",
    "            continue\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        org_id = data.get(\"org_id\") or data.get(\"orgId\")\n",
    "        if org_id:\n",
    "            return str(org_id)\n",
    "    raise RuntimeError(\"Unable to resolve org_id from /api/v1/me or /me\")\n",
    "\n",
    "\n",
    "LOCAL_TASK_PORT = 8132\n",
    "LOCAL_TASK_HOST = \"127.0.0.1\"\n",
    "\n",
    "\n",
    "def _validate_api_key(api_key: str) -> bool:\n",
    "    if not api_key:\n",
    "        return False\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    try:\n",
    "        resp = httpx.get(f\"{SYNTH_API_BASE}/api/v1/me\", headers=headers, timeout=10)\n",
    "    except Exception:\n",
    "        return False\n",
    "    return resp.status_code == 200\n",
    "\n",
    "\n",
    "print(f\"Backend: {SYNTH_API_BASE}\")\n",
    "\n",
    "# Check backend health\n",
    "r = httpx.get(f\"{SYNTH_API_BASE}/health\", timeout=30)\n",
    "if r.status_code != 200:\n",
    "    raise RuntimeError(f\"Backend not healthy: status {r.status_code}\")\n",
    "print(f\"Backend health: {r.json()}\")\n",
    "\n",
    "# Get API Key (use env var or mint demo key)\n",
    "API_KEY = os.environ.get(\"SYNTH_API_KEY\", \"\").strip()\n",
    "if not API_KEY or not _validate_api_key(API_KEY):\n",
    "    print(\"\")\n",
    "    print(\"SYNTH_API_KEY missing or invalid for this backend; minting demo key...\")\n",
    "    resp = httpx.post(f\"{SYNTH_API_BASE}/api/demo/keys\", json={\"ttl_hours\": 4}, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    API_KEY = resp.json()[\"api_key\"]\n",
    "    print(f\"Demo API Key: {API_KEY[:25]}...\")\n",
    "else:\n",
    "    print(\"\")\n",
    "    print(\"Using SYNTH_API_KEY:\")\n",
    "    print(f\"{API_KEY[:20]}...\")\n",
    "\n",
    "# Set API key in environment for SDK to use\n",
    "os.environ[\"SYNTH_API_KEY\"] = API_KEY\n",
    "\n",
    "# Mint and upload environment key (for local API authentication)\n",
    "ENVIRONMENT_API_KEY = mint_environment_api_key()\n",
    "print(\"\")\n",
    "print(\"Minted env key:\")\n",
    "print(f\"{ENVIRONMENT_API_KEY[:12]}...{ENVIRONMENT_API_KEY[-4:]}\")\n",
    "\n",
    "try:\n",
    "    result = setup_environment_api_key(SYNTH_API_BASE, API_KEY, token=ENVIRONMENT_API_KEY)\n",
    "    print(f\"Uploaded env key: {result}\")\n",
    "except Exception as exc:\n",
    "    print(f\"Env key upload failed (continuing locally): {exc}\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 50)\n",
    "print(\"SETUP COMPLETE\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a302a",
   "metadata": {},
   "source": [
    "## Step 2: Dataset + Task App\n",
    "\n",
    "We define a fixed dataset of essay prompts and gold examples. The task app generates essays using the\n",
    "candidate prompt and scores outputs with the zero-shot contrastive verifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9faaa16e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T20:01:52.756733Z",
     "iopub.status.busy": "2026-01-05T20:01:52.756535Z",
     "iopub.status.idle": "2026-01-05T20:01:52.785036Z",
     "shell.execute_reply": "2026-01-05T20:01:52.784657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task dataset size: 4\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Fixed dataset + task app definition\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a thoughtful essayist with a direct, builder-first voice. \"\n",
    "    \"Write crisp, opinionated essays with concrete examples, minimal fluff, \"\n",
    "    \"and a short, memorable closing line. Aim for ~500 words (roughly 450-650).\"\n",
    ")\n",
    "\n",
    "USER_PROMPT_TEMPLATE = (\n",
    "    \"Title: {title}\\n\"\n",
    "    \"Outline:\\n{outline}\\n\\n\"\n",
    "    \"Notes:\\n{notes}\\n\\n\"\n",
    "    \"Target length: ~500 words.\\n\\n\"\n",
    "    \"Write the essay now.\"\n",
    ")\n",
    "\n",
    "TASKS: List[Dict[str, Any]] = [\n",
    "    {\n",
    "        \"id\": \"outline_speed\",\n",
    "        \"input\": {\n",
    "            \"title\": \"Shipping Fast Without Burning Out\",\n",
    "            \"outline\": (\n",
    "                \"1. Why speed compounds learning\\n\"\n",
    "                \"2. The tradeoff between velocity and quality\\n\"\n",
    "                \"3. How to keep teams aligned under pressure\\n\"\n",
    "                \"4. Practical rituals that preserve momentum\\n\"\n",
    "                \"5. Closing: pace as a competitive advantage\"\n",
    "            ),\n",
    "            \"notes\": [\"short feedback loops\", \"protect maker time\", \"use small bets\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"outline_focus\",\n",
    "        \"input\": {\n",
    "            \"title\": \"Focus Beats Optionality\",\n",
    "            \"outline\": (\n",
    "                \"1. Optionality feels safe but slows decisions\\n\"\n",
    "                \"2. Focus creates a compounding advantage\\n\"\n",
    "                \"3. The cost of context switching in small teams\\n\"\n",
    "                \"4. Saying no as a leadership skill\\n\"\n",
    "                \"5. Closing: clarity is leverage\"\n",
    "            ),\n",
    "            \"notes\": [\"pick one wedge\", \"eliminate parallel bets\", \"repeat a simple story\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"outline_learning\",\n",
    "        \"input\": {\n",
    "            \"title\": \"Learning in Public\",\n",
    "            \"outline\": (\n",
    "                \"1. Writing as a forcing function\\n\"\n",
    "                \"2. How sharing drafts accelerates feedback\\n\"\n",
    "                \"3. The credibility flywheel\\n\"\n",
    "                \"4. Risks of over-sharing (and how to avoid them)\\n\"\n",
    "                \"5. Closing: publish to clarify\"\n",
    "            ),\n",
    "            \"notes\": [\n",
    "                \"ship drafts, not polished essays\",\n",
    "                \"ask for specific feedback\",\n",
    "                \"be concrete about failures\",\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"outline_quality\",\n",
    "        \"input\": {\n",
    "            \"title\": \"Quality as a Constraint\",\n",
    "            \"outline\": (\n",
    "                \"1. The myth that quality slows you down\\n\"\n",
    "                \"2. Cheap fixes vs durable systems\\n\"\n",
    "                \"3. When to accept rough edges\\n\"\n",
    "                \"4. How to build taste in a team\\n\"\n",
    "                \"5. Closing: quality is a habit\"\n",
    "            ),\n",
    "            \"notes\": [\"use guardrails\", \"invest in tooling\", \"make quality visible\"],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "GOLD_EXAMPLES: List[Dict[str, Any]] = [\n",
    "    {\n",
    "        \"summary\": \"Direct, builder tone with concrete examples\",\n",
    "        \"gold_score\": 0.95,\n",
    "        \"gold_reasoning\": \"Short sentences, decisive stance, concrete advice, crisp closing line.\",\n",
    "        \"trace\": {\n",
    "            \"session_id\": \"gold-1\",\n",
    "            \"session_time_steps\": [\n",
    "                {\n",
    "                    \"step_id\": \"1\",\n",
    "                    \"step_index\": 0,\n",
    "                    \"events\": [\n",
    "                        {\n",
    "                            \"event_type\": \"runtime\",\n",
    "                            \"event_id\": 1,\n",
    "                            \"type\": \"user_message\",\n",
    "                            \"content\": \"Write about shipping fast.\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"event_type\": \"runtime\",\n",
    "                            \"event_id\": 2,\n",
    "                            \"type\": \"assistant_message\",\n",
    "                            \"content\": (\n",
    "                                \"Speed is a learning engine. Ship a small bet, watch users react, then sharpen the next move. \"\n",
    "                                \"Protect maker time, keep scope tight, and treat every release as feedback, not theater. \"\n",
    "                                \"Momentum beats motivation.\"\n",
    "                            ),\n",
    "                        },\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"summary\": \"Opinionated essay with a sharp closing line\",\n",
    "        \"gold_score\": 0.92,\n",
    "        \"gold_reasoning\": \"Clear thesis, direct claims, practical advice, and a memorable end.\",\n",
    "        \"trace\": {\n",
    "            \"session_id\": \"gold-2\",\n",
    "            \"session_time_steps\": [\n",
    "                {\n",
    "                    \"step_id\": \"1\",\n",
    "                    \"step_index\": 0,\n",
    "                    \"events\": [\n",
    "                        {\n",
    "                            \"event_type\": \"runtime\",\n",
    "                            \"event_id\": 1,\n",
    "                            \"type\": \"user_message\",\n",
    "                            \"content\": \"Write about focus.\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"event_type\": \"runtime\",\n",
    "                            \"event_id\": 2,\n",
    "                            \"type\": \"assistant_message\",\n",
    "                            \"content\": (\n",
    "                                \"Optionality feels safe, but it dilutes learning. Pick one wedge, cut parallel bets, and ship. \"\n",
    "                                \"Small teams win by saying no early and often. Clarity is leverage.\"\n",
    "                            ),\n",
    "                        },\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"summary\": \"Concrete, tactical quality guidance with guardrails\",\n",
    "        \"gold_score\": 0.96,\n",
    "        \"gold_reasoning\": \"Direct stance, concrete guardrails, and a crisp closing line.\",\n",
    "        \"trace\": {\n",
    "            \"session_id\": \"gold-3\",\n",
    "            \"session_time_steps\": [\n",
    "                {\n",
    "                    \"step_id\": \"1\",\n",
    "                    \"step_index\": 0,\n",
    "                    \"events\": [\n",
    "                        {\n",
    "                            \"event_type\": \"runtime\",\n",
    "                            \"event_id\": 1,\n",
    "                            \"type\": \"user_message\",\n",
    "                            \"content\": \"Write about quality as a constraint.\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"event_type\": \"runtime\",\n",
    "                            \"event_id\": 2,\n",
    "                            \"type\": \"assistant_message\",\n",
    "                            \"content\": (\n",
    "                                \"Quality is the guardrail that keeps speed from turning into chaos. Ship small, test the riskiest paths, \"\n",
    "                                \"and make rollback cheap. Fix root causes once, then automate the prevention. \"\n",
    "                                \"Quality is a habit, not a milestone.\"\n",
    "                            ),\n",
    "                        },\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# Baseline verifier (zero-shot contrastive)\n",
    "BASELINE_VERIFIER_JOB_ID = \"zero_shot_verifier_contrastive_single\"\n",
    "VERIFIER_JOB_ID = BASELINE_VERIFIER_JOB_ID\n",
    "VERIFIER_MODEL = \"gpt-4.1-nano\"\n",
    "\n",
    "ROLLOUT_LOG: List[Dict[str, Any]] = []\n",
    "\n",
    "\n",
    "def _verify_api_key(x_api_key: Optional[str]) -> bool:\n",
    "    if not ENVIRONMENT_API_KEY:\n",
    "        return True\n",
    "    return x_api_key == ENVIRONMENT_API_KEY\n",
    "\n",
    "\n",
    "def _format_notes(notes: List[str]) -> str:\n",
    "    if not notes:\n",
    "        return \"- (none)\"\n",
    "    return \"\\n\".join(f\"- {note}\" for note in notes)\n",
    "\n",
    "\n",
    "def _safe_format(text: str, values: Dict[str, Any]) -> str:\n",
    "    class _DefaultDict(dict):\n",
    "        def __missing__(self, key: str) -> str:\n",
    "            return \"\"\n",
    "\n",
    "    return text.format_map(_DefaultDict(values))\n",
    "\n",
    "\n",
    "def _render_messages_from_sections(\n",
    "    sections: List[Dict[str, Any]], values: Dict[str, Any]\n",
    ") -> List[Dict[str, str]]:\n",
    "    rendered = []\n",
    "    for section in sorted(sections, key=lambda s: s.get(\"order\", 0)):\n",
    "        role = section.get(\"role\", \"user\")\n",
    "        content = section.get(\"content\") or section.get(\"pattern\") or \"\"\n",
    "        if content:\n",
    "            rendered.append({\"role\": str(role), \"content\": _safe_format(str(content), values)})\n",
    "    return rendered\n",
    "\n",
    "\n",
    "def _build_messages(\n",
    "    task_input: Dict[str, Any], prompt_sections: Optional[List[Dict[str, Any]]] = None\n",
    ") -> List[Dict[str, str]]:\n",
    "    notes_text = _format_notes(task_input.get(\"notes\", []))\n",
    "    values = {\n",
    "        \"title\": task_input.get(\"title\", \"\"),\n",
    "        \"outline\": task_input.get(\"outline\", \"\"),\n",
    "        \"notes\": notes_text,\n",
    "    }\n",
    "    if prompt_sections:\n",
    "        return _render_messages_from_sections(prompt_sections, values)\n",
    "\n",
    "    user_prompt = USER_PROMPT_TEMPLATE.format(\n",
    "        title=values[\"title\"],\n",
    "        outline=values[\"outline\"],\n",
    "        notes=values[\"notes\"],\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "\n",
    "def _build_inference_url(inference_url: str) -> str:\n",
    "    if \"?\" in inference_url:\n",
    "        base, query = inference_url.split(\"?\", 1)\n",
    "        return f\"{base.rstrip('/')}/chat/completions?{query}\"\n",
    "    return f\"{inference_url.rstrip('/')}/chat/completions\"\n",
    "\n",
    "\n",
    "def _extract_completion(data: Dict[str, Any]) -> str:\n",
    "    try:\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"] or \"\"\n",
    "    except (KeyError, IndexError, TypeError):\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _extract_verifier_score(result: Dict[str, Any]) -> float:\n",
    "    output = result.get(\"output\", result)\n",
    "    if isinstance(output, dict):\n",
    "        outcome_review = output.get(\"outcome_review\")\n",
    "        if isinstance(outcome_review, dict) and isinstance(\n",
    "            outcome_review.get(\"total\"), (int, float)\n",
    "        ):\n",
    "            return float(outcome_review[\"total\"])\n",
    "        event_reviews = output.get(\"event_reviews\")\n",
    "        if isinstance(event_reviews, list) and event_reviews:\n",
    "            totals = [rev.get(\"total\") for rev in event_reviews if isinstance(rev, dict)]\n",
    "            totals = [t for t in totals if isinstance(t, (int, float))]\n",
    "            if totals:\n",
    "                return float(sum(totals) / len(totals))\n",
    "        if isinstance(output.get(\"total\"), (int, float)):\n",
    "            return float(output[\"total\"])\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "async def _call_policy_llm(messages: List[Dict[str, str]], policy_config: Dict[str, Any]) -> str:\n",
    "    inference_url = policy_config.get(\"inference_url\")\n",
    "    if not inference_url:\n",
    "        raise RuntimeError(\"policy.config.inference_url is required\")\n",
    "\n",
    "    url = _build_inference_url(inference_url)\n",
    "    model = policy_config.get(\"model\", \"gpt-4.1-nano\")\n",
    "    model_lower = model.lower()\n",
    "    is_gpt5 = \"gpt-5\" in model_lower\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    api_key = policy_config.get(\"api_key\")\n",
    "    if api_key:\n",
    "        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "    elif ENVIRONMENT_API_KEY:\n",
    "        headers[\"X-API-Key\"] = ENVIRONMENT_API_KEY\n",
    "        headers[\"Authorization\"] = f\"Bearer {ENVIRONMENT_API_KEY}\"\n",
    "\n",
    "    payload = {\"model\": model, \"messages\": messages}\n",
    "    if is_gpt5:\n",
    "        payload[\"max_completion_tokens\"] = int(policy_config.get(\"max_completion_tokens\", 16000))\n",
    "        reasoning_effort = policy_config.get(\"reasoning_effort\")\n",
    "        if reasoning_effort:\n",
    "            payload[\"reasoning_effort\"] = reasoning_effort\n",
    "    else:\n",
    "        payload[\"temperature\"] = float(policy_config.get(\"temperature\", 0.7))\n",
    "        payload[\"max_tokens\"] = int(policy_config.get(\"max_completion_tokens\", 1200))\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=120.0) as client:\n",
    "        response = await client.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return _extract_completion(response.json())\n",
    "\n",
    "\n",
    "async def _score_with_verifier(\n",
    "    session_trace: Dict[str, Any], verifier_job_id: Optional[str] = None\n",
    ") -> float:\n",
    "    job_id = verifier_job_id or VERIFIER_JOB_ID\n",
    "    payload = {\n",
    "        \"job_id\": job_id,\n",
    "        \"input\": {\n",
    "            \"trace\": session_trace,\n",
    "            \"gold_examples\": GOLD_EXAMPLES,\n",
    "            \"candidate_score\": 0.5,\n",
    "            \"candidate_reasoning\": \"Auto-evaluated from style-matching task app\",\n",
    "            \"options\": {\"model\": VERIFIER_MODEL},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=120.0) as client:\n",
    "        response = await client.post(\n",
    "            f\"{SYNTH_API_BASE.rstrip('/')}/api/graphs/completions\",\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(\n",
    "                f\"Verifier failed: HTTP {response.status_code} {response.text[:500]}\"\n",
    "            )\n",
    "        result = response.json()\n",
    "\n",
    "    return _extract_verifier_score(result)\n",
    "\n",
    "\n",
    "app = FastAPI(title=\"Style Matching Task App\")\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health() -> Dict[str, str]:\n",
    "    return {\"status\": \"ok\", \"task_app\": \"style_matching\"}\n",
    "\n",
    "\n",
    "@app.get(\"/task_info\")\n",
    "async def task_info() -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"taskset\": {\n",
    "            \"name\": \"style_matching\",\n",
    "            \"description\": \"Style matching demo task app\",\n",
    "            \"size\": len(TASKS),\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "@app.get(\"/tasks\")\n",
    "async def list_tasks() -> Dict[str, Any]:\n",
    "    return {\"tasks\": TASKS, \"gold_examples\": GOLD_EXAMPLES}\n",
    "\n",
    "\n",
    "@app.get(\"/rollouts\")\n",
    "async def list_rollouts(x_api_key: Optional[str] = Header(None)) -> Dict[str, Any]:\n",
    "    if not _verify_api_key(x_api_key):\n",
    "        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n",
    "    return {\"rollouts\": ROLLOUT_LOG}\n",
    "\n",
    "\n",
    "@app.post(\"/rollout\")\n",
    "async def rollout(request: Request, x_api_key: Optional[str] = Header(None)) -> Dict[str, Any]:\n",
    "    if not _verify_api_key(x_api_key):\n",
    "        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n",
    "\n",
    "    try:\n",
    "        data = await request.json()\n",
    "    except Exception:\n",
    "        raise HTTPException(status_code=400, detail=\"Invalid JSON\")\n",
    "\n",
    "    run_id = data.get(\"run_id\")\n",
    "    env = data.get(\"env\", {})\n",
    "    policy = data.get(\"policy\", {})\n",
    "    policy_config = policy.get(\"config\", {})\n",
    "\n",
    "    trace_correlation_id = policy_config.get(\"trace_correlation_id\")\n",
    "\n",
    "    env_config = env.get(\"config\", {}) or {}\n",
    "    verifier_job_id = env_config.get(\"verifier_job_id\") or VERIFIER_JOB_ID\n",
    "    prompt_sections = env_config.get(\"prompt_sections\")\n",
    "\n",
    "    seed = int(env.get(\"seed\", 0))\n",
    "    task = TASKS[seed % len(TASKS)]\n",
    "    task_input = task[\"input\"]\n",
    "\n",
    "    messages = _build_messages(task_input, prompt_sections=prompt_sections)\n",
    "\n",
    "    try:\n",
    "        essay = await _call_policy_llm(messages, policy_config)\n",
    "    except Exception as exc:\n",
    "        essay = f\"[error: {exc}]\"\n",
    "\n",
    "    session_trace = {\n",
    "        \"session_id\": f\"style-matching-{task['id']}\",\n",
    "        \"session_time_steps\": [\n",
    "            {\n",
    "                \"step_id\": \"1\",\n",
    "                \"step_index\": 0,\n",
    "                \"events\": [\n",
    "                    {\n",
    "                        \"event_type\": \"runtime\",\n",
    "                        \"event_id\": 1,\n",
    "                        \"type\": \"user_message\",\n",
    "                        \"content\": messages[-1][\"content\"],\n",
    "                    },\n",
    "                    {\n",
    "                        \"event_type\": \"runtime\",\n",
    "                        \"event_id\": 2,\n",
    "                        \"type\": \"assistant_message\",\n",
    "                        \"content\": essay,\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    score = await _score_with_verifier(session_trace, verifier_job_id=verifier_job_id)\n",
    "\n",
    "    ROLLOUT_LOG.append(\n",
    "        {\n",
    "            \"run_id\": run_id,\n",
    "            \"seed\": seed,\n",
    "            \"task_id\": task[\"id\"],\n",
    "            \"title\": task_input.get(\"title\", \"\"),\n",
    "            \"essay\": essay,\n",
    "            \"score\": score,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"metrics\": {\n",
    "            \"mean_return\": score,\n",
    "            \"outcome_score\": score,\n",
    "            \"num_steps\": 1,\n",
    "            \"details\": {\"verifier_score\": score},\n",
    "        },\n",
    "        \"trajectories\": [\n",
    "            {\"steps\": [{\"observation\": task_input, \"action\": {\"essay\": essay}, \"reward\": score}]}\n",
    "        ],\n",
    "        \"metadata\": {\"task_id\": task[\"id\"]},\n",
    "        \"trace_correlation_id\": trace_correlation_id or \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "print(f\"Task dataset size: {len(TASKS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7db7157",
   "metadata": {},
   "source": [
    "## Step 3: Optimize Verifier Graph\n",
    "\n",
    "We use Graph Evolve to optimize a verifier graph on a small, fixed calibration set.\n",
    "This graph will serve as an alternative verifier in the GEPA loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a3eb63b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T20:01:52.786250Z",
     "iopub.status.busy": "2026-01-05T20:01:52.786190Z",
     "iopub.status.idle": "2026-01-05T20:04:22.503200Z",
     "shell.execute_reply": "2026-01-05T20:04:22.502367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved org_id: e77ef3a8-677d-4ddd-92d6-0f114d6bbdaf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph evolve job: graph_evolve_4aca6751f48a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifier optimization complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized verifier job id: graph_evolve_4aca6751f48a\n",
      "Optimized verifier graph id: 5bd0d553-40ba-4aca-a157-126009dee1bd\n",
      "Best score: 0.675\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Optimize a verifier graph with Graph Evolve\n",
    "\n",
    "\n",
    "def _make_trace(user_text: str, assistant_text: str) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"session_id\": \"trace\",\n",
    "        \"session_time_steps\": [\n",
    "            {\n",
    "                \"step_id\": \"1\",\n",
    "                \"step_index\": 0,\n",
    "                \"events\": [\n",
    "                    {\n",
    "                        \"event_type\": \"runtime\",\n",
    "                        \"event_id\": 1,\n",
    "                        \"type\": \"user_message\",\n",
    "                        \"content\": user_text,\n",
    "                    },\n",
    "                    {\n",
    "                        \"event_type\": \"runtime\",\n",
    "                        \"event_id\": 2,\n",
    "                        \"type\": \"assistant_message\",\n",
    "                        \"content\": assistant_text,\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "VERIFIER_EXAMPLES = [\n",
    "    {\n",
    "        \"task_id\": \"good_speed\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about shipping fast.\",\n",
    "            \"Speed compounds learning. Ship small bets, learn fast, keep scope tight, and protect maker time.\",\n",
    "        ),\n",
    "        \"score\": 0.95,\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"good_focus\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about focus.\",\n",
    "            \"Optionality dilutes learning. Pick one wedge, cut parallel bets, and repeat a simple story.\",\n",
    "        ),\n",
    "        \"score\": 0.92,\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"good_quality\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about quality as a constraint.\",\n",
    "            \"Quality is a guardrail. Ship small, test risky paths, and make rollback cheap.\",\n",
    "        ),\n",
    "        \"score\": 0.93,\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"good_learning\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about learning in public.\",\n",
    "            \"Publish drafts to accelerate feedback, build credibility, and clarify thinking.\",\n",
    "        ),\n",
    "        \"score\": 0.91,\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"bad_rambling\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about focus.\",\n",
    "            \"Focus is important because focus is important. You should focus on focusing and focus on focus.\",\n",
    "        ),\n",
    "        \"score\": 0.10,\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"bad_vague\",\n",
    "        \"trace\": _make_trace(\n",
    "            \"Write about quality.\",\n",
    "            \"Quality is good. Teams should be good and do good things to make quality good.\",\n",
    "        ),\n",
    "        \"score\": 0.05,\n",
    "    },\n",
    "]\n",
    "\n",
    "VERIFIER_TRAIN_SEEDS = list(range(4))\n",
    "VERIFIER_VAL_SEEDS = list(range(4, len(VERIFIER_EXAMPLES)))\n",
    "\n",
    "verifier_dataset = {\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_id\": example[\"task_id\"],\n",
    "            \"input\": {\"trace\": example[\"trace\"], \"gold_examples\": GOLD_EXAMPLES},\n",
    "        }\n",
    "        for example in VERIFIER_EXAMPLES\n",
    "    ],\n",
    "    \"gold_outputs\": [\n",
    "        {\"task_id\": example[\"task_id\"], \"output\": {}, \"score\": example[\"score\"]}\n",
    "        for example in VERIFIER_EXAMPLES\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"name\": \"style_matching_verifier\",\n",
    "        \"task_description\": \"Score style-matching quality using strict verifier-style outputs. Return event_reviews, outcome_review, event_totals; use a 1.0 baseline with deductions for discrepancies (generic outputs < 0.3).\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"trace\": {\"type\": \"object\"}, \"gold_examples\": {\"type\": \"array\"}},\n",
    "            \"required\": [\"trace\", \"gold_examples\"],\n",
    "        },\n",
    "        \"output_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"event_reviews\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"criteria\": {\"type\": \"object\"},\n",
    "                            \"total\": {\"type\": \"number\"},\n",
    "                            \"summary\": {\"type\": \"string\"},\n",
    "                        },\n",
    "                        \"required\": [\"criteria\", \"total\"],\n",
    "                    },\n",
    "                },\n",
    "                \"outcome_review\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"criteria\": {\"type\": \"object\"},\n",
    "                        \"total\": {\"type\": \"number\"},\n",
    "                        \"summary\": {\"type\": \"string\"},\n",
    "                    },\n",
    "                    \"required\": [\"criteria\", \"total\"],\n",
    "                },\n",
    "                \"event_totals\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}},\n",
    "                \"score\": {\"type\": \"number\"},\n",
    "            },\n",
    "            \"required\": [\"event_reviews\", \"outcome_review\", \"event_totals\"],\n",
    "        },\n",
    "        \"output_config\": {\n",
    "            \"format\": \"json\",\n",
    "            \"strict\": True,\n",
    "            \"extract_from\": [\"(root)\"],\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"event_reviews\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"criteria\": {\"type\": \"object\"},\n",
    "                                \"total\": {\"type\": \"number\"},\n",
    "                                \"summary\": {\"type\": \"string\"},\n",
    "                            },\n",
    "                            \"required\": [\"criteria\", \"total\"],\n",
    "                        },\n",
    "                    },\n",
    "                    \"outcome_review\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"criteria\": {\"type\": \"object\"},\n",
    "                            \"total\": {\"type\": \"number\"},\n",
    "                            \"summary\": {\"type\": \"string\"},\n",
    "                        },\n",
    "                        \"required\": [\"criteria\", \"total\"],\n",
    "                    },\n",
    "                    \"event_totals\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}},\n",
    "                    \"score\": {\"type\": \"number\"},\n",
    "                },\n",
    "                \"required\": [\"event_reviews\", \"outcome_review\", \"event_totals\"],\n",
    "            },\n",
    "        },\n",
    "        \"domain\": \"text\",\n",
    "    },\n",
    "}\n",
    "\n",
    "verifier_config = GraphOptimizationConfig(\n",
    "    algorithm=\"graph_evolve\",\n",
    "    dataset_name=\"style_matching_verifier\",\n",
    "    graph_type=\"verifier\",\n",
    "    graph_structure=\"single_prompt\",\n",
    "    topology_guidance=(\n",
    "        \"Single-node VerifierGraph. Use one DagNode (e.g., judge_style) with template_transform. \"\n",
    "        \"Set output_mapping to copy event_reviews, outcome_review, event_totals, score to root. \"\n",
    "        \"Include verdict_weights and aggregation_policy: weighted_average.\"\n",
    "    ),\n",
    "    allowed_policy_models=[\"gpt-4.1-nano\", \"gpt-4o-mini\"],\n",
    "    evolution=EvolutionConfig(num_generations=3, children_per_generation=2),\n",
    "    proposer=ProposerConfig(model=\"gpt-4.1\", temperature=0.0),\n",
    "    seeds=SeedsConfig(train=VERIFIER_TRAIN_SEEDS, validation=VERIFIER_VAL_SEEDS),\n",
    "    limits=LimitsConfig(max_spend_usd=5.0, timeout_seconds=3600),\n",
    "    verifier_mode=\"contrastive\",\n",
    "    verifier_model=VERIFIER_MODEL,\n",
    "    dataset=verifier_dataset,\n",
    "    output_schema=verifier_dataset[\"metadata\"][\"output_schema\"],\n",
    "    output_config=verifier_dataset[\"metadata\"][\"output_config\"],\n",
    "    task_description=\"Score style-matching quality using strict verifier-style outputs. Return event_reviews, outcome_review, event_totals; use a 1.0 baseline with deductions for discrepancies (generic outputs < 0.3).\",\n",
    "    problem_spec=(\n",
    "        \"You are generating a VerifierGraph. The final output MUST be a JSON object with: \"\n",
    "        \"event_reviews (list of per-event review objects with criteria, total, summary), \"\n",
    "        \"outcome_review (object with criteria, total, summary), and event_totals (list of numbers). \"\n",
    "        \"Include a top-level score if helpful, but the verifier contract is based on outcome_review.total and event_totals. \"\n",
    "        \"Make totals floats in [0,1]. \"\n",
    "        \"Scoring policy must be strict: start at 1.0 and deduct for every discrepancy vs gold examples. \"\n",
    "        \"Generic/standard outputs should score below 0.3. \"\n",
    "        \"Deduction guide: obvious/giveaway discrepancy deduct 0.15-0.3, major discrepancy 0.08-0.15, minor 0.02-0.08.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "async def run_verifier_optimization():\n",
    "    async with GraphOptimizationClient(SYNTH_API_BASE, API_KEY) as client:\n",
    "        job_id = await client.start_job(verifier_config)\n",
    "        print(f\"Graph evolve job: {job_id}\")\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=90.0) as http:\n",
    "        headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "        for _ in range(900):\n",
    "            try:\n",
    "                status_resp = await http.get(\n",
    "                    f\"{SYNTH_API_BASE}/graph-evolve/jobs/{job_id}/status\", headers=headers\n",
    "                )\n",
    "                if status_resp.status_code == 404:\n",
    "                    await asyncio.sleep(2.0)\n",
    "                    continue\n",
    "                status_resp.raise_for_status()\n",
    "                status = status_resp.json().get(\"status\")\n",
    "                if status in {\"completed\", \"failed\", \"cancelled\"}:\n",
    "                    result_resp = await http.get(\n",
    "                        f\"{SYNTH_API_BASE}/graph-evolve/jobs/{job_id}/result\", headers=headers\n",
    "                    )\n",
    "                    result_resp.raise_for_status()\n",
    "                    return job_id, result_resp.json()\n",
    "            except httpx.HTTPStatusError:\n",
    "                await asyncio.sleep(2.0)\n",
    "                continue\n",
    "            await asyncio.sleep(2.0)\n",
    "\n",
    "    raise RuntimeError(f\"Graph evolve job {job_id} did not complete in time\")\n",
    "\n",
    "\n",
    "ORG_ID = _get_org_id()\n",
    "print(f\"Resolved org_id: {ORG_ID}\")\n",
    "\n",
    "\n",
    "async def save_verifier_graph(job_id: str) -> str:\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"name\": f\"style-matching-verifier-{job_id[:8]}\",\n",
    "        \"org_id\": ORG_ID,\n",
    "        \"kind\": \"verifier\",\n",
    "    }\n",
    "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "        resp = await client.post(\n",
    "            f\"{SYNTH_API_BASE}/graph-evolve/jobs/{job_id}/save-graph\",\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "        )\n",
    "        if resp.status_code >= 400:\n",
    "            raise RuntimeError(f\"save-graph failed ({resp.status_code}): {resp.text[:500]}\")\n",
    "        data = resp.json()\n",
    "    graph_id = data.get(\"graph_id\") or data.get(\"id\")\n",
    "    if not graph_id:\n",
    "        raise RuntimeError(f\"save-graph did not return graph_id: {data}\")\n",
    "    return str(graph_id)\n",
    "\n",
    "\n",
    "OPTIMIZED_VERIFIER_JOB_ID, OPTIMIZED_VERIFIER_RESULT = await run_verifier_optimization()\n",
    "status = OPTIMIZED_VERIFIER_RESULT.get(\"status\")\n",
    "if status != \"completed\":\n",
    "    error_msg = OPTIMIZED_VERIFIER_RESULT.get(\"error\") or \"unknown error\"\n",
    "    raise RuntimeError(f\"Graph evolve job failed with status={status}: {error_msg}\")\n",
    "print(\"\\nVerifier optimization complete\")\n",
    "OPTIMIZED_VERIFIER_GRAPH_ID = await save_verifier_graph(OPTIMIZED_VERIFIER_JOB_ID)\n",
    "print(f\"Optimized verifier job id: {OPTIMIZED_VERIFIER_JOB_ID}\")\n",
    "print(f\"Optimized verifier graph id: {OPTIMIZED_VERIFIER_GRAPH_ID}\")\n",
    "print(f\"Best score: {OPTIMIZED_VERIFIER_RESULT.get('best_score')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80166d77",
   "metadata": {},
   "source": [
    "## Step 4: Start Task App + Tunnel\n",
    "\n",
    "Run the task app locally, then create a tunnel so the Synth backend can call it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6fc1fa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T20:04:22.507597Z",
     "iopub.status.busy": "2026-01-05T20:04:22.507328Z",
     "iopub.status.idle": "2026-01-05T20:04:23.053629Z",
     "shell.execute_reply": "2026-01-05T20:04:23.052889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task app on port 8132...\n",
      "Waiting for task app health check...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task app ready!\n",
      "Using localhost task app URL (no tunnel)\n",
      "Task app URL: http://127.0.0.1:8132\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Start task app and create tunnel\n",
    "\n",
    "import asyncio\n",
    "import socket\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "def wait_for_system_dns(hostname: str, timeout: float = 90.0, interval: float = 3.0) -> None:\n",
    "    deadline = time.time() + timeout\n",
    "    last_exc = None\n",
    "    while time.time() < deadline:\n",
    "        try:\n",
    "            socket.gethostbyname(hostname)\n",
    "            return\n",
    "        except Exception as exc:\n",
    "            last_exc = exc\n",
    "            time.sleep(interval)\n",
    "    raise RuntimeError(f\"System DNS did not resolve {hostname} within {timeout}s: {last_exc}\")\n",
    "\n",
    "\n",
    "def _task_app_healthcheck(host: str, port: int) -> bool:\n",
    "    try:\n",
    "        resp = httpx.get(\n",
    "            f\"http://{host}:{port}/health\",\n",
    "            headers={\"X-API-Key\": ENVIRONMENT_API_KEY},\n",
    "            timeout=5,\n",
    "        )\n",
    "        return resp.status_code == 200\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _wait_for_task_app(host: str, port: int, timeout: float = 30.0) -> None:\n",
    "    deadline = time.time() + timeout\n",
    "    while time.time() < deadline:\n",
    "        if _task_app_healthcheck(host, port):\n",
    "            return\n",
    "        time.sleep(1.0)\n",
    "    raise RuntimeError(f\"Task app health check failed after {timeout}s\")\n",
    "\n",
    "\n",
    "task_app_thread = None\n",
    "_task_app_lock = threading.Lock()\n",
    "\n",
    "\n",
    "def _start_task_app() -> None:\n",
    "    global LOCAL_TASK_PORT\n",
    "    global task_app_thread\n",
    "\n",
    "    kill_port(LOCAL_TASK_PORT)\n",
    "    if not is_port_available(LOCAL_TASK_PORT):\n",
    "        LOCAL_TASK_PORT = find_available_port(LOCAL_TASK_PORT + 1)\n",
    "        print(f\"Port in use; switched to {LOCAL_TASK_PORT}\")\n",
    "\n",
    "    task_app_thread = run_server_background(app, LOCAL_TASK_PORT, host=LOCAL_TASK_HOST)\n",
    "    _wait_for_task_app(LOCAL_TASK_HOST, LOCAL_TASK_PORT, timeout=30.0)\n",
    "\n",
    "\n",
    "def _start_task_app_monitor(interval: float = 5.0) -> threading.Thread:\n",
    "    def _monitor() -> None:\n",
    "        while True:\n",
    "            time.sleep(interval)\n",
    "            with _task_app_lock:\n",
    "                if _task_app_healthcheck(LOCAL_TASK_HOST, LOCAL_TASK_PORT):\n",
    "                    continue\n",
    "                print(\"Task app health check failed; restarting...\")\n",
    "                try:\n",
    "                    _start_task_app()\n",
    "                except Exception as exc:\n",
    "                    print(f\"Task app restart failed: {exc}\")\n",
    "\n",
    "    thread = threading.Thread(target=_monitor, daemon=True, name=\"task-app-monitor\")\n",
    "    thread.start()\n",
    "    return thread\n",
    "\n",
    "\n",
    "print(f\"Starting task app on port {LOCAL_TASK_PORT}...\")\n",
    "with _task_app_lock:\n",
    "    _start_task_app()\n",
    "print(\"Task app ready!\")\n",
    "monitor_thread = _start_task_app_monitor()\n",
    "\n",
    "if USE_LOCAL_BACKEND:\n",
    "    print(\"Using localhost task app URL (no tunnel)\")\n",
    "    TASK_APP_URL = f\"http://{LOCAL_TASK_HOST}:{LOCAL_TASK_PORT}\"\n",
    "    tunnel = None\n",
    "else:\n",
    "    print(\"\")\n",
    "    print(\"Provisioning Cloudflare tunnel...\")\n",
    "    try:\n",
    "        tunnel = await TunneledLocalAPI.create(\n",
    "            local_port=LOCAL_TASK_PORT,\n",
    "            backend=TunnelBackend.CloudflareManagedTunnel,\n",
    "            api_key=API_KEY,\n",
    "            env_api_key=ENVIRONMENT_API_KEY,\n",
    "            backend_url=SYNTH_API_BASE,\n",
    "            reason=\"style_matching_notebook\",\n",
    "            progress=True,\n",
    "        )\n",
    "        print(f\"Waiting for system DNS to resolve {tunnel.hostname}...\")\n",
    "        await wait_for_system_dns(tunnel.hostname)\n",
    "    except Exception as exc:\n",
    "        print(f\"Managed tunnel failed or DNS unresolved ({exc}). Falling back to quick tunnel...\")\n",
    "        tunnel = await TunneledLocalAPI.create(\n",
    "            local_port=LOCAL_TASK_PORT,\n",
    "            backend=TunnelBackend.CloudflareQuickTunnel,\n",
    "            env_api_key=ENVIRONMENT_API_KEY,\n",
    "            progress=True,\n",
    "        )\n",
    "\n",
    "if tunnel is not None:\n",
    "    TASK_APP_URL = tunnel.url\n",
    "print(f\"Task app URL: {TASK_APP_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59279a1d",
   "metadata": {},
   "source": [
    "## Step 5: Run GEPA With Baseline vs Optimized Verifiers\n",
    "\n",
    "We run GEPA twice: once with the baseline verifier and once with the optimized verifier graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7107309b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T20:04:23.056349Z",
     "iopub.status.busy": "2026-01-05T20:04:23.056020Z",
     "iopub.status.idle": "2026-01-05T20:16:56.893002Z",
     "shell.execute_reply": "2026-01-05T20:16:56.891988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running GEPA (baseline) with verifier: zero_shot_verifier_contrastive_single\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEPA job id (baseline): pl_b926ded654954042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:00] queued | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:03] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:06] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:09] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:12] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:16] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:19] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:22] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:25] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:28] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:32] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:35] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:38] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:41] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:45] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:48] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:51] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:54] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:57] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:01] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:04] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:07] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:10] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:14] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:17] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:20] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:23] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:26] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:30] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:33] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:36] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:39] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:43] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:46] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:49] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:52] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:56] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:59] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:02] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:05] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:08] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:12] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:15] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:18] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:21] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:25] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:28] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:31] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:34] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:37] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:41] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:44] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:47] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:50] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:54] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:57] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:00] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:03] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:06] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:10] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:13] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:16] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:19] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:23] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:26] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:29] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:32] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:36] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:39] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:42] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:46] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:49] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:52] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:56] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:59] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:02] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:06] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:09] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:12] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:16] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:19] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:22] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:25] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:29] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:32] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:35] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:38] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:42] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:45] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:48] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:51] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:55] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:58] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[05:01] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[05:04] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[05:08] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[05:11] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[05:14] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[05:18] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[05:21] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[05:24] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[05:28] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[05:31] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[05:34] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[05:38] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[05:41] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[05:44] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[05:48] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[05:51] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[05:54] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[05:57] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:01] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:04] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:07] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:10] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:13] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:16] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:19] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:23] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:26] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:29] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:32] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:35] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:38] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:41] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:45] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:48] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:51] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:54] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[06:57] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[07:00] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[07:03] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[07:07] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[07:10] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[07:13] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[07:16] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[07:19] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[07:23] succeeded | score: 0.28                    \n",
      "GEPA finished (baseline): succeeded\n",
      "\n",
      "Running GEPA (optimized) with verifier: graph_evolve_4aca6751f48a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEPA job id (optimized): pl_eee7add542264148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:00] queued | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:03] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:06] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:10] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:13] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:16] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:19] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:23] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:26] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:29] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:32] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:35] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:39] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:42] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:45] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:48] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:51] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:55] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[00:58] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:01] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:04] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:08] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:11] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:14] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:17] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:21] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:24] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:27] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:30] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:33] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:37] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:40] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:43] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:46] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:50] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:53] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:56] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[01:59] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:02] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:06] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:09] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:12] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:15] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:19] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:22] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:25] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:28] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:32] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:35] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:38] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:41] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:45] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:48] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:51] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:54] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[02:58] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:01] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:04] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:07] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:10] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:14] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:17] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:20] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:24] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:27] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:30] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:33] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:37] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:40] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:43] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:46] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:49] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:53] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:56] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[03:59] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:02] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:05] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:08] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:11] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:14] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:17] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:20] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:23] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:26] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:29] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:32] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:35] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:38] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:41] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:44] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:47] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:50] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:53] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:56] running | score: --                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[04:59] succeeded | score: 0.85                    \n",
      "GEPA finished (optimized): succeeded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline best score: 0.27849999999999997\n",
      "Optimized best score: 0.8499999999999999\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Run GEPA (baseline vs optimized verifier)\n",
    "\n",
    "POLICY_MODEL = \"gpt-4.1-nano\"\n",
    "\n",
    "\n",
    "def _make_gepa_config(task_app_url: str) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"prompt_learning\": {\n",
    "            \"algorithm\": \"gepa\",\n",
    "            \"task_app_url\": task_app_url,\n",
    "            \"task_app_api_key\": ENVIRONMENT_API_KEY,\n",
    "            \"env_name\": \"style-matching\",\n",
    "            \"initial_prompt\": {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"order\": 0, \"pattern\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"order\": 1, \"pattern\": USER_PROMPT_TEMPLATE},\n",
    "                ],\n",
    "                \"wildcards\": {\"title\": \"REQUIRED\", \"outline\": \"REQUIRED\", \"notes\": \"REQUIRED\"},\n",
    "            },\n",
    "            \"policy\": {\n",
    "                \"inference_mode\": \"synth_hosted\",\n",
    "                \"model\": POLICY_MODEL,\n",
    "                \"provider\": \"openai\",\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_completion_tokens\": 1200,\n",
    "            },\n",
    "            \"gepa\": {\n",
    "                \"env_name\": \"style-matching\",\n",
    "                \"evaluation\": {\"seeds\": list(range(13)), \"validation_seeds\": list(range(13, 17))},\n",
    "                \"rollout\": {\"budget\": 48, \"max_concurrent\": 3, \"minibatch_size\": 3},\n",
    "                \"mutation\": {\"rate\": 0.3},\n",
    "                \"population\": {\n",
    "                    \"initial_size\": 3,\n",
    "                    \"num_generations\": 3,\n",
    "                    \"children_per_generation\": 2,\n",
    "                },\n",
    "                \"archive\": {\"size\": 5, \"pareto_set_size\": 10},\n",
    "                \"token\": {\"counting_model\": \"gpt-4\"},\n",
    "            },\n",
    "            \"verifier\": {\"enabled\": False, \"reward_source\": \"task_app\"},\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "async def run_gepa_with_verifier(label: str, verifier_job_id: str):\n",
    "    global VERIFIER_JOB_ID\n",
    "    VERIFIER_JOB_ID = verifier_job_id\n",
    "    print(f\"\\nRunning GEPA ({label}) with verifier: {verifier_job_id}\")\n",
    "\n",
    "    config_body = _make_gepa_config(TASK_APP_URL)\n",
    "    job = PromptLearningJob.from_dict(\n",
    "        config_dict=config_body,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        task_app_api_key=ENVIRONMENT_API_KEY,\n",
    "    )\n",
    "\n",
    "    job_id = job.submit()\n",
    "    print(f\"GEPA job id ({label}): {job_id}\")\n",
    "\n",
    "    result = job.poll_until_complete(timeout=3600.0, interval=3.0, progress=True)\n",
    "    print(f\"GEPA finished ({label}): {result.status.value}\")\n",
    "    if result.failed:\n",
    "        error_msg = result.error\n",
    "        if not error_msg:\n",
    "            try:\n",
    "                client = PromptLearningClient(SYNTH_API_BASE, API_KEY)\n",
    "                events = await client.get_events(job_id, limit=5000)\n",
    "            except Exception as exc:\n",
    "                error_msg = f\"Failed to fetch job events: {exc}\"\n",
    "            else:\n",
    "                for event in reversed(events):\n",
    "                    event_type = str(event.get(\"type\", \"\"))\n",
    "                    if event_type in {\"prompt.learning.failed\", \"mipro.job.failed\", \"job.failed\"}:\n",
    "                        error_msg = event.get(\"message\") or event.get(\"data\")\n",
    "                        break\n",
    "        raise RuntimeError(f\"GEPA job failed ({label}): {error_msg or 'unknown error'}\")\n",
    "    return job_id, result\n",
    "\n",
    "\n",
    "baseline_gepa_job_id, baseline_gepa_result = await run_gepa_with_verifier(\n",
    "    \"baseline\", BASELINE_VERIFIER_JOB_ID\n",
    ")\n",
    "\n",
    "optimized_gepa_job_id, optimized_gepa_result = await run_gepa_with_verifier(\n",
    "    \"optimized\", OPTIMIZED_VERIFIER_JOB_ID\n",
    ")\n",
    "\n",
    "# Fetch prompts\n",
    "pl_client = PromptLearningClient(SYNTH_API_BASE, API_KEY)\n",
    "\n",
    "baseline_prompts = await pl_client.get_prompts(baseline_gepa_job_id)\n",
    "optimized_prompts = await pl_client.get_prompts(optimized_gepa_job_id)\n",
    "\n",
    "\n",
    "def _select_prompt(result):\n",
    "    best = result.best_prompt\n",
    "    if best:\n",
    "        return best\n",
    "    top = result.top_prompts or []\n",
    "    if top:\n",
    "        return top[0]\n",
    "    raise RuntimeError(\"No prompts returned from GEPA job\")\n",
    "\n",
    "\n",
    "baseline_prompt_obj = _select_prompt(baseline_prompts)\n",
    "optimized_prompt_obj = _select_prompt(optimized_prompts)\n",
    "\n",
    "print(\"\\nBaseline best score:\", baseline_prompts.best_score)\n",
    "print(\"Optimized best score:\", optimized_prompts.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3305a57f",
   "metadata": {},
   "source": [
    "## Step 6: Heldout Evaluation (4 Final Scores)\n",
    "\n",
    "We evaluate both best prompts on a heldout set using **eval jobs** and both verifiers:\n",
    "\n",
    "1. Baseline prompt + baseline verifier\n",
    "2. Baseline prompt + optimized verifier\n",
    "3. Optimized prompt + baseline verifier\n",
    "4. Optimized prompt + optimized verifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8d5468",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T20:16:56.895826Z",
     "iopub.status.busy": "2026-01-05T20:16:56.895683Z",
     "iopub.status.idle": "2026-01-05T20:18:26.977335Z",
     "shell.execute_reply": "2026-01-05T20:18:26.976589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval job (baseline_prompt__baseline_verifier): eval_cd3258b0559148fb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:03] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:06] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:09] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:13] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:16] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:19] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:22] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:25] completed | mean_score: 0.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval job (baseline_prompt__optimized_verifier): eval_d70138772e7e48a5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:03] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:06] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:09] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:12] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:16] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:19] completed | mean_score: 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval job (optimized_prompt__baseline_verifier): eval_5a282dd87db84afb\n",
      "[00:00] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:03] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:06] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:09] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:12] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:16] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:19] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:22] completed | mean_score: 0.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval job (optimized_prompt__optimized_verifier): eval_9eef1a1fe97e458e\n",
      "[00:00] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:03] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:06] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:09] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:12] running | 0/4 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:15] completed | mean_score: 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Heldout results (mean verifier score):\n",
      "- baseline_prompt__baseline_verifier: 0.346\n",
      "- baseline_prompt__optimized_verifier: 0.850\n",
      "- optimized_prompt__baseline_verifier: 0.190\n",
      "- optimized_prompt__optimized_verifier: 0.850\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Heldout evaluation via eval jobs\n",
    "\n",
    "HELDOUT_SEEDS = [20, 21, 22, 23]\n",
    "EVAL_POLICY_MODEL = \"gpt-4.1-nano\"\n",
    "\n",
    "\n",
    "def _extract_prompt_sections(prompt_obj: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    template = prompt_obj.get(\"template\") or {}\n",
    "    sections = template.get(\"sections\")\n",
    "    if isinstance(sections, list) and sections:\n",
    "        return sections\n",
    "    messages = prompt_obj.get(\"messages\")\n",
    "    if isinstance(messages, list) and messages:\n",
    "        return messages\n",
    "    return []\n",
    "\n",
    "\n",
    "async def run_eval_job(\n",
    "    label: str, prompt_sections: List[Dict[str, Any]], verifier_job_id: str\n",
    ") -> float:\n",
    "    config = EvalJobConfig(\n",
    "        task_app_url=TASK_APP_URL,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        api_key=API_KEY,\n",
    "        task_app_api_key=ENVIRONMENT_API_KEY,\n",
    "        env_name=\"style-matching\",\n",
    "        seeds=HELDOUT_SEEDS,\n",
    "        policy_config={\"model\": EVAL_POLICY_MODEL, \"provider\": \"openai\"},\n",
    "        env_config={\n",
    "            \"prompt_sections\": prompt_sections,\n",
    "            \"verifier_job_id\": verifier_job_id,\n",
    "        },\n",
    "        concurrency=3,\n",
    "    )\n",
    "\n",
    "    job = EvalJob(config)\n",
    "    job_id = job.submit()\n",
    "    print(f\"Eval job ({label}): {job_id}\")\n",
    "\n",
    "    result = job.poll_until_complete(timeout=1800.0, interval=3.0, progress=True)\n",
    "    if not result.succeeded:\n",
    "        raise RuntimeError(f\"Eval job failed ({label}): {result.error}\")\n",
    "    return result.mean_score or 0.0\n",
    "\n",
    "\n",
    "baseline_prompt = baseline_prompt_obj\n",
    "optimized_prompt = optimized_prompt_obj\n",
    "\n",
    "baseline_sections = _extract_prompt_sections(baseline_prompt)\n",
    "optimized_sections = _extract_prompt_sections(optimized_prompt)\n",
    "\n",
    "results = {}\n",
    "results[\"baseline_prompt__baseline_verifier\"] = await run_eval_job(\n",
    "    \"baseline_prompt__baseline_verifier\", baseline_sections, BASELINE_VERIFIER_JOB_ID\n",
    ")\n",
    "results[\"baseline_prompt__optimized_verifier\"] = await run_eval_job(\n",
    "    \"baseline_prompt__optimized_verifier\", baseline_sections, OPTIMIZED_VERIFIER_JOB_ID\n",
    ")\n",
    "results[\"optimized_prompt__baseline_verifier\"] = await run_eval_job(\n",
    "    \"optimized_prompt__baseline_verifier\", optimized_sections, BASELINE_VERIFIER_JOB_ID\n",
    ")\n",
    "results[\"optimized_prompt__optimized_verifier\"] = await run_eval_job(\n",
    "    \"optimized_prompt__optimized_verifier\", optimized_sections, OPTIMIZED_VERIFIER_JOB_ID\n",
    ")\n",
    "\n",
    "print(\"\\nHeldout results (mean verifier score):\")\n",
    "for k, v in results.items():\n",
    "    print(f\"- {k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b573af83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T20:18:26.980192Z",
     "iopub.status.busy": "2026-01-05T20:18:26.980054Z",
     "iopub.status.idle": "2026-01-05T20:18:26.982890Z",
     "shell.execute_reply": "2026-01-05T20:18:26.982383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up tunnels...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "print(\"Cleaning up tunnels...\")\n",
    "cleanup_all()\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}