name: style_matching_verifier
metadata:
  type: VerifierGraph
  description: VerifierGraph that scores a trace for style-matching quality using strict, deduction-based scoring against
    gold examples. Produces event_reviews, outcome_review, event_totals, and score with rich reflective feedback.
start_nodes:
- judge_style
nodes:
  judge_style:
    name: judge_style
    type: DagNode
    input_mapping: '{"trace": state.get("trace", {}), "gold_examples": state.get("gold_examples", [])}'
    output_mapping:
      event_reviews: judge_style_output.event_reviews
      outcome_review: judge_style_output.outcome_review
      event_totals: judge_style_output.event_totals
      score: judge_style_output.outcome_review.total
    implementation:
      type: template_transform
      model_name: gpt-4.1-nano
      temperature: 0.0
      max_tokens: 1024
      sequence:
        template_likes:
        - type: prompt_template
          message_type: SYSTEM
          structured_template: You are a strict style-matching verifier. Your job is to evaluate the style of assistant responses
            in a trace against provided gold examples. Use a 1.0 baseline and deduct for any discrepancies in tone, directness,
            stance, or closing line. Generic or off-style outputs should score below 0.3. Provide rich, actionable feedback
            in your reviews.
        - type: prompt_template
          message_type: USER
          input_fields:
            trace:
              field_name: trace
              description: Session trace with events.
              is_required: true
            gold_examples:
              field_name: gold_examples
              description: Gold examples with scores and traces.
              is_required: true
          structured_template: "Evaluate the assistant's style in the following trace by comparing each assistant_message\
            \ event to the gold examples. For each assistant_message, review:\n- Tone (direct, builder, opinionated)\n- Clarity\
            \ and conciseness\n- Use of concrete advice/examples\n- Closing line quality\n\nDeduct from 1.0 for any mismatch.\
            \ If the output is generic or off-style, score below 0.3. Use the gold_examples as the style target (see their\
            \ summaries and gold_reasoning).\n\nFor each assistant_message event, produce an event_review object:\n- criteria:\
            \ {\"tone\": float, \"clarity\": float, \"concreteness\": float, \"closing\": float}\n- total: float (0.0-1.0)\n\
            - summary: short text\n\nAlso produce:\n- event_totals: list of total scores for each event_review\n- outcome_review:\
            \ {\"criteria\": {\"overall\": float}, \"total\": float, \"summary\": string, \"annotation\": {\"feedback\": string,\
            \ \"strengths\": [string], \"weaknesses\": [string], \"missed_opportunities\": [string], \"next_actions\": [string]}}\n\
            - score: float (should match outcome_review.total)\n\nOutput EXACTLY as JSON:\n{\n  \"event_reviews\": [\n   \
            \ {\"criteria\": {\"tone\": 0.9, \"clarity\": 1.0, \"concreteness\": 0.95, \"closing\": 0.9}, \"total\": 0.94,\
            \ \"summary\": \"Direct, clear, and matches builder tone.\"}\n  ],\n  \"event_totals\": [0.94],\n  \"outcome_review\"\
            : {\n    \"criteria\": {\"overall\": 0.94},\n    \"total\": 0.94,\n    \"summary\": \"Strong style match with\
            \ minor gaps.\",\n    \"annotation\": {\n      \"feedback\": \"The response is direct and concise, closely matching\
            \ the gold style. Consider adding a sharper closing line for full marks.\",\n      \"strengths\": [\"Direct tone\"\
            , \"Concrete advice\", \"Concise phrasing\"],\n      \"weaknesses\": [\"Closing line could be more memorable\"\
            ],\n      \"missed_opportunities\": [\"No strong closing statement\"],\n      \"next_actions\": [\"End with a\
            \ punchy, memorable line\"]\n    }\n  },\n  \"score\": 0.94\n}\n\nTrace:\n<input>trace</input>\n\nGold Examples:\n\
            <input>gold_examples</input>"
control_edges:
  judge_style: []
verdict_weights:
  judge_style: 1.0
aggregation_policy: weighted_average
