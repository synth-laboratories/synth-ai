{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Synth GEPA Demo - OOLONG RLM (MIT)\n",
    "\n",
    "This notebook demonstrates running GEPA prompt optimization through a **Synth Local API** task app that uses an **RLM (Recursive Language Model)** from the `rlm` library on the OOLONG dataset.\n",
    "\n",
    "We build a local task app that calls `rlm.RLM` for each rollout, expose it via a tunnel (or local URL), and then run a GEPA job with Synth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Install dependencies (Colab only)\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    import os\n",
    "\n",
    "    _INSTALLED_MARKER = \"/content/.synth_deps_v2\"\n",
    "\n",
    "    if os.path.exists(_INSTALLED_MARKER):\n",
    "        print(\"Dependencies ready.\")\n",
    "    else:\n",
    "        print(\"Installing dependencies...\")\n",
    "        %pip install -q git+https://github.com/alexzhang13/rlm.git datasets synth-ai\n",
    "\n",
    "        with open(_INSTALLED_MARKER, 'w') as f:\n",
    "            f.write(\"ok\")\n",
    "\n",
    "        print(\"Done! Restarting runtime...\")\n",
    "        os.kill(os.getpid(), 9)\n",
    "else:\n",
    "    print(\"Not in Colab - assuming dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Configure imports and API keys in one place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setup - imports, config, and API keys\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Iterable, List, Optional\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from synth_ai.sdk.api.train.prompt_learning import PromptLearningJob\n",
    "from synth_ai.sdk.localapi import LocalAPIConfig, create_local_api\n",
    "from synth_ai.sdk.localapi.auth import ensure_localapi_auth\n",
    "from synth_ai.sdk.localapi.helpers import extract_api_key\n",
    "from synth_ai.sdk.task import run_server_background\n",
    "from synth_ai.sdk.task.contracts import RolloutMetrics, RolloutRequest, RolloutResponse, TaskInfo\n",
    "from synth_ai.sdk.tunnels import TunneledLocalAPI, TunnelBackend, kill_port, wait_for_health_check\n",
    "\n",
    "from rlm import RLM\n",
    "from rlm.utils.prompts import RLM_SYSTEM_PROMPT, USER_PROMPT, USER_PROMPT_WITH_ROOT\n",
    "\n",
    "# Work around rlm QueryMetadata typing bug under Python 3.11\n",
    "from rlm.core import rlm as rlm_core\n",
    "from rlm.core import types as rlm_types\n",
    "\n",
    "\n",
    "class PatchedQueryMetadata:\n",
    "    def __init__(self, prompt):\n",
    "        if isinstance(prompt, str):\n",
    "            self.context_lengths = [len(prompt)]\n",
    "            self.context_type = \"str\"\n",
    "        elif isinstance(prompt, dict):\n",
    "            self.context_lengths = [len(chunk) for chunk in prompt.values()]\n",
    "            self.context_type = \"dict\"\n",
    "        elif isinstance(prompt, list):\n",
    "            self.context_type = \"list\"\n",
    "            if prompt and isinstance(prompt[0], dict):\n",
    "                if \"content\" in prompt[0]:\n",
    "                    self.context_lengths = [len(chunk[\"content\"]) for chunk in prompt]\n",
    "                else:\n",
    "                    self.context_lengths = [len(chunk) for chunk in prompt]\n",
    "            else:\n",
    "                self.context_lengths = [len(chunk) for chunk in prompt]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid prompt type: {type(prompt)}\")\n",
    "\n",
    "        self.context_total_length = sum(self.context_lengths)\n",
    "\n",
    "\n",
    "rlm_types.QueryMetadata = PatchedQueryMetadata\n",
    "rlm_core.QueryMetadata = PatchedQueryMetadata\n",
    "\n",
    "\n",
    "def patched_build_rlm_system_prompt(system_prompt, query_metadata=None, **_kwargs):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": \"{context_metadata}\"},\n",
    "    ]\n",
    "\n",
    "\n",
    "from rlm.utils import prompts as rlm_prompts\n",
    "\n",
    "rlm_prompts.build_rlm_system_prompt = patched_build_rlm_system_prompt\n",
    "rlm_core.build_rlm_system_prompt = patched_build_rlm_system_prompt\n",
    "\n",
    "\n",
    "from synth_ai.core.urls import BACKEND_URL_BASE\n",
    "from synth_ai.sdk.auth import get_or_mint_synth_api_key\n",
    "\n",
    "SYNTH_API_BASE = BACKEND_URL_BASE\n",
    "SYNTH_API_KEY = get_or_mint_synth_api_key(backend_url=SYNTH_API_BASE)\n",
    "ENVIRONMENT_API_KEY = ensure_localapi_auth(\n",
    "    backend_base=SYNTH_API_BASE,\n",
    "    synth_api_key=SYNTH_API_KEY,\n",
    ")\n",
    "\n",
    "LOCAL_API_PORT = int(os.getenv(\"LOCAL_API_PORT\", \"8115\"))\n",
    "USE_TUNNEL = os.getenv(\"USE_TUNNEL\", \"true\").lower() in {\"1\", \"true\", \"yes\", \"y\"}\n",
    "\n",
    "os.environ[\"SYNTH_API_KEY\"] = SYNTH_API_KEY\n",
    "\n",
    "print(\"Config loaded\")\n",
    "\n",
    "RLM_BASE_SYSTEM_PROMPT = (\n",
    "    \"You are a recursive language model. Use the REPL with the context variable to reason. \"\n",
    "    \"Call llm_query or llm_query_batched as needed. When finished, answer with FINAL.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Loader (OOLONG)\n",
    "\n",
    "We lazily load `oolongbench/oolong-real` (config: `dnd`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: OOLONG dataset wrapper\n",
    "@dataclass\n",
    "class OolongSample:\n",
    "    index: int\n",
    "    split: str\n",
    "    query: str\n",
    "    context: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "class OolongDataset:\n",
    "    def __init__(self, hf_dataset: str = \"oolongbench/oolong-real\", hf_config: str = \"dnd\"):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.hf_config = hf_config\n",
    "        self._cache = {}\n",
    "\n",
    "    def _load_split(self, split: str):\n",
    "        if split not in self._cache:\n",
    "            ds = load_dataset(self.hf_dataset, self.hf_config, split=split)\n",
    "            self._cache[split] = ds\n",
    "        return self._cache[split]\n",
    "\n",
    "    def ensure_ready(self, splits: Iterable[str]) -> None:\n",
    "        for split in splits:\n",
    "            self._load_split(split)\n",
    "\n",
    "    def size(self, split: str) -> int:\n",
    "        return len(self._load_split(split))\n",
    "\n",
    "    def sample(self, split: str, index: int) -> OolongSample:\n",
    "        ds = self._load_split(split)\n",
    "        idx = index % len(ds)\n",
    "        row = ds[idx]\n",
    "        query = row.get(\"query\") or row.get(\"question\") or \"\"\n",
    "        context = row.get(\"context_window_text\") or row.get(\"context\") or row.get(\"text\") or \"\"\n",
    "        answer = row.get(\"answer\") or \"\"\n",
    "        return OolongSample(\n",
    "            index=idx,\n",
    "            split=split,\n",
    "            query=str(query),\n",
    "            context=str(context),\n",
    "            answer=str(answer),\n",
    "        )\n",
    "\n",
    "\n",
    "oolong = OolongDataset()\n",
    "oolong.ensure_ready([\"validation\", \"test\"])\n",
    "print(\"Dataset ready:\", oolong.size(\"validation\"), oolong.size(\"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Step 3: Prompt Template Rendering\n",
    "\n",
    "GEPA sends candidate prompts to the task app via `request.policy.config.prompt_template`.\n",
    "We render those sections into messages, and use them to drive the RLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prompt template helpers\n",
    "def _normalize_prompt_template(policy_config: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    template = policy_config.get(\"prompt_template\") or {}\n",
    "    if not isinstance(template, dict):\n",
    "        template = {}\n",
    "    return template\n",
    "\n",
    "\n",
    "def _get_prompt_sections(policy_config: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    template = _normalize_prompt_template(policy_config)\n",
    "    sections = (\n",
    "        template.get(\"sections\")\n",
    "        or template.get(\"prompt_sections\")\n",
    "        or policy_config.get(\"prompt_sections\")\n",
    "        or []\n",
    "    )\n",
    "    if not isinstance(sections, list):\n",
    "        return []\n",
    "    return sorted(sections, key=lambda s: s.get(\"order\", 0))\n",
    "\n",
    "\n",
    "def render_prompt_sections(\n",
    "    sections: List[Dict[str, Any]], placeholders: Dict[str, str]\n",
    ") -> List[Dict[str, str]]:\n",
    "    rendered: List[Dict[str, str]] = []\n",
    "    for section in sections:\n",
    "        role = section.get(\"role\", \"user\")\n",
    "        pattern = section.get(\"content\") or section.get(\"pattern\") or \"\"\n",
    "        content = pattern.format(**placeholders)\n",
    "        rendered.append({\"role\": role, \"content\": content})\n",
    "    return rendered\n",
    "\n",
    "\n",
    "def split_system_and_user(messages: List[Dict[str, str]]) -> tuple[str, str]:\n",
    "    system_parts = [m[\"content\"] for m in messages if m.get(\"role\") == \"system\"]\n",
    "    user_parts = [m[\"content\"] for m in messages if m.get(\"role\") != \"system\"]\n",
    "    system_prompt = \"\\n\\n\".join(system_parts).strip()\n",
    "    user_prompt = \"\\n\\n\".join(user_parts).strip()\n",
    "    return system_prompt, user_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Step 4: Local API Factory (RLM Task App)\n",
    "\n",
    "We implement a Local API that calls `rlm.RLM` for each rollout.\n",
    "The task app reads the prompt template from `request.policy.config`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Local API for OOLONG RLM\n",
    "APP_ID = \"oolong_rlm\"\n",
    "APP_NAME = \"OOLONG RLM (Recursive Language Model) QA\"\n",
    "\n",
    "\n",
    "def normalize_answer(text: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    return \"\".join(ch.lower() for ch in text.strip() if ch.isalnum() or ch.isspace()).strip()\n",
    "\n",
    "\n",
    "def create_oolong_rlm_local_api():\n",
    "    async def run_rollout(request: RolloutRequest, fastapi_request) -> RolloutResponse:\n",
    "        policy_config = request.policy.config or {}\n",
    "        split = env_config.get(\"split\", \"validation\")\n",
    "\n",
    "        sample = oolong.sample(split=split, index=seed)\n",
    "        placeholders = {\n",
    "            \"query\": sample.query,\n",
    "            \"context\": sample.context,\n",
    "            \"context_metadata\": \"{context_metadata}\",\n",
    "        }\n",
    "\n",
    "        sections = _get_prompt_sections(policy_config)\n",
    "        if not sections:\n",
    "            sections = [\n",
    "                {\"role\": \"system\", \"content\": COMPOSED_SYSTEM_PROMPT, \"order\": 0},\n",
    "                {\"role\": \"assistant\", \"content\": RLM_CONTEXT_METADATA_PATTERN, \"order\": 1},\n",
    "                {\"role\": \"user\", \"content\": RLM_FIRST_USER_PROMPT, \"order\": 2},\n",
    "                {\"role\": \"user\", \"content\": BASELINE_USER_PROMPT, \"order\": 3},\n",
    "            ]\n",
    "        rendered = render_prompt_sections(sections, placeholders)\n",
    "        messages_for_validation = []\n",
    "        for section in sections:\n",
    "            role = section.get(\"role\", \"user\")\n",
    "            pattern = section.get(\"content\") or section.get(\"pattern\") or \"\"\n",
    "            messages_for_validation.append({\"role\": role, \"content\": pattern})\n",
    "\n",
    "        system_prompt, root_prompt = split_system_and_user(rendered)\n",
    "        if system_prompt:\n",
    "            custom_system_prompt = system_prompt\n",
    "        else:\n",
    "            custom_system_prompt = RLM_BASE_SYSTEM_PROMPT\n",
    "        inference_url = (\n",
    "            policy_config.get(\"inference_url\")\n",
    "            or policy_config.get(\"api_base\")\n",
    "            or policy_config.get(\"base_url\")\n",
    "        )\n",
    "        if not inference_url:\n",
    "            raise ValueError(\"Missing inference_url in policy config\")\n",
    "\n",
    "        api_key = policy_config.get(\"api_key\") or SYNTH_API_KEY\n",
    "        if not api_key:\n",
    "            raise ValueError(\"Missing policy api_key for inference proxy\")\n",
    "\n",
    "        model_name = policy_config.get(\"model\", \"gpt-4o-mini\")\n",
    "        max_iterations = int(env_config.get(\"max_iterations\", 2))\n",
    "        max_depth = int(env_config.get(\"max_depth\", 0))\n",
    "\n",
    "        rlm = RLM(\n",
    "            backend=\"openai\",\n",
    "            backend_kwargs={\n",
    "                \"model_name\": model_name,\n",
    "                \"api_key\": api_key,\n",
    "                \"base_url\": inference_url,\n",
    "            },\n",
    "            environment=\"local\",\n",
    "            environment_kwargs={},\n",
    "            custom_system_prompt=custom_system_prompt,\n",
    "            max_iterations=max_iterations,\n",
    "            max_depth=max_depth,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        prompt_payload = rendered\n",
    "        completion = rlm.completion(\n",
    "            prompt_payload,\n",
    "        )\n",
    "\n",
    "        if isinstance(completion, str):\n",
    "            predicted = completion\n",
    "        else:\n",
    "            predicted = completion.response or \"\"\n",
    "        gold = sample.answer or \"\"\n",
    "\n",
    "        reward = 1.0 if normalize_answer(predicted) == normalize_answer(gold) else 0.0\n",
    "\n",
    "        return RolloutResponse(\n",
    "            run_id=request.run_id,\n",
    "            reward_info=RolloutMetrics(\n",
    "                outcome_reward=reward,\n",
    "                details={\"messages\": messages_for_validation, \"predicted\": predicted, \"gold\": gold},\n",
    "            ),\n",
    "            trace=None,\n",
    "            trace_correlation_id=policy_config.get(\"trace_correlation_id\"),\n",
    "        )\n",
    "\n",
    "    def provide_taskset_description():\n",
    "        return {\n",
    "            \"splits\": [\"validation\", \"test\"],\n",
    "            \"sizes\": {\"validation\": oolong.size(\"validation\"), \"test\": oolong.size(\"test\")},\n",
    "        }\n",
    "\n",
    "    def provide_task_instances(seeds):\n",
    "        for seed in seeds:\n",
    "            sample = oolong.sample(split=\"validation\", index=seed)\n",
    "            yield TaskInfo(\n",
    "                task={\"id\": APP_ID, \"name\": APP_NAME},\n",
    "                dataset={\"id\": APP_ID, \"split\": sample.split, \"index\": sample.index},\n",
    "                inference={\"tool\": \"rlm_repl\"},\n",
    "                limits={\"max_turns\": 1},\n",
    "                task_metadata={\"query\": sample.query},\n",
    "            )\n",
    "\n",
    "    return create_local_api(\n",
    "        LocalAPIConfig(\n",
    "            app_id=APP_ID,\n",
    "            name=APP_NAME,\n",
    "            description=\"OOLONG RLM local API for prompt optimization.\",\n",
    "            provide_taskset_description=provide_taskset_description,\n",
    "            provide_task_instances=provide_task_instances,\n",
    "            rollout=run_rollout,\n",
    "            cors_origins=[\"*\"],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step 5: Start the Local API\n",
    "\n",
    "This spins up the task app and exposes it (optionally via a tunnel).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Start Local API\n",
    "print(\"Starting local API...\")\n",
    "app = create_oolong_rlm_local_api()\n",
    "\n",
    "kill_port(LOCAL_API_PORT)\n",
    "run_server_background(app, LOCAL_API_PORT)\n",
    "\n",
    "print(f\"Waiting for local API on port {LOCAL_API_PORT}...\")\n",
    "await wait_for_health_check(\"localhost\", LOCAL_API_PORT, ENVIRONMENT_API_KEY, timeout=60.0)\n",
    "print(\"Local API ready!\")\n",
    "\n",
    "if USE_TUNNEL:\n",
    "    print(\"Provisioning Cloudflare tunnel...\")\n",
    "    tunnel = await TunneledLocalAPI.create(\n",
    "        local_port=LOCAL_API_PORT,\n",
    "        backend=TunnelBackend.CloudflareManagedTunnel,\n",
    "        api_key=SYNTH_API_KEY,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        progress=True,\n",
    "    )\n",
    "    LOCAL_API_URL = tunnel.url\n",
    "else:\n",
    "    LOCAL_API_URL = f\"http://localhost:{LOCAL_API_PORT}\"\n",
    "\n",
    "print(\"Local API URL:\", LOCAL_API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Step 6: Run GEPA Prompt Optimization\n",
    "\n",
    "We configure GEPA to optimize the prompt sections passed to the local RLM task app.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: GEPA optimization\n",
    "BASELINE_SYSTEM_PROMPT = \"Answer questions using the context.\"\n",
    "BASELINE_USER_PROMPT = (\n",
    "    \"Query: {query}\\n\\nContext:\\n{context}\\n\\nAnswer the query using the context.\"\n",
    ")\n",
    "\n",
    "RLM_CONTEXT_METADATA_PATTERN = \"{context_metadata}\"\n",
    "RLM_FIRST_USER_PROMPT = (\n",
    "    \"You have not interacted with the REPL environment or seen your prompt / context yet. \"\n",
    "    \"Your next action should be to look through and figure out how to answer the prompt, \"\n",
    "    \"so don't just provide a final answer yet.\\n\\n\" + USER_PROMPT\n",
    ")\n",
    "\n",
    "COMPOSED_SYSTEM_PROMPT = RLM_BASE_SYSTEM_PROMPT + \" \" + BASELINE_SYSTEM_PROMPT\n",
    "\n",
    "config_body = {\n",
    "    \"prompt_learning\": {\n",
    "        \"algorithm\": \"gepa\",\n",
    "        \"task_app_url\": LOCAL_API_URL,\n",
    "        \"env_name\": \"oolong\",\n",
    "        \"initial_prompt\": {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"order\": 0, \"pattern\": COMPOSED_SYSTEM_PROMPT},\n",
    "                {\"role\": \"assistant\", \"order\": 1, \"pattern\": RLM_CONTEXT_METADATA_PATTERN},\n",
    "                {\"role\": \"user\", \"order\": 2, \"pattern\": RLM_FIRST_USER_PROMPT},\n",
    "                {\"role\": \"user\", \"order\": 3, \"pattern\": BASELINE_USER_PROMPT},\n",
    "            ],\n",
    "            \"wildcards\": {\n",
    "                \"query\": \"REQUIRED\",\n",
    "                \"context\": \"REQUIRED\",\n",
    "                \"context_metadata\": \"REQUIRED\",\n",
    "            },\n",
    "        },\n",
    "        \"policy\": {\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"inference_mode\": \"synth_hosted\",\n",
    "            \"provider\": \"openai\",\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_completion_tokens\": 256,\n",
    "        },\n",
    "        \"gepa\": {\n",
    "            \"env_name\": \"oolong\",\n",
    "            \"evaluation\": {\n",
    "                \"seeds\": list(range(13)),\n",
    "                \"validation_seeds\": list(range(13, 15)),\n",
    "            },\n",
    "            \"rollout\": {\"budget\": 6, \"max_concurrent\": 3, \"minibatch_size\": 3},\n",
    "            \"mutation\": {\"rate\": 0.3},\n",
    "            \"population\": {\"initial_size\": 2, \"num_generations\": 1, \"children_per_generation\": 1},\n",
    "            \"archive\": {\"size\": 10, \"pareto_set_size\": 10},\n",
    "            \"token\": {\"counting_model\": \"gpt-4\"},\n",
    "        },\n",
    "        \"env_config\": {\n",
    "            \"split\": \"validation\",\n",
    "            \"max_iterations\": 2,\n",
    "            \"max_depth\": 0,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "job = PromptLearningJob.from_dict(\n",
    "    config_dict=config_body,\n",
    ")\n",
    "\n",
    "job_id = job.submit()\n",
    "print(\"GEPA job submitted:\", job_id)\n",
    "result = job.poll_until_complete(timeout=3600.0, interval=5.0, progress=True)\n",
    "print(\"Final status:\", result.status.value)\n",
    "print(\"Best score:\", result.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Step 7: Next Steps\n",
    "\n",
    "- Inspect the best prompt and rerun a manual rollout.\n",
    "- Increase rollout budget and population size for stronger results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
