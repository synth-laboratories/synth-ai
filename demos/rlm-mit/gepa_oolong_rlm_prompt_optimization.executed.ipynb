{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd06df3",
   "metadata": {},
   "source": [
    "# Synth GEPA Demo - OOLONG RLM (MIT)\n",
    "\n",
    "This notebook demonstrates running GEPA prompt optimization through a **Synth Local API** task app that uses an **RLM (Recursive Language Model)** from the `rlm` library on the OOLONG dataset.\n",
    "\n",
    "We build a local task app that calls `rlm.RLM` for each rollout, expose it via a tunnel (or local URL), and then run a GEPA job with Synth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b4d318f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T04:58:06.488999Z",
     "iopub.status.busy": "2026-01-04T04:58:06.488807Z",
     "iopub.status.idle": "2026-01-04T04:58:06.494912Z",
     "shell.execute_reply": "2026-01-04T04:58:06.494249Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 0: Install dependencies (run this first on Colab)\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    %pip install -q git+https://github.com/alexzhang13/rlm.git datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c738f357",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Configure imports and API keys in one place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96232e43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T04:58:06.497277Z",
     "iopub.status.busy": "2026-01-04T04:58:06.497098Z",
     "iopub.status.idle": "2026-01-04T04:58:08.229945Z",
     "shell.execute_reply": "2026-01-04T04:58:08.229572Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshpurtell/Documents/GitHub/synth-ai/synth_ai/sdk/task/contracts.py:34: UserWarning: Field name \"schema\" in \"StructuredOutputConfig\" shadows an attribute in parent \"BaseModel\"\n",
      "  class StructuredOutputConfig(BaseModel):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[celery_app] EXPERIMENT_QUEUE_DB_PATH not set, will use default path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[celery_app] Using default database path: /Users/joshpurtell/.synth_ai/experiment_queue.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[celery_app] Initializing with database: /Users/joshpurtell/.synth_ai/experiment_queue.db (broker: redis://localhost:6379/0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setup - imports, config, and API keys\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Iterable, List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "\n",
    "from synth_ai.sdk.api.train.prompt_learning import PromptLearningJob\n",
    "from synth_ai.sdk.localapi import LocalAPIConfig, create_local_api\n",
    "from synth_ai.sdk.localapi.helpers import extract_api_key\n",
    "from synth_ai.sdk.task import run_server_background\n",
    "from synth_ai.sdk.task.contracts import RolloutMetrics, RolloutRequest, RolloutResponse, TaskInfo\n",
    "from synth_ai.sdk.tunnels import TunneledLocalAPI, TunnelBackend, kill_port, wait_for_health_check\n",
    "\n",
    "from rlm import RLM\n",
    "from rlm.utils.prompts import RLM_SYSTEM_PROMPT, USER_PROMPT_WITH_ROOT\n",
    "\n",
    "load_dotenv()\n",
    "# Work around rlm QueryMetadata typing bug under Python 3.11\n",
    "from rlm.core import rlm as rlm_core\n",
    "from rlm.core import types as rlm_types\n",
    "\n",
    "class PatchedQueryMetadata:\n",
    "    def __init__(self, prompt):\n",
    "        if isinstance(prompt, str):\n",
    "            self.context_lengths = [len(prompt)]\n",
    "            self.context_type = 'str'\n",
    "        elif isinstance(prompt, dict):\n",
    "            self.context_lengths = [len(chunk) for chunk in prompt.values()]\n",
    "            self.context_type = 'dict'\n",
    "        elif isinstance(prompt, list):\n",
    "            self.context_type = 'list'\n",
    "            if prompt and isinstance(prompt[0], dict):\n",
    "                if 'content' in prompt[0]:\n",
    "                    self.context_lengths = [len(chunk['content']) for chunk in prompt]\n",
    "                else:\n",
    "                    self.context_lengths = [len(chunk) for chunk in prompt]\n",
    "            else:\n",
    "                self.context_lengths = [len(chunk) for chunk in prompt]\n",
    "        else:\n",
    "            raise ValueError(f'Invalid prompt type: {type(prompt)}')\n",
    "\n",
    "        self.context_total_length = sum(self.context_lengths)\n",
    "\n",
    "rlm_types.QueryMetadata = PatchedQueryMetadata\n",
    "rlm_core.QueryMetadata = PatchedQueryMetadata\n",
    "\n",
    "def patched_build_rlm_system_prompt(system_prompt, query_metadata=None, **_kwargs):\n",
    "    return [\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'assistant', 'content': '{context_metadata}'},\n",
    "    ]\n",
    "\n",
    "from rlm.utils import prompts as rlm_prompts\n",
    "rlm_prompts.build_rlm_system_prompt = patched_build_rlm_system_prompt\n",
    "rlm_core.build_rlm_system_prompt = patched_build_rlm_system_prompt\n",
    "\n",
    "\n",
    "SYNTH_API_BASE = os.getenv('SYNTH_API_BASE', 'https://api.usesynth.ai')\n",
    "SYNTH_API_KEY = os.getenv('SYNTH_API_KEY', '')\n",
    "ENVIRONMENT_API_KEY = os.getenv('ENVIRONMENT_API_KEY', 'local_env_key')\n",
    "\n",
    "LOCAL_API_PORT = int(os.getenv('LOCAL_API_PORT', '8115'))\n",
    "USE_TUNNEL = os.getenv('USE_TUNNEL', 'true').lower() in {\n",
    "    '1', 'true', 'yes', 'y'\n",
    "}\n",
    "\n",
    "if not SYNTH_API_KEY:\n",
    "    raise ValueError('Missing SYNTH_API_KEY env var')\n",
    "\n",
    "print('Config loaded')\n",
    "\n",
    "RLM_BASE_SYSTEM_PROMPT = (\n",
    "    'You are a recursive language model. Use the REPL with the context variable to reason. '\n",
    "    'Call llm_query or llm_query_batched as needed. When finished, answer with FINAL.'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a8b1b",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Loader (OOLONG)\n",
    "\n",
    "We lazily load `oolongbench/oolong-real` (config: `dnd`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "778a5329",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T04:58:08.231105Z",
     "iopub.status.busy": "2026-01-04T04:58:08.230960Z",
     "iopub.status.idle": "2026-01-04T04:58:15.626954Z",
     "shell.execute_reply": "2026-01-04T04:58:15.626132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079ceddbd7a9451f8489a55debb9bad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea269067a33146e9905fb484d050b63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready: 5995 6072\n"
     ]
    }
   ],
   "source": [
    "# Step 2: OOLONG dataset wrapper\n",
    "@dataclass\n",
    "class OolongSample:\n",
    "    index: int\n",
    "    split: str\n",
    "    query: str\n",
    "    context: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "class OolongDataset:\n",
    "    def __init__(self, hf_dataset: str = 'oolongbench/oolong-real', hf_config: str = 'dnd'):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.hf_config = hf_config\n",
    "        self._cache = {}\n",
    "\n",
    "    def _load_split(self, split: str):\n",
    "        if split not in self._cache:\n",
    "            ds = load_dataset(self.hf_dataset, self.hf_config, split=split)\n",
    "            self._cache[split] = ds\n",
    "        return self._cache[split]\n",
    "\n",
    "    def ensure_ready(self, splits: Iterable[str]) -> None:\n",
    "        for split in splits:\n",
    "            self._load_split(split)\n",
    "\n",
    "    def size(self, split: str) -> int:\n",
    "        return len(self._load_split(split))\n",
    "\n",
    "    def sample(self, split: str, index: int) -> OolongSample:\n",
    "        ds = self._load_split(split)\n",
    "        idx = index % len(ds)\n",
    "        row = ds[idx]\n",
    "        query = row.get('query') or row.get('question') or ''\n",
    "        context = row.get('context_window_text') or row.get('context') or row.get('text') or ''\n",
    "        answer = row.get('answer') or ''\n",
    "        return OolongSample(\n",
    "            index=idx,\n",
    "            split=split,\n",
    "            query=str(query),\n",
    "            context=str(context),\n",
    "            answer=str(answer),\n",
    "        )\n",
    "\n",
    "\n",
    "oolong = OolongDataset()\n",
    "oolong.ensure_ready(['validation', 'test'])\n",
    "print('Dataset ready:', oolong.size('validation'), oolong.size('test'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aecec3f",
   "metadata": {},
   "source": [
    "## Step 3: Prompt Template Rendering\n",
    "\n",
    "GEPA sends candidate prompts to the task app via `request.policy.config.prompt_template`.\n",
    "We render those sections into messages, and use them to drive the RLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74eab862",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T04:58:15.630176Z",
     "iopub.status.busy": "2026-01-04T04:58:15.630073Z",
     "iopub.status.idle": "2026-01-04T04:58:15.633422Z",
     "shell.execute_reply": "2026-01-04T04:58:15.633020Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Prompt template helpers\n",
    "def _normalize_prompt_template(policy_config: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    template = policy_config.get('prompt_template') or {}\n",
    "    if not isinstance(template, dict):\n",
    "        template = {}\n",
    "    return template\n",
    "\n",
    "\n",
    "def _get_prompt_sections(policy_config: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    template = _normalize_prompt_template(policy_config)\n",
    "    sections = (\n",
    "        template.get('sections')\n",
    "        or template.get('prompt_sections')\n",
    "        or policy_config.get('prompt_sections')\n",
    "        or []\n",
    "    )\n",
    "    if not isinstance(sections, list):\n",
    "        return []\n",
    "    return sorted(sections, key=lambda s: s.get('order', 0))\n",
    "\n",
    "\n",
    "def render_prompt_sections(sections: List[Dict[str, Any]], placeholders: Dict[str, str]) -> List[Dict[str, str]]:\n",
    "    rendered: List[Dict[str, str]] = []\n",
    "    for section in sections:\n",
    "        role = section.get('role', 'user')\n",
    "        pattern = section.get('content') or section.get('pattern') or ''\n",
    "        content = pattern.format(**placeholders)\n",
    "        rendered.append({'role': role, 'content': content})\n",
    "    return rendered\n",
    "\n",
    "\n",
    "def split_system_and_user(messages: List[Dict[str, str]]) -> tuple[str, str]:\n",
    "    system_parts = [m['content'] for m in messages if m.get('role') == 'system']\n",
    "    user_parts = [m['content'] for m in messages if m.get('role') != 'system']\n",
    "    system_prompt = '\\n\\n'.join(system_parts).strip()\n",
    "    user_prompt = '\\n\\n'.join(user_parts).strip()\n",
    "    return system_prompt, user_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6ce12d",
   "metadata": {},
   "source": [
    "## Step 4: Local API Factory (RLM Task App)\n",
    "\n",
    "We implement a Local API that calls `rlm.RLM` for each rollout.\n",
    "The task app reads the prompt template from `request.policy.config`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b775a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T04:58:15.634354Z",
     "iopub.status.busy": "2026-01-04T04:58:15.634302Z",
     "iopub.status.idle": "2026-01-04T04:58:15.639033Z",
     "shell.execute_reply": "2026-01-04T04:58:15.638610Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Local API for OOLONG RLM\n",
    "APP_ID = 'oolong_rlm'\n",
    "APP_NAME = 'OOLONG RLM (Recursive Language Model) QA'\n",
    "\n",
    "def normalize_answer(text: str) -> str:\n",
    "    if text is None:\n",
    "        return ''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    return ''.join(ch.lower() for ch in text.strip() if ch.isalnum() or ch.isspace()).strip()\n",
    "\n",
    "\n",
    "def create_oolong_rlm_local_api(env_api_key: str):\n",
    "    os.environ['ENVIRONMENT_API_KEY'] = env_api_key\n",
    "\n",
    "    async def run_rollout(request: RolloutRequest, fastapi_request) -> RolloutResponse:\n",
    "        policy_config = request.policy.config or {}\n",
    "        env_config = request.env.config or {}\n",
    "        split = env_config.get('split', 'validation')\n",
    "        seed = request.env.seed or 0\n",
    "\n",
    "        sample = oolong.sample(split=split, index=seed)\n",
    "        placeholders = {\n",
    "            'query': sample.query,\n",
    "            'context': sample.context,\n",
    "        }\n",
    "\n",
    "        sections = _get_prompt_sections(policy_config)\n",
    "        rendered = render_prompt_sections(sections, placeholders)\n",
    "        messages_for_validation = []\n",
    "        for section in sections:\n",
    "            role = section.get('role', 'user')\n",
    "            pattern = section.get('content') or section.get('pattern') or ''\n",
    "            messages_for_validation.append({'role': role, 'content': pattern})\n",
    "\n",
    "        system_prompt, root_prompt = split_system_and_user(rendered)\n",
    "        if system_prompt:\n",
    "            custom_system_prompt = system_prompt\n",
    "        else:\n",
    "            custom_system_prompt = RLM_BASE_SYSTEM_PROMPT\n",
    "        inference_url = (\n",
    "            policy_config.get('inference_url')\n",
    "            or policy_config.get('api_base')\n",
    "            or policy_config.get('base_url')\n",
    "        )\n",
    "        if not inference_url:\n",
    "            raise ValueError('Missing inference_url in policy config')\n",
    "\n",
    "        api_key = policy_config.get('api_key') or extract_api_key(fastapi_request, policy_config)\n",
    "        if not api_key:\n",
    "            raise ValueError('Missing API key for inference proxy')\n",
    "\n",
    "        model_name = policy_config.get('model', 'gpt-4o-mini')\n",
    "\n",
    "        rlm = RLM(\n",
    "            backend='openai',\n",
    "            backend_kwargs={\n",
    "                'model_name': model_name,\n",
    "                'api_key': api_key,\n",
    "                'base_url': inference_url,\n",
    "            },\n",
    "            environment='local',\n",
    "            environment_kwargs={},\n",
    "            custom_system_prompt=custom_system_prompt,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        prompt_payload = rendered\n",
    "        completion = rlm.completion(\n",
    "            prompt_payload,\n",
    "        )\n",
    "\n",
    "        predicted = completion.response or ''\n",
    "        gold = sample.answer or ''\n",
    "\n",
    "        reward = 1.0 if normalize_answer(predicted) == normalize_answer(gold) else 0.0\n",
    "\n",
    "        return RolloutResponse(\n",
    "            run_id=request.run_id,\n",
    "            metrics=RolloutMetrics(outcome_reward=reward, details={'messages': messages_for_validation, 'predicted': predicted, 'gold': gold}),\n",
    "            trace=None,\n",
    "            trace_correlation_id=policy_config.get('trace_correlation_id'),\n",
    "        )\n",
    "\n",
    "    def provide_taskset_description():\n",
    "        return {\n",
    "            'splits': ['validation', 'test'],\n",
    "            'sizes': {'validation': oolong.size('validation'), 'test': oolong.size('test')},\n",
    "        }\n",
    "\n",
    "    def provide_task_instances(seeds):\n",
    "        for seed in seeds:\n",
    "            sample = oolong.sample(split='validation', index=seed)\n",
    "            yield TaskInfo(\n",
    "                task={'id': APP_ID, 'name': APP_NAME},\n",
    "                dataset={'id': APP_ID, 'split': sample.split, 'index': sample.index},\n",
    "                inference={'tool': 'rlm_repl'},\n",
    "                limits={'max_turns': 1},\n",
    "                task_metadata={'query': sample.query},\n",
    "            )\n",
    "\n",
    "    return create_local_api(LocalAPIConfig(\n",
    "        app_id=APP_ID,\n",
    "        name=APP_NAME,\n",
    "        description='OOLONG RLM local API for prompt optimization.',\n",
    "        provide_taskset_description=provide_taskset_description,\n",
    "        provide_task_instances=provide_task_instances,\n",
    "        rollout=run_rollout,\n",
    "        cors_origins=['*'],\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b19960",
   "metadata": {},
   "source": [
    "## Step 5: Start the Local API\n",
    "\n",
    "This spins up the task app and exposes it (optionally via a tunnel).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "963323e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T04:58:15.640136Z",
     "iopub.status.busy": "2026-01-04T04:58:15.640064Z",
     "iopub.status.idle": "2026-01-04T04:58:45.472170Z",
     "shell.execute_reply": "2026-01-04T04:58:45.471114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting local API...\n",
      "Waiting for local API on port 8115...\n",
      "Local API ready!\n",
      "Provisioning Cloudflare tunnel...\n",
      "Provisioning managed tunnel for port 8115...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cloudflared for task-8115-2698.usesynth.ai...\n",
      "Waiting for cloudflared to connect to Cloudflare edge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DNS resolution failed (attempt 1): [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying DNS propagation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DNS resolution failed (attempt 2): [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DNS resolution failed (attempt 3): [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DNS resolution failed (attempt 4): [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DNS resolution failed (attempt 5): [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DNS resolution failed (attempt 6): [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DNS resolution failed (attempt 7): [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DNS resolution failed (attempt 8): [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DNS resolution failed (attempt 9): [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunnel ready: https://task-8115-2698.usesynth.ai\n",
      "Local API URL: https://task-8115-2698.usesynth.ai\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Start Local API\n",
    "print('Starting local API...')\n",
    "app = create_oolong_rlm_local_api(ENVIRONMENT_API_KEY)\n",
    "\n",
    "kill_port(LOCAL_API_PORT)\n",
    "run_server_background(app, LOCAL_API_PORT)\n",
    "\n",
    "print(f'Waiting for local API on port {LOCAL_API_PORT}...')\n",
    "await wait_for_health_check('localhost', LOCAL_API_PORT, ENVIRONMENT_API_KEY, timeout=60.0)\n",
    "print('Local API ready!')\n",
    "\n",
    "if USE_TUNNEL:\n",
    "    print('Provisioning Cloudflare tunnel...')\n",
    "    tunnel = await TunneledLocalAPI.create(\n",
    "        local_port=LOCAL_API_PORT,\n",
    "        backend=TunnelBackend.CloudflareManagedTunnel,\n",
    "        api_key=SYNTH_API_KEY,\n",
    "        env_api_key=ENVIRONMENT_API_KEY,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        progress=True,\n",
    "    )\n",
    "    LOCAL_API_URL = tunnel.url\n",
    "else:\n",
    "    LOCAL_API_URL = f'http://localhost:{LOCAL_API_PORT}'\n",
    "\n",
    "print('Local API URL:', LOCAL_API_URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317a1eb5",
   "metadata": {},
   "source": [
    "## Step 6: Run GEPA Prompt Optimization\n",
    "\n",
    "We configure GEPA to optimize the prompt sections passed to the local RLM task app.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "796e1882",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T04:58:45.475584Z",
     "iopub.status.busy": "2026-01-04T04:58:45.475372Z",
     "iopub.status.idle": "2026-01-04T05:00:30.423345Z",
     "shell.execute_reply": "2026-01-04T05:00:30.422283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEPA job submitted: pl_703c0b1a74db4393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00] queued | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:05] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:38] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:43] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:49] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:59] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:05] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:10] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:16] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:21] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:27] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:38] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:43] failed | score: --\n",
      "Final status: failed\n",
      "Best score: None\n"
     ]
    }
   ],
   "source": [
    "# Step 6: GEPA optimization\n",
    "BASELINE_SYSTEM_PROMPT = 'Answer questions using the context.'\n",
    "\n",
    "COMPOSED_SYSTEM_PROMPT = RLM_BASE_SYSTEM_PROMPT + ' ' + BASELINE_SYSTEM_PROMPT\n",
    "\n",
    "SAFEGUARD = 'You have not interacted with the REPL environment or seen your prompt / context yet. Your next action should be to look through and figure out how to answer the prompt, so don\\'t just provide a final answer yet.\\n\\n'\n",
    "BASELINE_USER_PROMPT = SAFEGUARD + USER_PROMPT_WITH_ROOT.format(root_prompt='{query}')\n",
    "\n",
    "config_body = {\n",
    "    'prompt_learning': {\n",
    "        'algorithm': 'gepa',\n",
    "        'task_app_url': LOCAL_API_URL,\n",
    "        'task_app_api_key': ENVIRONMENT_API_KEY,\n",
    "        'env_name': 'oolong',\n",
    "        'initial_prompt': {\n",
    "            'messages': [\n",
    "                {'role': 'system', 'order': 0, 'pattern': COMPOSED_SYSTEM_PROMPT},\n",
    "                {'role': 'user', 'order': 1, 'pattern': BASELINE_USER_PROMPT},\n",
    "            ],\n",
    "            'wildcards': {'query': 'REQUIRED'},\n",
    "        },\n",
    "        'policy': {\n",
    "            'model': 'gpt-4o-mini',\n",
    "            'inference_mode': 'synth_hosted',\n",
    "            'provider': 'openai',\n",
    "            'temperature': 0.0,\n",
    "            'max_completion_tokens': 256,\n",
    "        },\n",
    "        'gepa': {\n",
    "            'env_name': 'oolong',\n",
    "            'evaluation': {\n",
    "                'seeds': list(range(13)),\n",
    "                'validation_seeds': list(range(13, 16)),\n",
    "            },\n",
    "            'rollout': {'budget': 12, 'max_concurrent': 3, 'minibatch_size': 3},\n",
    "            'mutation': {'rate': 0.3},\n",
    "            'population': {'initial_size': 3, 'num_generations': 2, 'children_per_generation': 2},\n",
    "            'archive': {'size': 5, 'pareto_set_size': 10},\n",
    "            'token': {'counting_model': 'gpt-4'},\n",
    "        },\n",
    "        'env_config': {\n",
    "            'split': 'validation',\n",
    "            'max_iterations': 8,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "job = PromptLearningJob.from_dict(\n",
    "    config_dict=config_body,\n",
    "    backend_url=SYNTH_API_BASE,\n",
    "    api_key=SYNTH_API_KEY,\n",
    "    task_app_api_key=ENVIRONMENT_API_KEY,\n",
    "    skip_health_check=True,\n",
    ")\n",
    "\n",
    "job_id = job.submit()\n",
    "print('GEPA job submitted:', job_id)\n",
    "result = job.poll_until_complete(timeout=3600.0, interval=5.0, progress=True)\n",
    "print('Final status:', result.status.value)\n",
    "print('Best score:', result.best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a029d6",
   "metadata": {},
   "source": [
    "## Step 7: Next Steps\n",
    "\n",
    "- Inspect the best prompt and rerun a manual rollout.\n",
    "- Increase rollout budget and population size for stronger results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "037b44ef729f4fd3a3f8a3d96c0baf0c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "079ceddbd7a9451f8489a55debb9bad7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7049308c73da41078d76942d9cdca68e",
        "IPY_MODEL_d21c6ce02f3745c2994afe23a4c0cc9b",
        "IPY_MODEL_4851289903d240149453839599d3d88f"
       ],
       "layout": "IPY_MODEL_2a0fc42b40104c75ae2a07ef3af973b2",
       "tabbable": null,
       "tooltip": null
      }
     },
     "0a36e234f4014e8c932de68a67919d64": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1fa927534b4e49129db811a9cf4832a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "299077ff4d3047898df6b9531f3ea02e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2a0fc42b40104c75ae2a07ef3af973b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2d6b1d35469847aba8591a4c110e3cdb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "30dfe494761840049a77e00e774a1623": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "32df914429094347885ade538ea8898d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3a39daa62df8425eb33bc110ee82283b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d65dc4847a57419dbb6ae6a5c94191f0",
       "placeholder": "​",
       "style": "IPY_MODEL_fb78ebba6bd44e4f91352552c2831189",
       "tabbable": null,
       "tooltip": null,
       "value": " 19/19 [00:02&lt;00:00,  7.33it/s]"
      }
     },
     "4851289903d240149453839599d3d88f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_832fe529629945c08465cd9d828b61fd",
       "placeholder": "​",
       "style": "IPY_MODEL_2d6b1d35469847aba8591a4c110e3cdb",
       "tabbable": null,
       "tooltip": null,
       "value": " 19/19 [00:02&lt;00:00,  6.53it/s]"
      }
     },
     "48e14ba87c3d475e8d6024eae3a9a0f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4d5a6136a5f244319d8a912ef7db7f3e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_32df914429094347885ade538ea8898d",
       "max": 19.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f7ff9a20180e4a9c8836ea99dd0367c8",
       "tabbable": null,
       "tooltip": null,
       "value": 19.0
      }
     },
     "5c2c866339ca4a0381ae27b8aae307e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7049308c73da41078d76942d9cdca68e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_299077ff4d3047898df6b9531f3ea02e",
       "placeholder": "​",
       "style": "IPY_MODEL_5c2c866339ca4a0381ae27b8aae307e0",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading dataset shards: 100%"
      }
     },
     "832fe529629945c08465cd9d828b61fd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c6eac85b8fa840249deb64957bb61dc6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0a36e234f4014e8c932de68a67919d64",
       "placeholder": "​",
       "style": "IPY_MODEL_1fa927534b4e49129db811a9cf4832a1",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading dataset shards: 100%"
      }
     },
     "d21c6ce02f3745c2994afe23a4c0cc9b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_30dfe494761840049a77e00e774a1623",
       "max": 19.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_48e14ba87c3d475e8d6024eae3a9a0f5",
       "tabbable": null,
       "tooltip": null,
       "value": 19.0
      }
     },
     "d65dc4847a57419dbb6ae6a5c94191f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ea269067a33146e9905fb484d050b63f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c6eac85b8fa840249deb64957bb61dc6",
        "IPY_MODEL_4d5a6136a5f244319d8a912ef7db7f3e",
        "IPY_MODEL_3a39daa62df8425eb33bc110ee82283b"
       ],
       "layout": "IPY_MODEL_037b44ef729f4fd3a3f8a3d96c0baf0c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f7ff9a20180e4a9c8836ea99dd0367c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fb78ebba6bd44e4f91352552c2831189": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
