{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd06df3",
   "metadata": {},
   "source": [
    "# Synth GEPA Demo - OOLONG RLM (MIT)\n",
    "\n",
    "This notebook demonstrates running GEPA prompt optimization through a **Synth Local API** task app that uses an **RLM (Recursive Language Model)** from the `rlm` library on the OOLONG dataset.\n",
    "\n",
    "We build a local task app that calls `rlm.RLM` for each rollout, expose it via a tunnel (or local URL), and then run a GEPA job with Synth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b4d318f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T08:42:48.993142Z",
     "iopub.status.busy": "2026-01-04T08:42:48.993021Z",
     "iopub.status.idle": "2026-01-04T08:42:48.997975Z",
     "shell.execute_reply": "2026-01-04T08:42:48.997395Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 0: Install dependencies (run this first on Colab)\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    %pip install -q git+https://github.com/alexzhang13/rlm.git datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c738f357",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Configure imports and API keys in one place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96232e43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T08:42:48.999746Z",
     "iopub.status.busy": "2026-01-04T08:42:48.999635Z",
     "iopub.status.idle": "2026-01-04T08:42:51.174796Z",
     "shell.execute_reply": "2026-01-04T08:42:51.174397Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshpurtell/Documents/GitHub/synth-ai/synth_ai/sdk/task/contracts.py:34: UserWarning: Field name \"schema\" in \"StructuredOutputConfig\" shadows an attribute in parent \"BaseModel\"\n",
      "  class StructuredOutputConfig(BaseModel):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[celery_app] EXPERIMENT_QUEUE_DB_PATH not set, will use default path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[celery_app] Using default database path: /Users/joshpurtell/.synth_ai/experiment_queue.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[celery_app] Initializing with database: /Users/joshpurtell/.synth_ai/experiment_queue.db (broker: redis://localhost:6379/0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setup - imports, config, and API keys\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Iterable, List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "\n",
    "from synth_ai.sdk.api.train.prompt_learning import PromptLearningJob\n",
    "from synth_ai.sdk.localapi import LocalAPIConfig, create_local_api\n",
    "from synth_ai.sdk.localapi.auth import ensure_localapi_auth\n",
    "from synth_ai.sdk.localapi.helpers import extract_api_key\n",
    "from synth_ai.sdk.task import run_server_background\n",
    "from synth_ai.sdk.task.contracts import RolloutMetrics, RolloutRequest, RolloutResponse, TaskInfo\n",
    "from synth_ai.sdk.tunnels import TunneledLocalAPI, TunnelBackend, kill_port, wait_for_health_check\n",
    "\n",
    "from rlm import RLM\n",
    "from rlm.utils.prompts import RLM_SYSTEM_PROMPT, USER_PROMPT, USER_PROMPT_WITH_ROOT\n",
    "\n",
    "load_dotenv()\n",
    "# Work around rlm QueryMetadata typing bug under Python 3.11\n",
    "from rlm.core import rlm as rlm_core\n",
    "from rlm.core import types as rlm_types\n",
    "\n",
    "class PatchedQueryMetadata:\n",
    "    def __init__(self, prompt):\n",
    "        if isinstance(prompt, str):\n",
    "            self.context_lengths = [len(prompt)]\n",
    "            self.context_type = 'str'\n",
    "        elif isinstance(prompt, dict):\n",
    "            self.context_lengths = [len(chunk) for chunk in prompt.values()]\n",
    "            self.context_type = 'dict'\n",
    "        elif isinstance(prompt, list):\n",
    "            self.context_type = 'list'\n",
    "            if prompt and isinstance(prompt[0], dict):\n",
    "                if 'content' in prompt[0]:\n",
    "                    self.context_lengths = [len(chunk['content']) for chunk in prompt]\n",
    "                else:\n",
    "                    self.context_lengths = [len(chunk) for chunk in prompt]\n",
    "            else:\n",
    "                self.context_lengths = [len(chunk) for chunk in prompt]\n",
    "        else:\n",
    "            raise ValueError(f'Invalid prompt type: {type(prompt)}')\n",
    "\n",
    "        self.context_total_length = sum(self.context_lengths)\n",
    "\n",
    "rlm_types.QueryMetadata = PatchedQueryMetadata\n",
    "rlm_core.QueryMetadata = PatchedQueryMetadata\n",
    "\n",
    "def patched_build_rlm_system_prompt(system_prompt, query_metadata=None, **_kwargs):\n",
    "    return [\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'assistant', 'content': '{context_metadata}'},\n",
    "    ]\n",
    "\n",
    "from rlm.utils import prompts as rlm_prompts\n",
    "rlm_prompts.build_rlm_system_prompt = patched_build_rlm_system_prompt\n",
    "rlm_core.build_rlm_system_prompt = patched_build_rlm_system_prompt\n",
    "\n",
    "\n",
    "SYNTH_API_BASE = os.getenv('SYNTH_API_BASE', 'https://api.usesynth.ai')\n",
    "SYNTH_API_KEY = os.getenv('SYNTH_API_KEY', '')\n",
    "ENVIRONMENT_API_KEY = ensure_localapi_auth(\n",
    "    backend_base=SYNTH_API_BASE,\n",
    "    synth_api_key=SYNTH_API_KEY,\n",
    ")\n",
    "\n",
    "LOCAL_API_PORT = int(os.getenv('LOCAL_API_PORT', '8115'))\n",
    "USE_TUNNEL = os.getenv('USE_TUNNEL', 'true').lower() in {\n",
    "    '1', 'true', 'yes', 'y'\n",
    "}\n",
    "\n",
    "if not SYNTH_API_KEY:\n",
    "    raise ValueError('Missing SYNTH_API_KEY env var')\n",
    "\n",
    "print('Config loaded')\n",
    "\n",
    "RLM_BASE_SYSTEM_PROMPT = (\n",
    "    'You are a recursive language model. Use the REPL with the context variable to reason. '\n",
    "    'Call llm_query or llm_query_batched as needed. When finished, answer with FINAL.'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a8b1b",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Loader (OOLONG)\n",
    "\n",
    "We lazily load `oolongbench/oolong-real` (config: `dnd`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "778a5329",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T08:42:51.175916Z",
     "iopub.status.busy": "2026-01-04T08:42:51.175778Z",
     "iopub.status.idle": "2026-01-04T08:43:01.648123Z",
     "shell.execute_reply": "2026-01-04T08:43:01.646279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2896a7138443ec822e73b19a018952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913e4d2063cf4945ab620f523e6bdf18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready: 5995 6072\n"
     ]
    }
   ],
   "source": [
    "# Step 2: OOLONG dataset wrapper\n",
    "@dataclass\n",
    "class OolongSample:\n",
    "    index: int\n",
    "    split: str\n",
    "    query: str\n",
    "    context: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "class OolongDataset:\n",
    "    def __init__(self, hf_dataset: str = 'oolongbench/oolong-real', hf_config: str = 'dnd'):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.hf_config = hf_config\n",
    "        self._cache = {}\n",
    "\n",
    "    def _load_split(self, split: str):\n",
    "        if split not in self._cache:\n",
    "            ds = load_dataset(self.hf_dataset, self.hf_config, split=split)\n",
    "            self._cache[split] = ds\n",
    "        return self._cache[split]\n",
    "\n",
    "    def ensure_ready(self, splits: Iterable[str]) -> None:\n",
    "        for split in splits:\n",
    "            self._load_split(split)\n",
    "\n",
    "    def size(self, split: str) -> int:\n",
    "        return len(self._load_split(split))\n",
    "\n",
    "    def sample(self, split: str, index: int) -> OolongSample:\n",
    "        ds = self._load_split(split)\n",
    "        idx = index % len(ds)\n",
    "        row = ds[idx]\n",
    "        query = row.get('query') or row.get('question') or ''\n",
    "        context = row.get('context_window_text') or row.get('context') or row.get('text') or ''\n",
    "        answer = row.get('answer') or ''\n",
    "        return OolongSample(\n",
    "            index=idx,\n",
    "            split=split,\n",
    "            query=str(query),\n",
    "            context=str(context),\n",
    "            answer=str(answer),\n",
    "        )\n",
    "\n",
    "\n",
    "oolong = OolongDataset()\n",
    "oolong.ensure_ready(['validation', 'test'])\n",
    "print('Dataset ready:', oolong.size('validation'), oolong.size('test'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aecec3f",
   "metadata": {},
   "source": [
    "## Step 3: Prompt Template Rendering\n",
    "\n",
    "GEPA sends candidate prompts to the task app via `request.policy.config.prompt_template`.\n",
    "We render those sections into messages, and use them to drive the RLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74eab862",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T08:43:01.654853Z",
     "iopub.status.busy": "2026-01-04T08:43:01.654064Z",
     "iopub.status.idle": "2026-01-04T08:43:01.664722Z",
     "shell.execute_reply": "2026-01-04T08:43:01.664225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Prompt template helpers\n",
    "def _normalize_prompt_template(policy_config: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    template = policy_config.get('prompt_template') or {}\n",
    "    if not isinstance(template, dict):\n",
    "        template = {}\n",
    "    return template\n",
    "\n",
    "\n",
    "def _get_prompt_sections(policy_config: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    template = _normalize_prompt_template(policy_config)\n",
    "    sections = (\n",
    "        template.get('sections')\n",
    "        or template.get('prompt_sections')\n",
    "        or policy_config.get('prompt_sections')\n",
    "        or []\n",
    "    )\n",
    "    if not isinstance(sections, list):\n",
    "        return []\n",
    "    return sorted(sections, key=lambda s: s.get('order', 0))\n",
    "\n",
    "\n",
    "def render_prompt_sections(sections: List[Dict[str, Any]], placeholders: Dict[str, str]) -> List[Dict[str, str]]:\n",
    "    rendered: List[Dict[str, str]] = []\n",
    "    for section in sections:\n",
    "        role = section.get('role', 'user')\n",
    "        pattern = section.get('content') or section.get('pattern') or ''\n",
    "        content = pattern.format(**placeholders)\n",
    "        rendered.append({'role': role, 'content': content})\n",
    "    return rendered\n",
    "\n",
    "\n",
    "def split_system_and_user(messages: List[Dict[str, str]]) -> tuple[str, str]:\n",
    "    system_parts = [m['content'] for m in messages if m.get('role') == 'system']\n",
    "    user_parts = [m['content'] for m in messages if m.get('role') != 'system']\n",
    "    system_prompt = '\\n\\n'.join(system_parts).strip()\n",
    "    user_prompt = '\\n\\n'.join(user_parts).strip()\n",
    "    return system_prompt, user_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6ce12d",
   "metadata": {},
   "source": [
    "## Step 4: Local API Factory (RLM Task App)\n",
    "\n",
    "We implement a Local API that calls `rlm.RLM` for each rollout.\n",
    "The task app reads the prompt template from `request.policy.config`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b775a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T08:43:01.666727Z",
     "iopub.status.busy": "2026-01-04T08:43:01.666571Z",
     "iopub.status.idle": "2026-01-04T08:43:01.675474Z",
     "shell.execute_reply": "2026-01-04T08:43:01.675053Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Local API for OOLONG RLM\n",
    "APP_ID = 'oolong_rlm'\n",
    "APP_NAME = 'OOLONG RLM (Recursive Language Model) QA'\n",
    "\n",
    "def normalize_answer(text: str) -> str:\n",
    "    if text is None:\n",
    "        return ''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    return ''.join(ch.lower() for ch in text.strip() if ch.isalnum() or ch.isspace()).strip()\n",
    "\n",
    "\n",
    "def create_oolong_rlm_local_api():\n",
    "\n",
    "    async def run_rollout(request: RolloutRequest, fastapi_request) -> RolloutResponse:\n",
    "        policy_config = request.policy.config or {}\n",
    "        env_config = request.env.config or {}\n",
    "        split = env_config.get('split', 'validation')\n",
    "        seed = request.env.seed or 0\n",
    "\n",
    "        sample = oolong.sample(split=split, index=seed)\n",
    "        placeholders = {\n",
    "            'query': sample.query,\n",
    "            'context': sample.context,\n",
    "            'context_metadata': '{context_metadata}',\n",
    "        }\n",
    "\n",
    "        sections = _get_prompt_sections(policy_config)\n",
    "        if not sections:\n",
    "            sections = [\n",
    "                {'role': 'system', 'content': COMPOSED_SYSTEM_PROMPT, 'order': 0},\n",
    "                {'role': 'assistant', 'content': RLM_CONTEXT_METADATA_PATTERN, 'order': 1},\n",
    "                {'role': 'user', 'content': RLM_FIRST_USER_PROMPT, 'order': 2},\n",
    "                {'role': 'user', 'content': BASELINE_USER_PROMPT, 'order': 3},\n",
    "            ]\n",
    "        rendered = render_prompt_sections(sections, placeholders)\n",
    "        messages_for_validation = []\n",
    "        for section in sections:\n",
    "            role = section.get('role', 'user')\n",
    "            pattern = section.get('content') or section.get('pattern') or ''\n",
    "            messages_for_validation.append({'role': role, 'content': pattern})\n",
    "\n",
    "        system_prompt, root_prompt = split_system_and_user(rendered)\n",
    "        if system_prompt:\n",
    "            custom_system_prompt = system_prompt\n",
    "        else:\n",
    "            custom_system_prompt = RLM_BASE_SYSTEM_PROMPT\n",
    "        inference_url = (\n",
    "            policy_config.get('inference_url')\n",
    "            or policy_config.get('api_base')\n",
    "            or policy_config.get('base_url')\n",
    "        )\n",
    "        if not inference_url:\n",
    "            raise ValueError('Missing inference_url in policy config')\n",
    "\n",
    "        api_key = policy_config.get('api_key') or SYNTH_API_KEY\n",
    "        if not api_key:\n",
    "            raise ValueError('Missing policy api_key for inference proxy')\n",
    "\n",
    "        model_name = policy_config.get('model', 'gpt-4o-mini')\n",
    "        max_iterations = int(env_config.get('max_iterations', 2))\n",
    "        max_depth = int(env_config.get('max_depth', 0))\n",
    "\n",
    "        rlm = RLM(\n",
    "            backend='openai',\n",
    "            backend_kwargs={\n",
    "                'model_name': model_name,\n",
    "                'api_key': api_key,\n",
    "                'base_url': inference_url,\n",
    "            },\n",
    "            environment='local',\n",
    "            environment_kwargs={},\n",
    "            custom_system_prompt=custom_system_prompt,\n",
    "            max_iterations=max_iterations,\n",
    "            max_depth=max_depth,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        prompt_payload = rendered\n",
    "        completion = rlm.completion(\n",
    "            prompt_payload,\n",
    "        )\n",
    "\n",
    "        if isinstance(completion, str):\n",
    "            predicted = completion\n",
    "        else:\n",
    "            predicted = completion.response or ''\n",
    "        gold = sample.answer or ''\n",
    "\n",
    "        reward = 1.0 if normalize_answer(predicted) == normalize_answer(gold) else 0.0\n",
    "\n",
    "        return RolloutResponse(\n",
    "            run_id=request.run_id,\n",
    "            metrics=RolloutMetrics(outcome_reward=reward, details={'messages': messages_for_validation, 'predicted': predicted, 'gold': gold}),\n",
    "            trace=None,\n",
    "            trace_correlation_id=policy_config.get('trace_correlation_id'),\n",
    "        )\n",
    "\n",
    "    def provide_taskset_description():\n",
    "        return {\n",
    "            'splits': ['validation', 'test'],\n",
    "            'sizes': {'validation': oolong.size('validation'), 'test': oolong.size('test')},\n",
    "        }\n",
    "\n",
    "    def provide_task_instances(seeds):\n",
    "        for seed in seeds:\n",
    "            sample = oolong.sample(split='validation', index=seed)\n",
    "            yield TaskInfo(\n",
    "                task={'id': APP_ID, 'name': APP_NAME},\n",
    "                dataset={'id': APP_ID, 'split': sample.split, 'index': sample.index},\n",
    "                inference={'tool': 'rlm_repl'},\n",
    "                limits={'max_turns': 1},\n",
    "                task_metadata={'query': sample.query},\n",
    "            )\n",
    "\n",
    "    return create_local_api(LocalAPIConfig(\n",
    "        app_id=APP_ID,\n",
    "        name=APP_NAME,\n",
    "        description='OOLONG RLM local API for prompt optimization.',\n",
    "        provide_taskset_description=provide_taskset_description,\n",
    "        provide_task_instances=provide_task_instances,\n",
    "        rollout=run_rollout,\n",
    "        cors_origins=['*'],\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b19960",
   "metadata": {},
   "source": [
    "## Step 5: Start the Local API\n",
    "\n",
    "This spins up the task app and exposes it (optionally via a tunnel).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "963323e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T08:43:01.677151Z",
     "iopub.status.busy": "2026-01-04T08:43:01.676758Z",
     "iopub.status.idle": "2026-01-04T08:43:30.936149Z",
     "shell.execute_reply": "2026-01-04T08:43:30.935451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting local API...\n",
      "Waiting for local API on port 8115...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local API ready!\n",
      "Provisioning Cloudflare tunnel...\n",
      "Provisioning managed tunnel for port 8115...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cloudflared for task-8115-16185.usesynth.ai...\n",
      "Waiting for cloudflared to connect to Cloudflare edge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DNS resolution failed (attempt 1): [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying DNS propagation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DNS resolution failed (attempt 2): [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DNS resolution failed (attempt 3): [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DNS resolution failed (attempt 4): [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DNS resolution failed (attempt 5): [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DNS resolution failed (attempt 6): [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DNS resolution failed (attempt 7): [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DNS resolution failed (attempt 8): [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunnel ready: https://task-8115-16185.usesynth.ai\n",
      "Local API URL: https://task-8115-16185.usesynth.ai\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Start Local API\n",
    "print('Starting local API...')\n",
    "app = create_oolong_rlm_local_api()\n",
    "\n",
    "kill_port(LOCAL_API_PORT)\n",
    "run_server_background(app, LOCAL_API_PORT)\n",
    "\n",
    "print(f'Waiting for local API on port {LOCAL_API_PORT}...')\n",
    "await wait_for_health_check('localhost', LOCAL_API_PORT, ENVIRONMENT_API_KEY, timeout=60.0)\n",
    "print('Local API ready!')\n",
    "\n",
    "if USE_TUNNEL:\n",
    "    print('Provisioning Cloudflare tunnel...')\n",
    "    tunnel = await TunneledLocalAPI.create(\n",
    "        local_port=LOCAL_API_PORT,\n",
    "        backend=TunnelBackend.CloudflareManagedTunnel,\n",
    "        api_key=SYNTH_API_KEY,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        progress=True,\n",
    "    )\n",
    "    LOCAL_API_URL = tunnel.url\n",
    "else:\n",
    "    LOCAL_API_URL = f'http://localhost:{LOCAL_API_PORT}'\n",
    "\n",
    "print('Local API URL:', LOCAL_API_URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317a1eb5",
   "metadata": {},
   "source": [
    "## Step 6: Run GEPA Prompt Optimization\n",
    "\n",
    "We configure GEPA to optimize the prompt sections passed to the local RLM task app.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "796e1882",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T08:43:30.938559Z",
     "iopub.status.busy": "2026-01-04T08:43:30.938420Z",
     "iopub.status.idle": "2026-01-04T08:47:25.144447Z",
     "shell.execute_reply": "2026-01-04T08:47:25.143766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEPA job submitted: pl_84d2beac44014a14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00] queued | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:05] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:10] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:16] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:21] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:27] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:32] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:38] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:43] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:48] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:54] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:59] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:05] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:10] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:15] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:21] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:26] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:37] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:42] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:48] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:53] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:59] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:04] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:10] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:15] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:21] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:26] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:31] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:37] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:42] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:48] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:53] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:58] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:09] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:20] running | score: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:31] succeeded | score: --\n",
      "Final status: succeeded\n",
      "Best score: None\n"
     ]
    }
   ],
   "source": [
    "# Step 6: GEPA optimization\n",
    "BASELINE_SYSTEM_PROMPT = 'Answer questions using the context.'\n",
    "BASELINE_USER_PROMPT = 'Query: {query}\\n\\nContext:\\n{context}\\n\\nAnswer the query using the context.'\n",
    "\n",
    "RLM_CONTEXT_METADATA_PATTERN = '{context_metadata}'\n",
    "RLM_FIRST_USER_PROMPT = (\n",
    "    'You have not interacted with the REPL environment or seen your prompt / context yet. '\n",
    "    'Your next action should be to look through and figure out how to answer the prompt, '\n",
    "    \"so don't just provide a final answer yet.\\n\\n\" + USER_PROMPT\n",
    ")\n",
    "\n",
    "COMPOSED_SYSTEM_PROMPT = RLM_BASE_SYSTEM_PROMPT + ' ' + BASELINE_SYSTEM_PROMPT\n",
    "\n",
    "config_body = {\n",
    "    'prompt_learning': {\n",
    "        'algorithm': 'gepa',\n",
    "        'task_app_url': LOCAL_API_URL,\n",
    "        'env_name': 'oolong',\n",
    "        'initial_prompt': {\n",
    "            'messages': [\n",
    "                {'role': 'system', 'order': 0, 'pattern': COMPOSED_SYSTEM_PROMPT},\n",
    "                {'role': 'assistant', 'order': 1, 'pattern': RLM_CONTEXT_METADATA_PATTERN},\n",
    "                {'role': 'user', 'order': 2, 'pattern': RLM_FIRST_USER_PROMPT},\n",
    "                {'role': 'user', 'order': 3, 'pattern': BASELINE_USER_PROMPT},\n",
    "            ],\n",
    "            'wildcards': {'query': 'REQUIRED', 'context': 'REQUIRED', 'context_metadata': 'REQUIRED'},\n",
    "        },\n",
    "        'policy': {\n",
    "            'model': 'gpt-4o-mini',\n",
    "            'inference_mode': 'synth_hosted',\n",
    "            'provider': 'openai',\n",
    "            'temperature': 0.0,\n",
    "            'max_completion_tokens': 256,\n",
    "        },\n",
    "        'gepa': {\n",
    "            'env_name': 'oolong',\n",
    "            'evaluation': {\n",
    "                'seeds': list(range(13)),\n",
    "                'validation_seeds': list(range(13, 15)),\n",
    "            },\n",
    "            'rollout': {'budget': 6, 'max_concurrent': 3, 'minibatch_size': 3},\n",
    "            'mutation': {'rate': 0.3},\n",
    "            'population': {'initial_size': 2, 'num_generations': 1, 'children_per_generation': 1},\n",
    "            'archive': {'size': 10, 'pareto_set_size': 10},\n",
    "            'token': {'counting_model': 'gpt-4'},\n",
    "        },\n",
    "        'env_config': {\n",
    "            'split': 'validation',\n",
    "            'max_iterations': 2,\n",
    "            'max_depth': 0,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "job = PromptLearningJob.from_dict(\n",
    "    config_dict=config_body,\n",
    "    backend_url=SYNTH_API_BASE,\n",
    "    api_key=SYNTH_API_KEY,\n",
    "    skip_health_check=True,\n",
    ")\n",
    "\n",
    "job_id = job.submit()\n",
    "print('GEPA job submitted:', job_id)\n",
    "result = job.poll_until_complete(timeout=3600.0, interval=5.0, progress=True)\n",
    "print('Final status:', result.status.value)\n",
    "print('Best score:', result.best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a029d6",
   "metadata": {},
   "source": [
    "## Step 7: Next Steps\n",
    "\n",
    "- Inspect the best prompt and rerun a manual rollout.\n",
    "- Increase rollout budget and population size for stronger results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f47f56ee65145879fca28905e2b2054": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_40924abcddbd49f1affdcf2214938fbb",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_539ec453275340b19b98c9aaadc6cf90",
       "tabbable": null,
       "tooltip": null,
       "value": "\u200719/19\u2007[00:02&lt;00:00,\u2007\u20077.06it/s]"
      }
     },
     "1915de143ebf45afa9b142c52080cb03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2689b13ea99140eebd849bc573d20bc2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2f2896a7138443ec822e73b19a018952": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_602dc536c1814df6b9b7d06d559253c3",
        "IPY_MODEL_aeaeaf33fea54f199fa5f005a3ef91b0",
        "IPY_MODEL_7393bf454ead41ceba6cdad3c7f4c55b"
       ],
       "layout": "IPY_MODEL_7ad2dda1eb274765921476536db21a79",
       "tabbable": null,
       "tooltip": null
      }
     },
     "40924abcddbd49f1affdcf2214938fbb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "48359b1e712147dbadeb274758eccc21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "48f272ed276a4e298eee068bc69d6094": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2689b13ea99140eebd849bc573d20bc2",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_b21ad78afdf44499b50af25bd2548ab3",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading\u2007dataset\u2007shards:\u2007100%"
      }
     },
     "539ec453275340b19b98c9aaadc6cf90": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "602dc536c1814df6b9b7d06d559253c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_df6f8cb04e3e4fa29db959ad4f2ab8e9",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_ab3ad553e9dc4f7baf54258bf9e04b21",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading\u2007dataset\u2007shards:\u2007100%"
      }
     },
     "7393bf454ead41ceba6cdad3c7f4c55b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_771169fd8eda4c6aaecf378f17310be2",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_1915de143ebf45afa9b142c52080cb03",
       "tabbable": null,
       "tooltip": null,
       "value": "\u200719/19\u2007[00:03&lt;00:00,\u2007\u20075.58it/s]"
      }
     },
     "771169fd8eda4c6aaecf378f17310be2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7ad2dda1eb274765921476536db21a79": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7cae8e94e0ff48afa87a4f2db05ae309": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "913e4d2063cf4945ab620f523e6bdf18": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_48f272ed276a4e298eee068bc69d6094",
        "IPY_MODEL_e37bab2b86074e0b99170ef9a61713e1",
        "IPY_MODEL_0f47f56ee65145879fca28905e2b2054"
       ],
       "layout": "IPY_MODEL_e8f1c478f3a44a18939b55216970178b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ab3ad553e9dc4f7baf54258bf9e04b21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "aeaeaf33fea54f199fa5f005a3ef91b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_eeee8cb87ba74606aefb0e8c06af40d6",
       "max": 19.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_48359b1e712147dbadeb274758eccc21",
       "tabbable": null,
       "tooltip": null,
       "value": 19.0
      }
     },
     "af3a1b51646147148aff6a0a523d04c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b21ad78afdf44499b50af25bd2548ab3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "df6f8cb04e3e4fa29db959ad4f2ab8e9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e37bab2b86074e0b99170ef9a61713e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_af3a1b51646147148aff6a0a523d04c0",
       "max": 19.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7cae8e94e0ff48afa87a4f2db05ae309",
       "tabbable": null,
       "tooltip": null,
       "value": 19.0
      }
     },
     "e8f1c478f3a44a18939b55216970178b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eeee8cb87ba74606aefb0e8c06af40d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
