{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Synth GEPA Demo - Banking77 (Production)\n",
    "\n",
    "Self-contained notebook for prompt optimization using GEPA.\n",
    "\n",
    "Banking77 is a task where an AI needs to label a customer service request with one of 77 fixed possible intents.\n",
    "\n",
    "**Run in Google Colab:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/synth-laboratories/synth-ai/blob/main/demos/gepa_banking77/demo_prod.ipynb)\n",
    "\n",
    "**What this demo does:**\n",
    "1. Spins up a local api that runs the banking77 classification pipeline\n",
    "2. Creates a Cloudflare tunnel to expose it to the internet\n",
    "3. Runs GEPA prompt optimization via Synth\n",
    "4. Compares baseline vs optimized prompts on held-out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ku8p0wn7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Install dependencies (run this first on Colab)\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab - installing dependencies...\")\n",
    "    !pip install -q synth-ai httpx fastapi uvicorn datasets nest_asyncio\n",
    "    \n",
    "    # Install cloudflared\n",
    "    !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O /usr/local/bin/cloudflared\n",
    "    !chmod +x /usr/local/bin/cloudflared\n",
    "    !cloudflared --version\n",
    "    \n",
    "    print(\"Dependencies installed!\")\n",
    "else:\n",
    "    print(\"Not in Colab - assuming dependencies are already installed\")\n",
    "    print(\"Required: pip install synth-ai httpx fastapi uvicorn datasets nest_asyncio\")\n",
    "    print(\"Required: brew install cloudflare/cloudflare/cloudflared (macOS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0n3ycc08f4tq",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "To complete prompt optimization, we'll need to spin up services that expose our business logic to the optimizer via http. Because we'll evaluate the optimized prompt against the baseline prompt concurrently at the end, we'll select two unique ports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Imports, Config, and Backend Health Check\n",
    "import os, sys, time, asyncio, json, threading\n",
    "from typing import Any, Optional\n",
    "\n",
    "import httpx\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, Request, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Production backend\n",
    "SYNTH_API_BASE = 'https://api.usesynth.ai'\n",
    "TASK_APP_PORT = 8001\n",
    "OPTIMIZED_TASK_APP_PORT = 8002\n",
    "\n",
    "print(f'Backend: {SYNTH_API_BASE}')\n",
    "print(f'Task App Ports: {TASK_APP_PORT}, {OPTIMIZED_TASK_APP_PORT}')\n",
    "\n",
    "# Check backend health\n",
    "r = httpx.get(f'{SYNTH_API_BASE}/health', timeout=30)\n",
    "if r.status_code == 200:\n",
    "    print(f'Backend health: {r.json()}')\n",
    "else:\n",
    "    print(f'WARNING: Backend returned status {r.status_code}')\n",
    "    print(f'Response: {r.text[:200]}...' if len(r.text) > 200 else f'Response: {r.text}')\n",
    "    raise RuntimeError(f'Backend not healthy: status {r.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v9dkmn241l",
   "metadata": {},
   "source": [
    "## Step 2: Authentication\n",
    "\n",
    "To kick off any job via Synth, we need an API key. Typically, you'll use your own. But here, for simplicity's sake, we'll mint a demo API key which is scoped to these demos and should not be used in other instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Get API Key (use env var or mint demo key)\n",
    "API_KEY = os.environ.get('SYNTH_API_KEY', '')\n",
    "\n",
    "if not API_KEY:\n",
    "    print('No SYNTH_API_KEY found, minting demo key...')\n",
    "    resp = httpx.post(f'{SYNTH_API_BASE}/api/demo/keys', json={'ttl_hours': 4}, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    API_KEY = resp.json()['api_key']\n",
    "    print(f'Demo API Key: {API_KEY[:25]}...')\n",
    "else:\n",
    "    print(f'Using SYNTH_API_KEY: {API_KEY[:20]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7jqt92io4",
   "metadata": {},
   "source": [
    "## Step 3: Environment Key\n",
    "\n",
    "Because your business logic's integrity is important, you should require authentication to the local api you serve during optimization. The SDK has functions to help you mint and register your key, which we'll then use to authenticate requests from the Synth optimization job to our local API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Mint and Upload Environment Key (using SDK helpers)\n",
    "from synth_ai.sdk.learning.rl import mint_environment_api_key, setup_environment_api_key\n",
    "\n",
    "ENVIRONMENT_API_KEY = mint_environment_api_key()\n",
    "print(f'Minted env key: {ENVIRONMENT_API_KEY[:12]}...{ENVIRONMENT_API_KEY[-4:]}')\n",
    "\n",
    "result = setup_environment_api_key(SYNTH_API_BASE, API_KEY, token=ENVIRONMENT_API_KEY)\n",
    "print(f'Uploaded env key: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5hgu087g",
   "metadata": {},
   "source": [
    "## Step 4: Local API Definition\n",
    "\n",
    "Here, we're going to define a FastAPI service that will serve our Banking77 classification pipeline for the optimizer to use.\n",
    "\n",
    "The service needs these routes:\n",
    "\n",
    "| Route | Method | Input | Output | Description |\n",
    "|-------|--------|-------|--------|-------------|\n",
    "| `/` | GET | - | `str` | Root endpoint, returns welcome message |\n",
    "| `/health` | GET | - | `{\"status\": \"ok\"}` | Health check for liveness probes |\n",
    "| `/info` | GET | - | `TaskInfo` | Static metadata about the task app |\n",
    "| `/task_info` | GET | `?seeds=0,1,2` | `list[TaskInfo]` | Task instances for specific seeds |\n",
    "| `/rollout` | POST | `RolloutRequest` | `RolloutResponse` | Execute a rollout and return metrics + trace |\n",
    "\n",
    "**Key Classes:**\n",
    "- `RolloutRequest`: Contains `run_id`, `env` (seed, config), `policy` (inference_url, model settings)\n",
    "- `RolloutResponse`: Contains `metrics` (scores), `trace` (messages, response), `trace_correlation_id`\n",
    "- `TaskInfo`: Static metadata (task descriptor, dataset info, rubric, inference defaults, limits)\n",
    "\n",
    "The SDK's `LocalAPIConfig` + `create_local_api()` handles all the routing boilerplateâ€”you just provide callbacks for `rollout`, `describe_taskset`, and `provide_task_instances`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define Banking77 Local API using SDK Abstractions\n",
    "import json\n",
    "import os\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "# Synth-AI SDK LocalAPI imports (preferred over task.* namespace)\n",
    "from synth_ai.sdk.localapi import (\n",
    "    LocalAPIConfig,\n",
    "    create_local_api,\n",
    "    RolloutResponseBuilder,\n",
    ")\n",
    "from synth_ai.sdk.localapi.helpers import (\n",
    "    call_chat_completion_api,\n",
    "    create_http_client_hooks,\n",
    "    extract_api_key,\n",
    "    normalize_chat_completion_url,\n",
    ")\n",
    "from synth_ai.sdk.task.contracts import (\n",
    "    RolloutMetrics,\n",
    "    RolloutRequest,\n",
    "    RolloutResponse,\n",
    "    TaskInfo,\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Business Logic (inline for Colab compatibility)\n",
    "# ============================================================================\n",
    "\n",
    "BANKING77_LABELS = [\n",
    "    \"activate_my_card\", \"age_limit\", \"apple_pay_or_google_pay\", \"atm_support\", \"automatic_top_up\",\n",
    "    \"balance_not_updated_after_bank_transfer\", \"balance_not_updated_after_cheque_or_cash_deposit\",\n",
    "    \"beneficiary_not_allowed\", \"cancel_transfer\", \"card_about_to_expire\", \"card_acceptance\",\n",
    "    \"card_arrival\", \"card_delivery_estimate\", \"card_linking\", \"card_not_working\",\n",
    "    \"card_payment_fee_charged\", \"card_payment_not_recognised\", \"card_payment_wrong_exchange_rate\",\n",
    "    \"card_swallowed\", \"cash_withdrawal_charge\", \"cash_withdrawal_not_recognised\", \"change_pin\",\n",
    "    \"compromised_card\", \"contactless_not_working\", \"country_support\", \"declined_card_payment\",\n",
    "    \"declined_cash_withdrawal\", \"declined_transfer\", \"direct_debit_payment_not_recognised\",\n",
    "    \"disposable_card_limits\", \"edit_personal_details\", \"exchange_charge\", \"exchange_rate\",\n",
    "    \"exchange_via_app\", \"extra_charge_on_statement\", \"failed_transfer\", \"fiat_currency_support\",\n",
    "    \"get_disposable_virtual_card\", \"get_physical_card\", \"getting_spare_card\", \"getting_virtual_card\",\n",
    "    \"lost_or_stolen_card\", \"lost_or_stolen_phone\", \"order_physical_card\", \"passcode_forgotten\",\n",
    "    \"pending_card_payment\", \"pending_cash_withdrawal\", \"pending_top_up\", \"pending_transfer\",\n",
    "    \"pin_blocked\", \"receiving_money\", \"Refund_not_showing_up\", \"request_refund\",\n",
    "    \"reverted_card_payment?\", \"supported_cards_and_currencies\", \"terminate_account\",\n",
    "    \"top_up_by_bank_transfer_charge\", \"top_up_by_card_charge\", \"top_up_by_cash_or_cheque\",\n",
    "    \"top_up_failed\", \"top_up_limits\", \"top_up_reverted\", \"topping_up_by_card\",\n",
    "    \"transaction_charged_twice\", \"transfer_fee_charged\", \"transfer_into_account\",\n",
    "    \"transfer_not_received_by_recipient\", \"transfer_timing\", \"unable_to_verify_identity\",\n",
    "    \"verify_my_identity\", \"verify_source_of_funds\", \"verify_top_up\", \"virtual_card_not_working\",\n",
    "    \"visa_or_mastercard\", \"why_verify_identity\", \"wrong_amount_of_cash_received\",\n",
    "    \"wrong_exchange_rate_for_cash_withdrawal\",\n",
    "]\n",
    "\n",
    "TOOL_NAME = \"banking77_classify\"\n",
    "TOOL_SCHEMA = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": TOOL_NAME,\n",
    "        \"description\": \"Return the predicted banking77 intent label.\",\n",
    "        \"parameters\": {\"type\": \"object\", \"properties\": {\"intent\": {\"type\": \"string\"}}, \"required\": [\"intent\"]},\n",
    "    },\n",
    "}\n",
    "\n",
    "class Banking77Dataset:\n",
    "    \"\"\"Lazy Hugging Face dataset loader for Banking77.\"\"\"\n",
    "    def __init__(self):\n",
    "        self._cache = {}\n",
    "        self._label_names = None\n",
    "\n",
    "    def _load_split(self, split: str):\n",
    "        if split not in self._cache:\n",
    "            ds = load_dataset(\"banking77\", split=split, trust_remote_code=False)\n",
    "            self._cache[split] = ds\n",
    "            if self._label_names is None and hasattr(ds.features.get(\"label\"), \"names\"):\n",
    "                self._label_names = ds.features[\"label\"].names\n",
    "        return self._cache[split]\n",
    "\n",
    "    def ensure_ready(self, splits):\n",
    "        for split in splits:\n",
    "            self._load_split(split)\n",
    "\n",
    "    def size(self, split: str) -> int:\n",
    "        return len(self._load_split(split))\n",
    "\n",
    "    def sample(self, *, split: str, index: int) -> dict:\n",
    "        ds = self._load_split(split)\n",
    "        idx = index % len(ds)\n",
    "        row = ds[idx]\n",
    "        label_idx = int(row.get(\"label\", 0))\n",
    "        label_text = self._label_names[label_idx] if self._label_names and label_idx < len(self._label_names) else f\"label_{label_idx}\"\n",
    "        return {\"index\": idx, \"split\": split, \"text\": str(row.get(\"text\", \"\")), \"label\": label_text}\n",
    "\n",
    "    @property\n",
    "    def label_names(self) -> list:\n",
    "        if self._label_names is None:\n",
    "            self._load_split(\"train\")\n",
    "        return self._label_names or []\n",
    "\n",
    "class Banking77Scorer:\n",
    "    \"\"\"Scorer for Banking77 intent classification.\"\"\"\n",
    "    @staticmethod\n",
    "    def normalize_intent(intent: str) -> str:\n",
    "        return intent.lower().replace(\"_\", \" \").strip()\n",
    "\n",
    "    @classmethod\n",
    "    def score(cls, predicted: str, expected: str) -> tuple:\n",
    "        is_correct = cls.normalize_intent(predicted) == cls.normalize_intent(expected)\n",
    "        return is_correct, 1.0 if is_correct else 0.0\n",
    "\n",
    "def format_available_intents(label_names: list) -> str:\n",
    "    return \"\\n\".join(f\"{i+1}. {l}\" for i, l in enumerate(label_names))\n",
    "\n",
    "# ============================================================================\n",
    "# Local API Factory using SDK abstractions\n",
    "# ============================================================================\n",
    "\n",
    "def create_banking77_local_api(system_prompt: str, env_api_key: str):\n",
    "    \"\"\"Factory to create a Banking77 local API using SDK abstractions.\"\"\"\n",
    "    from fastapi import Request\n",
    "    \n",
    "    # Set ENVIRONMENT_API_KEY so SDK auth middleware can validate it\n",
    "    os.environ[\"ENVIRONMENT_API_KEY\"] = env_api_key\n",
    "    \n",
    "    # Initialize dataset\n",
    "    dataset = Banking77Dataset()\n",
    "    dataset.ensure_ready([\"train\", \"test\"])\n",
    "    \n",
    "    # Create HTTP client lifecycle hooks\n",
    "    startup_http_client, shutdown_http_client = create_http_client_hooks(\n",
    "        timeout=60.0,\n",
    "        log_prefix=\"banking77_local_api\",\n",
    "    )\n",
    "    \n",
    "    async def rollout_executor(request: RolloutRequest, fastapi_request: Request) -> RolloutResponse:\n",
    "        \"\"\"Execute a rollout for the banking77 classification task.\"\"\"\n",
    "        policy_config = request.policy.config or {}\n",
    "        split = (request.env.config or {}).get(\"split\", \"train\")\n",
    "        seed = request.env.seed or 0\n",
    "        \n",
    "        # Get sample from dataset\n",
    "        sample = dataset.sample(split=split, index=seed)\n",
    "        intents_list = format_available_intents(dataset.label_names)\n",
    "        \n",
    "        # Build messages\n",
    "        user_msg = f\"Customer Query: {sample['text']}\\n\\nAvailable Intents:\\n{intents_list}\\n\\nClassify this query into one of the above banking intents using the tool call.\"\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_msg}]\n",
    "        \n",
    "        # Extract API key using SDK helper\n",
    "        api_key = extract_api_key(fastapi_request, policy_config)\n",
    "        \n",
    "        # Get HTTP client from app state\n",
    "        http_client = getattr(fastapi_request.app.state, \"http_client\", None)\n",
    "        \n",
    "        # Call chat completion API using SDK helper\n",
    "        response_text, response_json, tool_calls = await call_chat_completion_api(\n",
    "            policy_config=policy_config,\n",
    "            messages=messages,\n",
    "            tools=[TOOL_SCHEMA],\n",
    "            tool_choice=\"required\",\n",
    "            api_key=api_key,\n",
    "            http_client=http_client,\n",
    "            enable_dns_preresolution=True,\n",
    "            expected_tool_name=TOOL_NAME,\n",
    "            log_prefix=\"[LOCAL_API]\",\n",
    "        )\n",
    "        \n",
    "        # Extract predicted intent from tool calls\n",
    "        predicted_intent = \"\"\n",
    "        if tool_calls:\n",
    "            for tc in tool_calls:\n",
    "                if tc.get(\"function\", {}).get(\"name\") == TOOL_NAME:\n",
    "                    args_str = tc.get(\"function\", {}).get(\"arguments\", \"{}\")\n",
    "                    try:\n",
    "                        args = json.loads(args_str)\n",
    "                        predicted_intent = args.get(\"intent\", \"\")\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        elif response_text:\n",
    "            predicted_intent = response_text.strip().split()[0] if response_text.strip() else \"\"\n",
    "        \n",
    "        if not predicted_intent:\n",
    "            predicted_intent = \"__NO_PREDICTION__\"\n",
    "        \n",
    "        # Score prediction using business logic\n",
    "        expected_intent = sample[\"label\"]\n",
    "        is_correct, reward = Banking77Scorer.score(predicted_intent, expected_intent)\n",
    "        \n",
    "        # Build trace correlation ID\n",
    "        trace_correlation_id = policy_config.get(\"trace_correlation_id\")\n",
    "        if not trace_correlation_id:\n",
    "            from urllib.parse import urlsplit, parse_qs\n",
    "            try:\n",
    "                parsed = urlsplit(policy_config.get(\"inference_url\", \"\"))\n",
    "                cid_vals = parse_qs(parsed.query or \"\").get(\"cid\", [])\n",
    "                if cid_vals:\n",
    "                    trace_correlation_id = cid_vals[0]\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        # Build trace\n",
    "        trace = {\n",
    "            \"messages\": messages,\n",
    "            \"response\": response_json,\n",
    "            \"correlation_id\": trace_correlation_id,\n",
    "            \"model\": response_json.get(\"model\") if isinstance(response_json, dict) else None,\n",
    "            \"metadata\": {\"env\": \"banking77\", \"split\": sample[\"split\"], \"index\": sample[\"index\"], \"correct\": is_correct}\n",
    "        }\n",
    "        \n",
    "        metrics = RolloutMetrics(\n",
    "            episode_returns=[reward],\n",
    "            mean_return=reward,\n",
    "            num_steps=1,\n",
    "            num_episodes=1,\n",
    "            outcome_score=reward,\n",
    "            events_score=reward,\n",
    "            details={\"correct\": is_correct},\n",
    "        )\n",
    "        \n",
    "        return RolloutResponse(\n",
    "            run_id=request.run_id,\n",
    "            branches={},\n",
    "            metrics=metrics,\n",
    "            aborted=False,\n",
    "            trace_correlation_id=trace_correlation_id,\n",
    "            trace=trace,\n",
    "            pipeline_metadata={\"inference_url\": policy_config.get(\"inference_url\", \"\")},\n",
    "        )\n",
    "    \n",
    "    # Define taskset descriptor\n",
    "    def describe_taskset():\n",
    "        return {\n",
    "            \"id\": \"banking77\",\n",
    "            \"name\": \"Banking77 Intent Classification\",\n",
    "            \"splits\": [\"train\", \"test\"],\n",
    "            \"num_labels\": len(dataset.label_names),\n",
    "            \"sizes\": {\"train\": dataset.size(\"train\"), \"test\": dataset.size(\"test\")},\n",
    "        }\n",
    "    \n",
    "    # Define task instance provider (use plain dicts to avoid pydantic model unpacking issues)\n",
    "    def provide_task_instances(seeds):\n",
    "        base_dataset = {\"id\": \"banking77\", \"splits\": [\"train\", \"test\"]}\n",
    "        base_task_meta = {\"format\": \"tool_call\"}\n",
    "        for seed in seeds:\n",
    "            sample = dataset.sample(split=\"train\", index=seed)\n",
    "            yield TaskInfo(\n",
    "                task={\"id\": \"banking77\", \"name\": \"Banking77 Intent Classification\", \"version\": \"1.0.0\"},\n",
    "                environment=\"banking77\",\n",
    "                dataset={**base_dataset, \"split\": sample[\"split\"], \"index\": sample[\"index\"]},\n",
    "                rubric={\"version\": \"1\", \"criteria_count\": 1},\n",
    "                inference={\"supports_proxy\": True, \"tool\": TOOL_NAME},\n",
    "                limits={\"max_turns\": 1},\n",
    "                task_metadata={**base_task_meta, \"query\": sample[\"text\"], \"expected_intent\": sample[\"label\"]},\n",
    "            )\n",
    "    \n",
    "    # Build LocalAPIConfig\n",
    "    config = LocalAPIConfig(\n",
    "        app_id=\"banking77\",\n",
    "        name=\"Banking77 Intent Classification\",\n",
    "        description=\"Banking77 local API for classifying customer queries into banking intents.\",\n",
    "        base_task_info=TaskInfo(\n",
    "            task={\"id\": \"banking77\", \"name\": \"Banking77 Intent Classification\", \"version\": \"1.0.0\"},\n",
    "            environment=\"banking77\",\n",
    "            dataset={\"id\": \"banking77\", \"splits\": [\"train\", \"test\"]},\n",
    "            rubric={\"version\": \"1\", \"criteria_count\": 1},\n",
    "            inference={\"supports_proxy\": True, \"tool\": TOOL_NAME},\n",
    "            limits={\"max_turns\": 1},\n",
    "            task_metadata={\"format\": \"tool_call\"},\n",
    "        ),\n",
    "        describe_taskset=describe_taskset,\n",
    "        provide_task_instances=provide_task_instances,\n",
    "        rollout=rollout_executor,\n",
    "        app_state={\"banking77_dataset\": dataset},\n",
    "        cors_origins=[\"*\"],\n",
    "        startup_hooks=[startup_http_client],\n",
    "        shutdown_hooks=[shutdown_http_client],\n",
    "    )\n",
    "    \n",
    "    # Create the FastAPI app using SDK\n",
    "    return create_local_api(config)\n",
    "\n",
    "print('Banking77 local API factory defined (using SDK abstractions)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y0l3voghzj",
   "metadata": {},
   "source": "## Step 5: Start Baseline Local API\n\nNow we'll spin up the baseline local API and expose it via a Cloudflare tunnel. The SDK provides helpers for tunnel management (`rotate_tunnel`, `connect_managed_tunnel`) and process lifecycle (`track_process`, `kill_port`)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Start Baseline Local API with Cloudflare Tunnel\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Import tunnel helpers from SDK\n",
    "from synth_ai.sdk.tunnels import (\n",
    "    rotate_tunnel,\n",
    "    connect_managed_tunnel,  # High-level: start + wait + verify\n",
    "    track_process,\n",
    "    kill_port,\n",
    "    wait_for_health_check,\n",
    ")\n",
    "from synth_ai.sdk.task import run_server_background\n",
    "\n",
    "BASELINE_SYSTEM_PROMPT = \"You are an expert banking assistant that classifies customer queries into banking intents. Given a customer message, respond with exactly one intent label from the provided list using the `banking77_classify` tool.\"\n",
    "USER_PROMPT = \"Customer Query: {query}\\n\\nAvailable Intents:\\n{available_intents}\\n\\nClassify this query into one of the above banking intents using the tool call.\"\n",
    "\n",
    "# Create baseline local API using SDK abstractions\n",
    "baseline_app = create_banking77_local_api(BASELINE_SYSTEM_PROMPT, ENVIRONMENT_API_KEY)\n",
    "\n",
    "# Kill port if in use, start server in background\n",
    "kill_port(TASK_APP_PORT)\n",
    "run_server_background(baseline_app, TASK_APP_PORT)\n",
    "\n",
    "# Wait for local health check\n",
    "print(f'Waiting for baseline local API on port {TASK_APP_PORT}...')\n",
    "await wait_for_health_check(\"localhost\", TASK_APP_PORT, ENVIRONMENT_API_KEY, timeout=30.0)\n",
    "print('Baseline local API ready!')\n",
    "\n",
    "# Get tunnel from backend\n",
    "print('\\nProvisioning Cloudflare tunnel for baseline...')\n",
    "baseline_tunnel = await rotate_tunnel(API_KEY, TASK_APP_PORT, reason=\"baseline_notebook\")\n",
    "BASELINE_TUNNEL_HOSTNAME = baseline_tunnel['hostname']\n",
    "BASELINE_LOCAL_API_URL = f'https://{BASELINE_TUNNEL_HOSTNAME}'\n",
    "\n",
    "# Connect tunnel with automatic waiting and verification (handles all the timing)\n",
    "baseline_proc = await connect_managed_tunnel(\n",
    "    baseline_tunnel['tunnel_token'],\n",
    "    BASELINE_TUNNEL_HOSTNAME,\n",
    "    api_key=ENVIRONMENT_API_KEY,\n",
    ")\n",
    "track_process(baseline_proc)\n",
    "\n",
    "print(f'\\nBaseline local API URL: {BASELINE_LOCAL_API_URL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "btowuuijz1w",
   "metadata": {},
   "source": [
    "## Step 6: Run GEPA Optimization\n",
    "\n",
    "This kicks off the GEPA prompt optimization job on the Synth backend. GEPA will evolve the system prompt over multiple generations, evaluating each candidate against the training seeds via our tunneled local API. Because evolutionary prompt optimization requires multiple sequential rounds of evals, expect the process to take roughly as long as (time to eval) x (number of generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Run GEPA Optimization (using tunnel URL)\n",
    "async def run_gepa():\n",
    "    config_body = {\n",
    "        'prompt_learning': {\n",
    "            'algorithm': 'gepa',\n",
    "            'run_local': False,  # Run on remote backend\n",
    "            'task_app_url': BASELINE_LOCAL_API_URL,\n",
    "            'task_app_api_key': ENVIRONMENT_API_KEY,\n",
    "            'env_name': 'banking77',\n",
    "            'initial_prompt': {\n",
    "                'messages': [\n",
    "                    {'role': 'system', 'order': 0, 'pattern': BASELINE_SYSTEM_PROMPT},\n",
    "                    {'role': 'user', 'order': 1, 'pattern': USER_PROMPT},\n",
    "                ],\n",
    "                'wildcards': {'query': 'REQUIRED', 'available_intents': 'OPTIONAL'},\n",
    "            },\n",
    "            'policy': {'model': 'gpt-4.1-nano', 'provider': 'openai', 'temperature': 0.0, 'max_completion_tokens': 256},\n",
    "            'gepa': {\n",
    "                'env_name': 'banking77',\n",
    "                'evaluation': {'seeds': list(range(30)), 'validation_seeds': list(range(50, 56))},\n",
    "                'rollout': {'budget': 50, 'max_concurrent': 5, 'minibatch_size': 5},\n",
    "                'mutation': {'rate': 0.3, 'llm_model': 'gpt-4.1-nano'},\n",
    "                'population': {'initial_size': 3, 'num_generations': 2, 'children_per_generation': 2},\n",
    "                'archive': {'size': 5, 'pareto_set_size': 10},\n",
    "                'token': {'counting_model': 'gpt-4'},\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(f'Creating GEPA job (local_api_url={BASELINE_LOCAL_API_URL})...')\n",
    "    async with httpx.AsyncClient(timeout=30) as client:\n",
    "        resp = await client.post(\n",
    "            f'{SYNTH_API_BASE}/api/prompt-learning/online/jobs',\n",
    "            json={'algorithm': 'gepa', 'config_body': config_body},\n",
    "            headers={'Authorization': f'Bearer {API_KEY}'}\n",
    "        )\n",
    "        if resp.status_code != 200:\n",
    "            print(f'ERROR: {resp.status_code} - {resp.text[:500]}')\n",
    "            resp.raise_for_status()\n",
    "        job_id = resp.json()['job_id']\n",
    "    print(f'Job ID: {job_id}')\n",
    "\n",
    "    print('Polling...')\n",
    "    start = time.time()\n",
    "    last_status = None\n",
    "    job = None\n",
    "    \n",
    "    while True:\n",
    "        async with httpx.AsyncClient(timeout=30) as client:\n",
    "            resp = await client.get(\n",
    "                f'{SYNTH_API_BASE}/api/prompt-learning/online/jobs/{job_id}',\n",
    "                headers={'Authorization': f'Bearer {API_KEY}'}\n",
    "            )\n",
    "            resp.raise_for_status()\n",
    "            job = resp.json()\n",
    "        \n",
    "        status = job['status']\n",
    "        elapsed = int(time.time() - start)\n",
    "        best = job.get('best_train_score') or job.get('best_score')\n",
    "        \n",
    "        if status != last_status or elapsed % 15 == 0:\n",
    "            print(f'    [{elapsed}s] {status} (best={best})')\n",
    "            last_status = status\n",
    "        \n",
    "        if status in ['succeeded', 'failed', 'cancelled']:\n",
    "            break\n",
    "        await asyncio.sleep(3)\n",
    "\n",
    "    print(f'\\nFINAL: {status}')\n",
    "    if status == 'succeeded':\n",
    "        best = job.get('best_score') or job.get('best_train_score')\n",
    "        print(f'BEST SCORE: {best}')\n",
    "    elif status == 'failed':\n",
    "        print(f'ERROR: {job.get(\"error\")}')\n",
    "    \n",
    "    return job\n",
    "\n",
    "job = await run_gepa()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fgaj2jzfvr",
   "metadata": {},
   "source": [
    "## Step 7: Evaluation\n",
    "\n",
    "Now, we'll run an evaluation for both the baseline prompt we started with, and the optimized prompt. This will be on a separate, but still randomly drawn dataset, so it will tell us if the optimizer in fact improved the prompt generally, or if it overfit to the trainset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Run Formal Eval Jobs (Baseline vs Optimized)\n",
    "from synth_ai.sdk.learning.prompt_learning_client import PromptLearningClient\n",
    "\n",
    "EVAL_SEEDS = list(range(100, 120))  # 20 held-out test samples\n",
    "\n",
    "async def run_eval_job(local_api_url: str, local_api_key: str, seeds: list, mode: str) -> dict:\n",
    "    \"\"\"Run an eval job and wait for completion.\"\"\"\n",
    "    async with httpx.AsyncClient(timeout=30) as client:\n",
    "        resp = await client.post(\n",
    "            f'{SYNTH_API_BASE}/api/eval/jobs',\n",
    "            json={\n",
    "                'task_app_url': local_api_url,\n",
    "                'task_app_api_key': local_api_key,\n",
    "                'env_name': 'banking77',\n",
    "                'seeds': seeds,\n",
    "                'env_config': {'split': 'test'},\n",
    "                'policy': {'model': 'gpt-4.1-nano', 'provider': 'openai'},\n",
    "                'mode': mode,\n",
    "                'max_concurrent': 10,\n",
    "            },\n",
    "            headers={'Authorization': f'Bearer {API_KEY}'}\n",
    "        )\n",
    "        if resp.status_code != 200:\n",
    "            print(f'ERROR creating {mode} eval job: {resp.status_code} - {resp.text[:300]}')\n",
    "            return {'status': 'failed', 'error': resp.text}\n",
    "        \n",
    "        job_id = resp.json()['job_id']\n",
    "        print(f'  {mode} eval job: {job_id}')\n",
    "    \n",
    "    start = time.time()\n",
    "    while True:\n",
    "        async with httpx.AsyncClient(timeout=30) as client:\n",
    "            resp = await client.get(\n",
    "                f'{SYNTH_API_BASE}/api/eval/jobs/{job_id}',\n",
    "                headers={'Authorization': f'Bearer {API_KEY}'}\n",
    "            )\n",
    "            if resp.status_code != 200:\n",
    "                print(f'  Error polling: {resp.status_code}')\n",
    "                await asyncio.sleep(2)\n",
    "                continue\n",
    "            job = resp.json()\n",
    "        \n",
    "        status = job.get('status', '')\n",
    "        elapsed = int(time.time() - start)\n",
    "        \n",
    "        if status in ['completed', 'failed']:\n",
    "            break\n",
    "        \n",
    "        if elapsed % 10 == 0:\n",
    "            results = job.get('results') or {}\n",
    "            completed = results.get('completed', 0)\n",
    "            total = results.get('total', len(seeds))\n",
    "            print(f'    [{elapsed}s] {status} ({completed}/{total})')\n",
    "        \n",
    "        await asyncio.sleep(2)\n",
    "    \n",
    "    async with httpx.AsyncClient(timeout=30) as client:\n",
    "        resp = await client.get(\n",
    "            f'{SYNTH_API_BASE}/api/eval/jobs/{job_id}/results',\n",
    "            headers={'Authorization': f'Bearer {API_KEY}'}\n",
    "        )\n",
    "        if resp.status_code != 200:\n",
    "            return {'status': 'failed', 'error': f'Failed to get results: {resp.status_code}'}\n",
    "        return resp.json()\n",
    "\n",
    "if job['status'] == 'succeeded':\n",
    "    print(\"GEPA Job Succeeded!\\n\")\n",
    "    \n",
    "    # Use PromptLearningClient to properly retrieve the optimized prompt from events\n",
    "    pl_client = PromptLearningClient(SYNTH_API_BASE, API_KEY)\n",
    "    prompt_results = await pl_client.get_prompts(job['job_id'])\n",
    "    \n",
    "    # Extract optimized system prompt from best_prompt\n",
    "    optimized_system = None\n",
    "    best_prompt = prompt_results.best_prompt\n",
    "    if best_prompt:\n",
    "        # best_prompt may have 'messages' list with role/pattern structure\n",
    "        messages = best_prompt.get('messages', [])\n",
    "        for msg in messages:\n",
    "            if msg.get('role') == 'system':\n",
    "                optimized_system = msg.get('pattern') or msg.get('content')\n",
    "                break\n",
    "        \n",
    "        # If messages is empty, try 'sections' structure (alternative format)\n",
    "        if not optimized_system:\n",
    "            sections = best_prompt.get('sections', [])\n",
    "            for sec in sections:\n",
    "                if sec.get('role') == 'system':\n",
    "                    optimized_system = sec.get('content')\n",
    "                    break\n",
    "    \n",
    "    # Fallback: try extracting from top_prompts full_text\n",
    "    if not optimized_system and prompt_results.top_prompts:\n",
    "        top_prompt = prompt_results.top_prompts[0]\n",
    "        full_text = top_prompt.get('full_text', '')\n",
    "        # Extract system content from full_text format: \"[system | system_prompt]\\ncontent\"\n",
    "        if '[system' in full_text.lower():\n",
    "            import re\n",
    "            match = re.search(r'\\[system[^\\]]*\\]\\s*\\n(.*?)(?:\\n\\n\\[|$)', full_text, re.DOTALL | re.IGNORECASE)\n",
    "            if match:\n",
    "                optimized_system = match.group(1).strip()\n",
    "    \n",
    "    # If we still don't have optimized_system, use baseline for comparison\n",
    "    if not optimized_system:\n",
    "        print(\"NOTE: Could not extract optimized prompt from job details, using baseline for comparison\")\n",
    "        optimized_system = BASELINE_SYSTEM_PROMPT\n",
    "    \n",
    "    # Get scores from prompt_results\n",
    "    best_score = prompt_results.best_score\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('BASELINE SYSTEM PROMPT')\n",
    "    print('=' * 60)\n",
    "    print(BASELINE_SYSTEM_PROMPT)\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print('OPTIMIZED SYSTEM PROMPT (from GEPA)')\n",
    "    print('=' * 60)\n",
    "    print(optimized_system[:800] + \"...\" if len(optimized_system) > 800 else optimized_system)\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print('GEPA TRAINING RESULTS')\n",
    "    print('=' * 60)\n",
    "    if best_score:\n",
    "        print(f\"Best Train Score: {best_score:.1%}\")\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print(f'FORMAL EVAL JOBS (test split, seeds {EVAL_SEEDS[0]}-{EVAL_SEEDS[-1]})')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Start optimized local API on different port using SDK abstractions\n",
    "    print(f'\\nStarting optimized local API on port {OPTIMIZED_TASK_APP_PORT}...')\n",
    "    optimized_app = create_banking77_local_api(optimized_system, ENVIRONMENT_API_KEY)\n",
    "    \n",
    "    kill_port(OPTIMIZED_TASK_APP_PORT)\n",
    "    run_server_background(optimized_app, OPTIMIZED_TASK_APP_PORT)\n",
    "    await wait_for_health_check(\"localhost\", OPTIMIZED_TASK_APP_PORT, ENVIRONMENT_API_KEY, timeout=30.0)\n",
    "    print('Optimized local API ready!')\n",
    "    \n",
    "    # Get tunnel for optimized\n",
    "    print('\\nProvisioning Cloudflare tunnel for optimized...')\n",
    "    optimized_tunnel = await rotate_tunnel(API_KEY, OPTIMIZED_TASK_APP_PORT, reason=\"optimized_notebook\")\n",
    "    OPTIMIZED_TUNNEL_HOSTNAME = optimized_tunnel['hostname']\n",
    "    OPTIMIZED_LOCAL_API_URL = f'https://{OPTIMIZED_TUNNEL_HOSTNAME}'\n",
    "    \n",
    "    # Connect tunnel with automatic waiting and verification (handles all the timing)\n",
    "    optimized_proc = await connect_managed_tunnel(\n",
    "        optimized_tunnel['tunnel_token'],\n",
    "        OPTIMIZED_TUNNEL_HOSTNAME,\n",
    "        api_key=ENVIRONMENT_API_KEY,\n",
    "    )\n",
    "    track_process(optimized_proc)\n",
    "    \n",
    "    # Run baseline eval\n",
    "    print('\\nRunning BASELINE eval job...')\n",
    "    baseline_results = await run_eval_job(\n",
    "        local_api_url=BASELINE_LOCAL_API_URL,\n",
    "        local_api_key=ENVIRONMENT_API_KEY,\n",
    "        seeds=EVAL_SEEDS,\n",
    "        mode='baseline'\n",
    "    )\n",
    "    \n",
    "    baseline_summary = baseline_results.get('summary', {})\n",
    "    baseline_eval_score = baseline_summary.get('mean_score')\n",
    "    print(f'  Baseline eval: {baseline_eval_score:.1%}' if baseline_eval_score is not None else '  Baseline eval: N/A')\n",
    "    \n",
    "    # Run optimized eval\n",
    "    print('\\nRunning OPTIMIZED eval job...')\n",
    "    optimized_results = await run_eval_job(\n",
    "        local_api_url=OPTIMIZED_LOCAL_API_URL,\n",
    "        local_api_key=ENVIRONMENT_API_KEY,\n",
    "        seeds=EVAL_SEEDS,\n",
    "        mode='optimized'\n",
    "    )\n",
    "    \n",
    "    optimized_summary = optimized_results.get('summary', {})\n",
    "    optimized_eval_score = optimized_summary.get('mean_score')\n",
    "    print(f'  Optimized eval: {optimized_eval_score:.1%}' if optimized_eval_score is not None else '  Optimized eval: N/A')\n",
    "    \n",
    "    # Final comparison\n",
    "    print('\\n' + '=' * 60)\n",
    "    print('FINAL COMPARISON')\n",
    "    print('=' * 60)\n",
    "    print(f\"Training:\")\n",
    "    if best_score:\n",
    "        print(f\"  Best Score: {best_score:.1%}\")\n",
    "    \n",
    "    print(f\"\\nEval (seeds {EVAL_SEEDS[0]}-{EVAL_SEEDS[-1]}, held-out):\")\n",
    "    if baseline_eval_score is not None:\n",
    "        print(f\"  Baseline:  {baseline_eval_score:.1%}\")\n",
    "    if optimized_eval_score is not None:\n",
    "        print(f\"  Optimized: {optimized_eval_score:.1%}\")\n",
    "    if baseline_eval_score is not None and optimized_eval_score is not None:\n",
    "        eval_lift = optimized_eval_score - baseline_eval_score\n",
    "        print(f\"  Lift:      {eval_lift:+.1%}\")\n",
    "        \n",
    "        if eval_lift > 0:\n",
    "            print(\"\\n>>> OPTIMIZATION GENERALIZES TO HELD-OUT DATA!\")\n",
    "        elif eval_lift == 0:\n",
    "            print(\"\\n=== Same performance on held-out data\")\n",
    "        else:\n",
    "            print(\"\\n<<< Baseline better on held-out (possible overfitting)\")\n",
    "else:\n",
    "    print(f\"Job did not succeed: {job.get('status')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0smmbd65001",
   "metadata": {},
   "source": [
    "## Step 8: Cleanup\n",
    "\n",
    "Terminate cloudflared tunnel processes to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Cleanup\n",
    "from synth_ai.sdk.tunnels import cleanup_all\n",
    "\n",
    "print('Cleaning up cloudflared processes...')\n",
    "cleanup_all()\n",
    "print('Demo complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}