{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Synth GEPA Demo - Banking77 (Production)\n",
    "\n",
    "Self-contained notebook for prompt optimization using GEPA against the **production backend**.\n",
    "\n",
    "**Run in Google Colab:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/synth-laboratories/synth-ai/blob/main/demos/gepa_banking77/demo_prod.ipynb)\n",
    "\n",
    "**What this demo does:**\n",
    "1. Spins up a local Banking77 classification task app\n",
    "2. Creates a Cloudflare tunnel to expose it to the internet\n",
    "3. Runs GEPA prompt optimization on the Synth backend\n",
    "4. Compares baseline vs optimized prompts on held-out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ku8p0wn7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Install dependencies (run this first on Colab)\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab - installing dependencies...\")\n",
    "    !pip install -q httpx pynacl fastapi uvicorn datasets nest_asyncio\n",
    "    \n",
    "    # Install cloudflared\n",
    "    !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O /usr/local/bin/cloudflared\n",
    "    !chmod +x /usr/local/bin/cloudflared\n",
    "    !cloudflared --version\n",
    "    \n",
    "    print(\"Dependencies installed!\")\n",
    "else:\n",
    "    print(\"Not in Colab - assuming dependencies are already installed\")\n",
    "    print(\"Required: pip install httpx pynacl fastapi uvicorn datasets nest_asyncio\")\n",
    "    print(\"Required: brew install cloudflare/cloudflare/cloudflared (macOS)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Imports and Config\nimport os, sys, time, secrets, base64, asyncio, json, threading\nfrom typing import Any, Optional\nfrom contextlib import asynccontextmanager\n\nimport httpx\nimport uvicorn\nfrom nacl.public import PublicKey, SealedBox\nfrom fastapi import FastAPI, Request, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom datasets import load_dataset\n\n# Production backend\nSYNTH_API_BASE = 'https://api.usesynth.ai'\nTASK_APP_PORT = 8001\nOPTIMIZED_TASK_APP_PORT = 8002\n\nprint(f'Backend: {SYNTH_API_BASE}')\nprint(f'Task App Ports: {TASK_APP_PORT}, {OPTIMIZED_TASK_APP_PORT}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Check Backend Health\n",
    "r = httpx.get(f'{SYNTH_API_BASE}/health', timeout=30)\n",
    "if r.status_code == 200:\n",
    "    print(f'Backend health: {r.json()}')\n",
    "else:\n",
    "    print(f'WARNING: Backend returned status {r.status_code}')\n",
    "    print(f'Response: {r.text[:200]}...' if len(r.text) > 200 else f'Response: {r.text}')\n",
    "    raise RuntimeError(f'Backend not healthy: status {r.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No SYNTH_API_KEY found, minting demo key...\n",
      "Demo API Key: sk_demo_a3T-ldJZ0ZqDJ0TE7...\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Get API Key (use env var or mint demo key)\n",
    "API_KEY = os.environ.get('SYNTH_API_KEY', '')\n",
    "\n",
    "if not API_KEY:\n",
    "    print('No SYNTH_API_KEY found, minting demo key...')\n",
    "    resp = httpx.post(f'{SYNTH_API_BASE}/api/demo/keys', json={'ttl_hours': 4}, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    API_KEY = resp.json()['api_key']\n",
    "    print(f'Demo API Key: {API_KEY[:25]}...')\n",
    "else:\n",
    "    print(f'Using SYNTH_API_KEY: {API_KEY[:20]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minted env key: 115860176ce8...47c3\n",
      "Uploaded env key: {'id': '88c0a188-d511-4f23-a59b-9e1fa46ceda9', 'name': 'ENVIRONMENT_API_KEY', 'updated_at': '2025-12-27T05:15:01.143970Z', 'org_id': 'e77ef3a8-677d-4ddd-92d6-0f114d6bbdaf', 'upserted': True}\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Mint and Upload Environment Key\n",
    "ENVIRONMENT_API_KEY = secrets.token_hex(32)\n",
    "print(f'Minted env key: {ENVIRONMENT_API_KEY[:12]}...{ENVIRONMENT_API_KEY[-4:]}')\n",
    "\n",
    "pub_resp = httpx.get(f'{SYNTH_API_BASE}/api/v1/crypto/public-key', \n",
    "                     headers={'Authorization': f'Bearer {API_KEY}'}, timeout=30)\n",
    "pub_resp.raise_for_status()\n",
    "pubkey_b64 = pub_resp.json()['public_key']\n",
    "\n",
    "key_bytes = base64.b64decode(pubkey_b64, validate=True)\n",
    "box = SealedBox(PublicKey(key_bytes))\n",
    "ciphertext = box.encrypt(ENVIRONMENT_API_KEY.encode('utf-8'))\n",
    "ciphertext_b64 = base64.b64encode(ciphertext).decode('ascii')\n",
    "\n",
    "upload_resp = httpx.post(\n",
    "    f'{SYNTH_API_BASE}/api/v1/env-keys',\n",
    "    headers={'Authorization': f'Bearer {API_KEY}', 'Content-Type': 'application/json'},\n",
    "    json={'name': 'ENVIRONMENT_API_KEY', 'ciphertext_b64': ciphertext_b64},\n",
    "    timeout=30\n",
    ")\n",
    "upload_resp.raise_for_status()\n",
    "print(f'Uploaded env key: {upload_resp.json()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task app factory defined (with debug logging)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Define Banking77 Task App Factory\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "BANKING77_LABELS = [\n",
    "    \"activate_my_card\", \"age_limit\", \"apple_pay_or_google_pay\", \"atm_support\", \"automatic_top_up\",\n",
    "    \"balance_not_updated_after_bank_transfer\", \"balance_not_updated_after_cheque_or_cash_deposit\",\n",
    "    \"beneficiary_not_allowed\", \"cancel_transfer\", \"card_about_to_expire\", \"card_acceptance\",\n",
    "    \"card_arrival\", \"card_delivery_estimate\", \"card_linking\", \"card_not_working\",\n",
    "    \"card_payment_fee_charged\", \"card_payment_not_recognised\", \"card_payment_wrong_exchange_rate\",\n",
    "    \"card_swallowed\", \"cash_withdrawal_charge\", \"cash_withdrawal_not_recognised\", \"change_pin\",\n",
    "    \"compromised_card\", \"contactless_not_working\", \"country_support\", \"declined_card_payment\",\n",
    "    \"declined_cash_withdrawal\", \"declined_transfer\", \"direct_debit_payment_not_recognised\",\n",
    "    \"disposable_card_limits\", \"edit_personal_details\", \"exchange_charge\", \"exchange_rate\",\n",
    "    \"exchange_via_app\", \"extra_charge_on_statement\", \"failed_transfer\", \"fiat_currency_support\",\n",
    "    \"get_disposable_virtual_card\", \"get_physical_card\", \"getting_spare_card\", \"getting_virtual_card\",\n",
    "    \"lost_or_stolen_card\", \"lost_or_stolen_phone\", \"order_physical_card\", \"passcode_forgotten\",\n",
    "    \"pending_card_payment\", \"pending_cash_withdrawal\", \"pending_top_up\", \"pending_transfer\",\n",
    "    \"pin_blocked\", \"receiving_money\", \"Refund_not_showing_up\", \"request_refund\",\n",
    "    \"reverted_card_payment?\", \"supported_cards_and_currencies\", \"terminate_account\",\n",
    "    \"top_up_by_bank_transfer_charge\", \"top_up_by_card_charge\", \"top_up_by_cash_or_cheque\",\n",
    "    \"top_up_failed\", \"top_up_limits\", \"top_up_reverted\", \"topping_up_by_card\",\n",
    "    \"transaction_charged_twice\", \"transfer_fee_charged\", \"transfer_into_account\",\n",
    "    \"transfer_not_received_by_recipient\", \"transfer_timing\", \"unable_to_verify_identity\",\n",
    "    \"verify_my_identity\", \"verify_source_of_funds\", \"verify_top_up\", \"virtual_card_not_working\",\n",
    "    \"visa_or_mastercard\", \"why_verify_identity\", \"wrong_amount_of_cash_received\",\n",
    "    \"wrong_exchange_rate_for_cash_withdrawal\",\n",
    "]\n",
    "TOOL_NAME = \"banking77_classify\"\n",
    "TOOL_SCHEMA = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": TOOL_NAME,\n",
    "        \"description\": \"Return the predicted banking77 intent label.\",\n",
    "        \"parameters\": {\"type\": \"object\", \"properties\": {\"intent\": {\"type\": \"string\"}}, \"required\": [\"intent\"]},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Dataset cache\n",
    "_dataset_cache = {}\n",
    "_label_names = None\n",
    "\n",
    "def load_dataset_split(split: str):\n",
    "    global _label_names\n",
    "    if split not in _dataset_cache:\n",
    "        ds = load_dataset(\"banking77\", split=split, trust_remote_code=False)\n",
    "        _dataset_cache[split] = ds\n",
    "        if _label_names is None and hasattr(ds.features.get(\"label\"), \"names\"):\n",
    "            _label_names = ds.features[\"label\"].names\n",
    "    return _dataset_cache[split]\n",
    "\n",
    "def get_sample(split: str, index: int) -> dict:\n",
    "    ds = load_dataset_split(split)\n",
    "    idx = index % len(ds)\n",
    "    row = ds[idx]\n",
    "    label_idx = int(row.get(\"label\", 0))\n",
    "    label_text = _label_names[label_idx] if _label_names and label_idx < len(_label_names) else f\"label_{label_idx}\"\n",
    "    return {\"index\": idx, \"split\": split, \"text\": str(row.get(\"text\", \"\")), \"label\": label_text}\n",
    "\n",
    "def format_intents() -> str:\n",
    "    return \"\\n\".join(f\"{i+1}. {l}\" for i, l in enumerate(BANKING77_LABELS))\n",
    "\n",
    "def normalize_intent(intent: str) -> str:\n",
    "    return intent.lower().replace(\"_\", \" \").strip()\n",
    "\n",
    "def score_prediction(predicted: str, expected: str) -> tuple:\n",
    "    is_correct = normalize_intent(predicted) == normalize_intent(expected)\n",
    "    return is_correct, 1.0 if is_correct else 0.0\n",
    "\n",
    "def normalize_chat_completion_url(url: str) -> str:\n",
    "    u = (url or \"\").rstrip(\"/\")\n",
    "    if not u:\n",
    "        return \"/chat/completions\"\n",
    "    parsed = urlparse(u)\n",
    "    path = parsed.path.rstrip(\"/\")\n",
    "    if path.endswith(\"/chat/completions\"):\n",
    "        return u\n",
    "    new_path = f\"{path}/chat/completions\"\n",
    "    return urlunparse((parsed.scheme, parsed.netloc, new_path, parsed.params, parsed.query, parsed.fragment))\n",
    "\n",
    "# Pydantic models\n",
    "class EnvConfig(BaseModel):\n",
    "    seed: int = 0\n",
    "    config: dict = {}\n",
    "\n",
    "class PolicyConfig(BaseModel):\n",
    "    config: dict = {}\n",
    "\n",
    "class RolloutRequest(BaseModel):\n",
    "    run_id: str = \"\"\n",
    "    env: EnvConfig = EnvConfig()\n",
    "    policy: PolicyConfig = PolicyConfig()\n",
    "    mode: str = \"rollout\"\n",
    "\n",
    "class RolloutMetrics(BaseModel):\n",
    "    episode_returns: list = []\n",
    "    mean_return: float = 0.0\n",
    "    num_steps: int = 1\n",
    "    num_episodes: int = 1\n",
    "    outcome_score: float = 0.0\n",
    "    events_score: float = 0.0\n",
    "    details: dict = {}\n",
    "\n",
    "class RolloutResponse(BaseModel):\n",
    "    run_id: str = \"\"\n",
    "    branches: dict = {}\n",
    "    metrics: RolloutMetrics = RolloutMetrics()\n",
    "    aborted: bool = False\n",
    "    trace_correlation_id: Optional[str] = None\n",
    "    trace: Optional[dict] = None\n",
    "    pipeline_metadata: dict = {}\n",
    "\n",
    "def create_banking77_task_app(system_prompt: str, env_api_key: str):\n",
    "    \"\"\"Factory to create a Banking77 task app with a specific system prompt.\"\"\"\n",
    "    _http_client_holder = {\"client\": None}\n",
    "    \n",
    "    @asynccontextmanager\n",
    "    async def lifespan(app: FastAPI):\n",
    "        _http_client_holder[\"client\"] = httpx.AsyncClient(timeout=120.0)\n",
    "        load_dataset_split(\"train\")\n",
    "        load_dataset_split(\"test\")\n",
    "        yield\n",
    "        await _http_client_holder[\"client\"].aclose()\n",
    "    \n",
    "    app = FastAPI(lifespan=lifespan)\n",
    "    app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n",
    "    \n",
    "    def check_auth(request: Request) -> bool:\n",
    "        if not env_api_key:\n",
    "            return True\n",
    "        auth = request.headers.get(\"Authorization\", \"\") or request.headers.get(\"X-API-Key\", \"\")\n",
    "        token = auth.replace(\"Bearer \", \"\").strip()\n",
    "        return token == env_api_key\n",
    "    \n",
    "    @app.get(\"/health\")\n",
    "    async def health(request: Request):\n",
    "        return {\"status\": \"healthy\", \"authorized\": check_auth(request)}\n",
    "    \n",
    "    @app.get(\"/health/rollout\")\n",
    "    async def health_rollout(request: Request):\n",
    "        return {\"ok\": True, \"authorized\": check_auth(request)}\n",
    "    \n",
    "    @app.post(\"/rollout\")\n",
    "    async def rollout(request: Request, body: RolloutRequest):\n",
    "        if not check_auth(request):\n",
    "            raise HTTPException(status_code=401, detail=\"Unauthorized\")\n",
    "\n",
    "        policy_config = body.policy.config or {}\n",
    "        split = (body.env.config or {}).get(\"split\", \"train\")\n",
    "        seed = body.env.seed or 0\n",
    "\n",
    "        # DEBUG: Log received policy config\n",
    "        print(f\"[ROLLOUT] seed={seed}, split={split}, mode={body.mode}\", flush=True)\n",
    "        print(f\"[ROLLOUT] policy_config keys: {list(policy_config.keys())}\", flush=True)\n",
    "\n",
    "        sample = get_sample(split, seed)\n",
    "        intents_list = format_intents()\n",
    "\n",
    "        user_msg = f\"Customer Query: {sample['text']}\\n\\nAvailable Intents:\\n{intents_list}\\n\\nClassify this query into one of the above banking intents using the tool call.\"\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_msg}]\n",
    "\n",
    "        inference_url_raw = policy_config.get(\"inference_url\", \"\")\n",
    "\n",
    "        # DEBUG: Log raw inference_url before normalization\n",
    "        print(f\"[ROLLOUT] RAW inference_url: {inference_url_raw}\", flush=True)\n",
    "\n",
    "        if not inference_url_raw:\n",
    "            print(f\"[ROLLOUT] ERROR: Missing inference_url!\", flush=True)\n",
    "            raise HTTPException(status_code=400, detail=\"Missing inference_url\")\n",
    "\n",
    "        inference_url = normalize_chat_completion_url(inference_url_raw)\n",
    "\n",
    "        # DEBUG: Log normalized inference_url\n",
    "        print(f\"[ROLLOUT] NORMALIZED inference_url: {inference_url}\", flush=True)\n",
    "\n",
    "        model = policy_config.get(\"model\", \"gpt-4.1-nano\")\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"tools\": [TOOL_SCHEMA],\n",
    "            \"tool_choice\": \"required\",\n",
    "            \"max_completion_tokens\": policy_config.get(\"max_completion_tokens\", 256)\n",
    "        }\n",
    "        if policy_config.get(\"temperature\", 0.0) != 0.0:\n",
    "            payload[\"temperature\"] = policy_config[\"temperature\"]\n",
    "\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        auth_header = request.headers.get(\"Authorization\") or request.headers.get(\"X-API-Key\")\n",
    "        if auth_header:\n",
    "            headers[\"X-API-Key\"] = auth_header.replace(\"Bearer \", \"\").strip()\n",
    "\n",
    "        try:\n",
    "            print(f\"[ROLLOUT] POST to: {inference_url}\", flush=True)\n",
    "            resp = await _http_client_holder[\"client\"].post(inference_url, json=payload, headers=headers)\n",
    "            print(f\"[ROLLOUT] Response status: {resp.status_code}\", flush=True)\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"[ROLLOUT] Error response: {resp.text[:300]}\", flush=True)\n",
    "                if resp.status_code == 307:\n",
    "                    print(f\"[ROLLOUT] 307 REDIRECT! Location: {resp.headers.get('location', 'N/A')}\", flush=True)\n",
    "                raise HTTPException(status_code=resp.status_code, detail=f\"LLM error: {resp.text[:500]}\")\n",
    "            response_json = resp.json()\n",
    "        except httpx.HTTPError as e:\n",
    "            print(f\"[ROLLOUT] HTTPError: {e}\", flush=True)\n",
    "            raise HTTPException(status_code=502, detail=f\"LLM call failed: {e}\")\n",
    "\n",
    "        predicted_intent = \"\"\n",
    "        choices = response_json.get(\"choices\", [])\n",
    "        if choices:\n",
    "            message = choices[0].get(\"message\", {})\n",
    "            tool_calls = message.get(\"tool_calls\", [])\n",
    "            if tool_calls:\n",
    "                for tc in tool_calls:\n",
    "                    if tc.get(\"function\", {}).get(\"name\") == TOOL_NAME:\n",
    "                        try:\n",
    "                            args = json.loads(tc[\"function\"].get(\"arguments\", \"{}\"))\n",
    "                            predicted_intent = args.get(\"intent\", \"\")\n",
    "                        except: pass\n",
    "            if not predicted_intent:\n",
    "                predicted_intent = message.get(\"content\", \"\").strip().split()[0] if message.get(\"content\") else \"\"\n",
    "        if not predicted_intent:\n",
    "            predicted_intent = \"__NO_PREDICTION__\"\n",
    "\n",
    "        expected_intent = sample[\"label\"]\n",
    "        is_correct, reward = score_prediction(predicted_intent, expected_intent)\n",
    "\n",
    "        trace_correlation_id = policy_config.get(\"trace_correlation_id\")\n",
    "        if not trace_correlation_id:\n",
    "            from urllib.parse import urlsplit, parse_qs\n",
    "            try:\n",
    "                parsed = urlsplit(policy_config.get(\"inference_url\", \"\"))\n",
    "                cid_vals = parse_qs(parsed.query or \"\").get(\"cid\", [])\n",
    "                if cid_vals: trace_correlation_id = cid_vals[0]\n",
    "            except: pass\n",
    "\n",
    "        llm_model = response_json.get(\"model\") if isinstance(response_json, dict) else None\n",
    "        trace = {\n",
    "            \"messages\": messages,\n",
    "            \"response\": response_json,\n",
    "            \"correlation_id\": trace_correlation_id,\n",
    "            \"model\": llm_model,\n",
    "            \"metadata\": {\"env\": \"banking77\", \"split\": sample[\"split\"], \"index\": sample[\"index\"], \"correct\": is_correct}\n",
    "        }\n",
    "        metrics = RolloutMetrics(\n",
    "            episode_returns=[reward], mean_return=reward, num_steps=1, num_episodes=1,\n",
    "            outcome_score=reward, events_score=reward, details={\"correct\": is_correct}\n",
    "        )\n",
    "\n",
    "        return RolloutResponse(\n",
    "            run_id=body.run_id, branches={}, metrics=metrics, aborted=False,\n",
    "            trace_correlation_id=trace_correlation_id, trace=trace,\n",
    "            pipeline_metadata={\"inference_url\": policy_config.get(\"inference_url\", \"\")}\n",
    "        )\n",
    "\n",
    "    return app\n",
    "\n",
    "print('Task app factory defined (with debug logging)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# Step 6: Cloudflare Tunnel Helpers (from SDK)\nimport nest_asyncio\nnest_asyncio.apply()\n\n# Import tunnel helpers from SDK - no need to define inline!\nfrom synth_ai.sdk.tunnels import (\n    rotate_tunnel,\n    open_managed_tunnel,\n    track_process,\n    verify_tunnel_dns_resolution,\n    kill_port,\n    wait_for_health_check,\n)\nfrom synth_ai.sdk.task import run_server_background\n\nprint('Tunnel helpers imported from SDK')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# Step 7: Start Baseline Task App with Cloudflare Tunnel\n\nBASELINE_SYSTEM_PROMPT = \"You are an expert banking assistant that classifies customer queries into banking intents. Given a customer message, respond with exactly one intent label from the provided list using the `banking77_classify` tool.\"\nUSER_PROMPT = \"Customer Query: {query}\\n\\nAvailable Intents:\\n{available_intents}\\n\\nClassify this query into one of the above banking intents using the tool call.\"\n\n# Create baseline task app\nbaseline_app = create_banking77_task_app(BASELINE_SYSTEM_PROMPT, ENVIRONMENT_API_KEY)\n\n# Kill port if in use, start server in background\nkill_port(TASK_APP_PORT)\nrun_server_background(baseline_app, TASK_APP_PORT)\n\n# Wait for local health check\nprint(f'Waiting for baseline task app on port {TASK_APP_PORT}...')\nawait wait_for_health_check(\"localhost\", TASK_APP_PORT, ENVIRONMENT_API_KEY, timeout=30.0)\nprint('Baseline task app ready!')\n\n# Get tunnel from backend using SDK function\nprint('\\nProvisioning Cloudflare tunnel for baseline...')\nbaseline_tunnel = await rotate_tunnel(API_KEY, TASK_APP_PORT, reason=\"baseline_notebook\")\nBASELINE_TUNNEL_HOSTNAME = baseline_tunnel['hostname']\nBASELINE_TASK_APP_URL = f'https://{BASELINE_TUNNEL_HOSTNAME}'\nprint(f'Baseline tunnel: {BASELINE_TUNNEL_HOSTNAME}')\n\n# Start cloudflared with tracking (auto-cleanup on exit)\ntrack_process(open_managed_tunnel(baseline_tunnel['tunnel_token']))\n\n# Verify tunnel is reachable\nprint('Waiting for tunnel DNS...')\nawait verify_tunnel_dns_resolution(BASELINE_TASK_APP_URL, name=\"baseline\", api_key=ENVIRONMENT_API_KEY)\n\nprint(f'\\nBaseline task app URL: {BASELINE_TASK_APP_URL}')"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating GEPA job (task_app_url=https://task-8001-12511.usesynth.ai)...\n",
      "Job ID: pl_2606908964e941c8\n",
      "Polling...\n",
      "    [0s] queued (best=None)\n",
      "[ROLLOUT] seed=0, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trace_correlation_id', 'api_base', 'base_url']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/baseline-0-517dac51?cid=trace_validation-0-b47fe941\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/baseline-0-517dac51/chat/completions?cid=trace_validation-0-b47fe941\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/baseline-0-517dac51/chat/completions?cid=trace_validation-0-b47fe941\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=4, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s4?cid=trace_pl-2606908964e941c8-i0000-t0000-s4\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s4\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s4\n",
      "[ROLLOUT] seed=27, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s27?cid=trace_pl-2606908964e941c8-i0000-t0000-s27\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s27\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s27\n",
      "[ROLLOUT] seed=1, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s1?cid=trace_pl-2606908964e941c8-i0000-t0000-s1\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s1\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s1\n",
      "[ROLLOUT] seed=28, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s28?cid=trace_pl-2606908964e941c8-i0000-t0000-s28\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s28\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s28\n",
      "[ROLLOUT] seed=16, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s16?cid=trace_pl-2606908964e941c8-i0000-t0000-s16\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s16\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s16\n",
      "    [3s] running (best=None)\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=20, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s20?cid=trace_pl-2606908964e941c8-i0000-t0001-s20\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s20\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s20\n",
      "[ROLLOUT] seed=1, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s1?cid=trace_pl-2606908964e941c8-i0000-t0001-s1\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s1\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s1\n",
      "[ROLLOUT] seed=8, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s8?cid=trace_pl-2606908964e941c8-i0000-t0001-s8\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s8\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s8\n",
      "[ROLLOUT] seed=6, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s6?cid=trace_pl-2606908964e941c8-i0000-t0001-s6\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s6\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s6\n",
      "[ROLLOUT] seed=28, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s28?cid=trace_pl-2606908964e941c8-i0000-t0001-s28\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s28\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s28\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=4, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s4?cid=trace_pl-2606908964e941c8-i0000-t0001-s4\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s4\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s4\n",
      "[ROLLOUT] seed=16, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s16?cid=trace_pl-2606908964e941c8-i0000-t0001-s16\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s16\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s16\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=27, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s27?cid=trace_pl-2606908964e941c8-i0000-t0001-s27\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s27\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s27\n",
      "[ROLLOUT] seed=0, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s0?cid=trace_pl-2606908964e941c8-i0000-t0001-s0\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s0\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s0\n",
      "[ROLLOUT] seed=13, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s13?cid=trace_pl-2606908964e941c8-i0000-t0001-s13\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s13\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s13\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=6, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s6?cid=trace_pl-2606908964e941c8-i0000-t0002-s6\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s6\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s6\n",
      "[ROLLOUT] seed=4, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s4?cid=trace_pl-2606908964e941c8-i0000-t0002-s4\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s4\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s4\n",
      "[ROLLOUT] seed=1, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s1?cid=trace_pl-2606908964e941c8-i0000-t0002-s1\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s1\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s1\n",
      "[ROLLOUT] seed=8, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s8?cid=trace_pl-2606908964e941c8-i0000-t0002-s8\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s8\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s8\n",
      "[ROLLOUT] seed=20, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s20?cid=trace_pl-2606908964e941c8-i0000-t0002-s20\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s20\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s20\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=28, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s28?cid=trace_pl-2606908964e941c8-i0000-t0002-s28\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s28\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s28\n",
      "[ROLLOUT] seed=16, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s16?cid=trace_pl-2606908964e941c8-i0000-t0002-s16\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s16\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s16\n",
      "[ROLLOUT] seed=27, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s27?cid=trace_pl-2606908964e941c8-i0000-t0002-s27\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s27\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s27\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=13, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s13?cid=trace_pl-2606908964e941c8-i0000-t0002-s13\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s13\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s13\n",
      "[ROLLOUT] seed=0, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s0?cid=trace_pl-2606908964e941c8-i0000-t0002-s0\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s0\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s0\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=16, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s16?cid=trace_pl-2606908964e941c8-i0000-t0003-s16\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s16\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s16\n",
      "[ROLLOUT] seed=1, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s1?cid=trace_pl-2606908964e941c8-i0000-t0003-s1\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s1\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s1\n",
      "[ROLLOUT] seed=20, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s20?cid=trace_pl-2606908964e941c8-i0000-t0003-s20\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s20\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s20\n",
      "[ROLLOUT] seed=28, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s28?cid=trace_pl-2606908964e941c8-i0000-t0003-s28\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s28\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s28\n",
      "[ROLLOUT] seed=4, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s4?cid=trace_pl-2606908964e941c8-i0000-t0003-s4\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s4\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s4\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=8, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s8?cid=trace_pl-2606908964e941c8-i0000-t0003-s8\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s8\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s8\n",
      "[ROLLOUT] seed=6, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s6?cid=trace_pl-2606908964e941c8-i0000-t0003-s6\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s6\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s6\n",
      "[ROLLOUT] seed=27, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s27?cid=trace_pl-2606908964e941c8-i0000-t0003-s27\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s27\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s27\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=0, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s0?cid=trace_pl-2606908964e941c8-i0000-t0003-s0\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s0\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s0\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=13, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s13?cid=trace_pl-2606908964e941c8-i0000-t0003-s13\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s13\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s13\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=20, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s20?cid=trace_pl-2606908964e941c8-i0000-t0004-s20\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s20\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s20\n",
      "[ROLLOUT] seed=4, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s4?cid=trace_pl-2606908964e941c8-i0000-t0004-s4\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s4\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s4\n",
      "[ROLLOUT] seed=8, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s8?cid=trace_pl-2606908964e941c8-i0000-t0004-s8\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s8\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s8\n",
      "[ROLLOUT] seed=27, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s27?cid=trace_pl-2606908964e941c8-i0000-t0004-s27\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s27\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s27\n",
      "[ROLLOUT] seed=28, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s28?cid=trace_pl-2606908964e941c8-i0000-t0004-s28\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s28\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s28\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=6, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s6?cid=trace_pl-2606908964e941c8-i0000-t0004-s6\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s6\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s6\n",
      "[ROLLOUT] seed=1, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s1?cid=trace_pl-2606908964e941c8-i0000-t0004-s1\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s1\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s1\n",
      "[ROLLOUT] Response status: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/fastapi/applications.py\", line 1134, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 125, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 111, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 391, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 290, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/7b/96hbyfld35zflr_plpgvrdxm0000gn/T/ipykernel_47005/2179503155.py\", line 223, in rollout\n",
      "    is_correct, reward = score_prediction(predicted_intent, expected_intent)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/7b/96hbyfld35zflr_plpgvrdxm0000gn/T/ipykernel_47005/2179503155.py\", line 66, in score_prediction\n",
      "    is_correct = normalize_intent(predicted) == normalize_intent(expected)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/7b/96hbyfld35zflr_plpgvrdxm0000gn/T/ipykernel_47005/2179503155.py\", line 63, in normalize_intent\n",
      "    return intent.lower().replace(\"_\", \" \").strip()\n",
      "           ^^^^^^^^^^^^\n",
      "AttributeError: 'list' object has no attribute 'lower'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ROLLOUT] seed=16, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s16?cid=trace_pl-2606908964e941c8-i0000-t0004-s16\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s16\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s16\n",
      "[ROLLOUT] seed=0, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s0?cid=trace_pl-2606908964e941c8-i0000-t0004-s0\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s0\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s0\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=13, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s13?cid=trace_pl-2606908964e941c8-i0000-t0004-s13\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s13\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s13\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=7, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s7?cid=trace_pl-2606908964e941c8-i0000-t0006-s7\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s7/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s7\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s7/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s7\n",
      "[ROLLOUT] seed=21, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s21?cid=trace_pl-2606908964e941c8-i0000-t0005-s21\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s21/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s21\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s21/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s21\n",
      "[ROLLOUT] seed=19, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s19?cid=trace_pl-2606908964e941c8-i0000-t0006-s19\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s19/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s19\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s19/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s19\n",
      "[ROLLOUT] seed=7, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s7?cid=trace_pl-2606908964e941c8-i0000-t0005-s7\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s7/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s7\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s7/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s7\n",
      "[ROLLOUT] seed=18, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s18?cid=trace_pl-2606908964e941c8-i0000-t0005-s18\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s18/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s18\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s18/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s18\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=21, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s21?cid=trace_pl-2606908964e941c8-i0000-t0006-s21\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s21/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s21\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s21/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s21\n",
      "[ROLLOUT] seed=18, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s18?cid=trace_pl-2606908964e941c8-i0000-t0006-s18\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s18/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s18\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s18/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s18\n",
      "[ROLLOUT] seed=5, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s5?cid=trace_pl-2606908964e941c8-i0000-t0005-s5\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s5/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s5\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s5/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s5\n",
      "[ROLLOUT] seed=19, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s19?cid=trace_pl-2606908964e941c8-i0000-t0005-s19\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s19/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s19\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s19/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s19\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=7, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s7?cid=trace_pl-2606908964e941c8-i0000-t0008-s7\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s7/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s7\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s7/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s7\n",
      "[ROLLOUT] seed=5, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s5?cid=trace_pl-2606908964e941c8-i0000-t0006-s5\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s5/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s5\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s5/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s5\n",
      "[ROLLOUT] seed=21, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s21?cid=trace_pl-2606908964e941c8-i0000-t0008-s21\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s21/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s21\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s21/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s21\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=5, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s5?cid=trace_pl-2606908964e941c8-i0000-t0008-s5\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s5/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s5\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s5/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s5\n",
      "[ROLLOUT] seed=19, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s19?cid=trace_pl-2606908964e941c8-i0000-t0008-s19\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s19/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s19\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s19/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s19\n",
      "[ROLLOUT] seed=18, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s18?cid=trace_pl-2606908964e941c8-i0000-t0008-s18\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s18/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s18\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s18/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s18\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=21, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s21?cid=trace_pl-2606908964e941c8-i0000-t0007-s21\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s21/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s21\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s21/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s21\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=7, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s7?cid=trace_pl-2606908964e941c8-i0000-t0007-s7\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s7/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s7\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s7/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s7\n",
      "[ROLLOUT] seed=5, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s5?cid=trace_pl-2606908964e941c8-i0000-t0007-s5\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s5/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s5\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s5/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s5\n",
      "[ROLLOUT] seed=19, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s19?cid=trace_pl-2606908964e941c8-i0000-t0007-s19\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s19/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s19\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s19/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s19\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=18, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s18?cid=trace_pl-2606908964e941c8-i0000-t0007-s18\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s18/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s18\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s18/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s18\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=28, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s28?cid=trace_pl-2606908964e941c8-i0000-t0009-s28\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s28\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s28\n",
      "[ROLLOUT] seed=16, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s16?cid=trace_pl-2606908964e941c8-i0000-t0009-s16\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s16\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s16\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=1, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s1?cid=trace_pl-2606908964e941c8-i0000-t0009-s1\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s1\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s1\n",
      "[ROLLOUT] seed=8, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s8?cid=trace_pl-2606908964e941c8-i0000-t0009-s8\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s8\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s8\n",
      "[ROLLOUT] seed=20, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s20?cid=trace_pl-2606908964e941c8-i0000-t0009-s20\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s20\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s20\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=4, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s4?cid=trace_pl-2606908964e941c8-i0000-t0009-s4\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s4\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s4\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=6, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s6?cid=trace_pl-2606908964e941c8-i0000-t0009-s6\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s6\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s6\n",
      "[ROLLOUT] seed=13, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s13?cid=trace_pl-2606908964e941c8-i0000-t0009-s13\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s13\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s13\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=27, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s27?cid=trace_pl-2606908964e941c8-i0000-t0009-s27\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s27\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s27\n",
      "[ROLLOUT] seed=0, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s0?cid=trace_pl-2606908964e941c8-i0000-t0009-s0\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s0\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s0\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=0, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s0?cid=trace_pl-2606908964e941c8-i0000-t0010-s0\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s0\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s0\n",
      "[ROLLOUT] seed=8, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s8?cid=trace_pl-2606908964e941c8-i0000-t0010-s8\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s8\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s8\n",
      "[ROLLOUT] seed=16, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s16?cid=trace_pl-2606908964e941c8-i0000-t0010-s16\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s16\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s16\n",
      "[ROLLOUT] seed=27, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s27?cid=trace_pl-2606908964e941c8-i0000-t0010-s27\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s27\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s27\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=28, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s28?cid=trace_pl-2606908964e941c8-i0000-t0010-s28\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s28\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s28\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=6, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s6?cid=trace_pl-2606908964e941c8-i0000-t0010-s6\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s6\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s6\n",
      "[ROLLOUT] seed=20, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s20?cid=trace_pl-2606908964e941c8-i0000-t0010-s20\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s20\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s20\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=1, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s1?cid=trace_pl-2606908964e941c8-i0000-t0010-s1\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s1\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s1\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=13, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s13?cid=trace_pl-2606908964e941c8-i0000-t0010-s13\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s13\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s13\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=4, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s4?cid=trace_pl-2606908964e941c8-i0000-t0010-s4\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s4\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s4\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=50, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s50?cid=trace_pl-2606908964e941c8-i0000-t0011-s50\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s50/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s50\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s50/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s50\n",
      "[ROLLOUT] seed=52, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s52?cid=trace_pl-2606908964e941c8-i0000-t0011-s52\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s52/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s52\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s52/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s52\n",
      "[ROLLOUT] seed=53, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s53?cid=trace_pl-2606908964e941c8-i0000-t0011-s53\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s53/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s53\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s53/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s53\n",
      "[ROLLOUT] seed=54, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s54?cid=trace_pl-2606908964e941c8-i0000-t0011-s54\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s54/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s54\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s54/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s54\n",
      "[ROLLOUT] seed=55, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s55?cid=trace_pl-2606908964e941c8-i0000-t0011-s55\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s55/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s55\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s55/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s55\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=51, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s51?cid=trace_pl-2606908964e941c8-i0000-t0011-s51\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s51/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s51\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s51/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s51\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=53, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s53?cid=trace_pl-2606908964e941c8-i0001-t0012-s53\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s53/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s53\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s53/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s53\n",
      "[ROLLOUT] seed=52, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s52?cid=trace_pl-2606908964e941c8-i0001-t0012-s52\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s52/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s52\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s52/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s52\n",
      "[ROLLOUT] seed=54, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s54?cid=trace_pl-2606908964e941c8-i0001-t0012-s54\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s54/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s54\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s54/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s54\n",
      "[ROLLOUT] seed=50, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s50?cid=trace_pl-2606908964e941c8-i0001-t0012-s50\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s50/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s50\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s50/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s50\n",
      "[ROLLOUT] seed=55, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s55?cid=trace_pl-2606908964e941c8-i0001-t0012-s55\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s55/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s55\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s55/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s55\n",
      "    [45s] running (best=None)\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=51, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s51?cid=trace_pl-2606908964e941c8-i0001-t0012-s51\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s51/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s51\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s51/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s51\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=55, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s55?cid=trace_pl-2606908964e941c8-i0001-t0013-s55\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s55/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s55\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s55/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s55\n",
      "[ROLLOUT] seed=50, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s50?cid=trace_pl-2606908964e941c8-i0001-t0013-s50\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s50/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s50\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s50/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s50\n",
      "[ROLLOUT] seed=54, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s54?cid=trace_pl-2606908964e941c8-i0001-t0013-s54\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s54/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s54\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s54/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s54\n",
      "[ROLLOUT] seed=53, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s53?cid=trace_pl-2606908964e941c8-i0001-t0013-s53\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s53/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s53\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s53/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s53\n",
      "[ROLLOUT] seed=52, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s52?cid=trace_pl-2606908964e941c8-i0001-t0013-s52\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s52/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s52\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s52/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s52\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=51, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s51?cid=trace_pl-2606908964e941c8-i0001-t0013-s51\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s51/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s51\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s51/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s51\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=53, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s53?cid=trace_pl-2606908964e941c8-i0001-t0014-s53\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s53/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s53\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s53/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s53\n",
      "[ROLLOUT] seed=54, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s54?cid=trace_pl-2606908964e941c8-i0001-t0014-s54\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s54/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s54\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s54/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s54\n",
      "[ROLLOUT] seed=52, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s52?cid=trace_pl-2606908964e941c8-i0001-t0014-s52\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s52/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s52\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s52/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s52\n",
      "[ROLLOUT] seed=55, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s55?cid=trace_pl-2606908964e941c8-i0001-t0014-s55\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s55/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s55\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s55/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s55\n",
      "[ROLLOUT] seed=51, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s51?cid=trace_pl-2606908964e941c8-i0001-t0014-s51\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s51/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s51\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s51/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s51\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=50, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s50?cid=trace_pl-2606908964e941c8-i0001-t0014-s50\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s50/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s50\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s50/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s50\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "    [59s] succeeded (best=0.8)\n",
      "\n",
      "FINAL: succeeded\n",
      "BEST SCORE: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Run GEPA Optimization (using tunnel URL)\n",
    "async def run_gepa():\n",
    "    config_body = {\n",
    "        'prompt_learning': {\n",
    "            'algorithm': 'gepa',\n",
    "            'run_local': False,  # Run on remote backend\n",
    "            'task_app_url': BASELINE_TASK_APP_URL,\n",
    "            'task_app_api_key': ENVIRONMENT_API_KEY,\n",
    "            'env_name': 'banking77',\n",
    "            'initial_prompt': {\n",
    "                'messages': [\n",
    "                    {'role': 'system', 'order': 0, 'pattern': BASELINE_SYSTEM_PROMPT},\n",
    "                    {'role': 'user', 'order': 1, 'pattern': USER_PROMPT},\n",
    "                ],\n",
    "                'wildcards': {'query': 'REQUIRED', 'available_intents': 'OPTIONAL'},\n",
    "            },\n",
    "            'policy': {'model': 'gpt-4.1-nano', 'provider': 'openai', 'temperature': 0.0, 'max_completion_tokens': 256},\n",
    "            'gepa': {\n",
    "                'env_name': 'banking77',\n",
    "                'evaluation': {'seeds': list(range(30)), 'validation_seeds': list(range(50, 56))},\n",
    "                'rollout': {'budget': 50, 'max_concurrent': 5, 'minibatch_size': 5},\n",
    "                'mutation': {'rate': 0.3, 'llm_model': 'gpt-4.1-nano'},\n",
    "                'population': {'initial_size': 3, 'num_generations': 2, 'children_per_generation': 2},\n",
    "                'archive': {'size': 5, 'pareto_set_size': 10},\n",
    "                'token': {'counting_model': 'gpt-4'},\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(f'Creating GEPA job (task_app_url={BASELINE_TASK_APP_URL})...')\n",
    "    async with httpx.AsyncClient(timeout=30) as client:\n",
    "        resp = await client.post(\n",
    "            f'{SYNTH_API_BASE}/api/prompt-learning/online/jobs',\n",
    "            json={'algorithm': 'gepa', 'config_body': config_body},\n",
    "            headers={'Authorization': f'Bearer {API_KEY}'}\n",
    "        )\n",
    "        if resp.status_code != 200:\n",
    "            print(f'ERROR: {resp.status_code} - {resp.text[:500]}')\n",
    "            resp.raise_for_status()\n",
    "        job_id = resp.json()['job_id']\n",
    "    print(f'Job ID: {job_id}')\n",
    "\n",
    "    print('Polling...')\n",
    "    start = time.time()\n",
    "    last_status = None\n",
    "    job = None\n",
    "    \n",
    "    while True:\n",
    "        async with httpx.AsyncClient(timeout=30) as client:\n",
    "            resp = await client.get(\n",
    "                f'{SYNTH_API_BASE}/api/prompt-learning/online/jobs/{job_id}',\n",
    "                headers={'Authorization': f'Bearer {API_KEY}'}\n",
    "            )\n",
    "            resp.raise_for_status()\n",
    "            job = resp.json()\n",
    "        \n",
    "        status = job['status']\n",
    "        elapsed = int(time.time() - start)\n",
    "        best = job.get('best_train_score') or job.get('best_score')\n",
    "        \n",
    "        if status != last_status or elapsed % 15 == 0:\n",
    "            print(f'    [{elapsed}s] {status} (best={best})')\n",
    "            last_status = status\n",
    "        \n",
    "        if status in ['succeeded', 'failed', 'cancelled']:\n",
    "            break\n",
    "        await asyncio.sleep(3)\n",
    "\n",
    "    print(f'\\nFINAL: {status}')\n",
    "    if status == 'succeeded':\n",
    "        best = job.get('best_score') or job.get('best_train_score')\n",
    "        print(f'BEST SCORE: {best}')\n",
    "    elif status == 'failed':\n",
    "        print(f'ERROR: {job.get(\"error\")}')\n",
    "    \n",
    "    return job\n",
    "\n",
    "job = await run_gepa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Step 9: Run Formal Eval Jobs (Baseline vs Optimized)\n\nEVAL_SEEDS = list(range(100, 120))  # 20 held-out test samples\n\nasync def run_eval_job(task_app_url: str, task_app_api_key: str, seeds: list, mode: str) -> dict:\n    \"\"\"Run an eval job and wait for completion.\"\"\"\n    async with httpx.AsyncClient(timeout=30) as client:\n        resp = await client.post(\n            f'{SYNTH_API_BASE}/api/eval/jobs',\n            json={\n                'task_app_url': task_app_url,\n                'task_app_api_key': task_app_api_key,\n                'env_name': 'banking77',\n                'seeds': seeds,\n                'env_config': {'split': 'test'},\n                'policy': {'model': 'gpt-4.1-nano', 'provider': 'openai'},\n                'mode': mode,\n                'max_concurrent': 10,\n            },\n            headers={'Authorization': f'Bearer {API_KEY}'}\n        )\n        if resp.status_code != 200:\n            print(f'ERROR creating {mode} eval job: {resp.status_code} - {resp.text[:300]}')\n            return {'status': 'failed', 'error': resp.text}\n        \n        job_id = resp.json()['job_id']\n        print(f'  {mode} eval job: {job_id}')\n    \n    start = time.time()\n    while True:\n        async with httpx.AsyncClient(timeout=30) as client:\n            resp = await client.get(\n                f'{SYNTH_API_BASE}/api/eval/jobs/{job_id}',\n                headers={'Authorization': f'Bearer {API_KEY}'}\n            )\n            if resp.status_code != 200:\n                print(f'  Error polling: {resp.status_code}')\n                await asyncio.sleep(2)\n                continue\n            job = resp.json()\n        \n        status = job.get('status', '')\n        elapsed = int(time.time() - start)\n        \n        if status in ['completed', 'failed']:\n            break\n        \n        if elapsed % 10 == 0:\n            results = job.get('results') or {}\n            completed = results.get('completed', 0)\n            total = results.get('total', len(seeds))\n            print(f'    [{elapsed}s] {status} ({completed}/{total})')\n        \n        await asyncio.sleep(2)\n    \n    async with httpx.AsyncClient(timeout=30) as client:\n        resp = await client.get(\n            f'{SYNTH_API_BASE}/api/eval/jobs/{job_id}/results',\n            headers={'Authorization': f'Bearer {API_KEY}'}\n        )\n        if resp.status_code != 200:\n            return {'status': 'failed', 'error': f'Failed to get results: {resp.status_code}'}\n        return resp.json()\n\nif job['status'] == 'succeeded':\n    print(\"GEPA Job Succeeded!\\n\")\n    \n    # Get job details with error handling\n    async with httpx.AsyncClient(timeout=30) as client:\n        resp = await client.get(\n            f'{SYNTH_API_BASE}/api/prompt-learning/online/jobs/{job[\"job_id\"]}',\n            headers={'Authorization': f'Bearer {API_KEY}'}\n        )\n        if resp.status_code != 200:\n            print(f'ERROR getting job details: {resp.status_code} - {resp.text[:200]}')\n            job_details = {}\n        else:\n            job_details = resp.json()\n    \n    best_snapshot = job_details.get('best_snapshot') or {}\n    \n    # Try to get optimized prompt from job_details or from the job itself\n    baseline_score_info = best_snapshot.get('baseline_score_info') or {}\n    best_score_info = best_snapshot.get('best_score_info') or {}\n    baseline_train = baseline_score_info.get('mean_score') or baseline_score_info.get('score')\n    optimized_train = best_score_info.get('mean_score') or best_score_info.get('score') or job.get('best_score')\n    \n    best_prompt = best_snapshot.get('best_prompt', {})\n    best_prompt_messages = best_prompt.get('messages', [])\n    \n    # Extract optimized system prompt\n    optimized_system = None\n    for msg in best_prompt_messages:\n        if msg.get('role') == 'system':\n            optimized_system = msg.get('pattern') or msg.get('content')\n            break\n    \n    # If we didn't get optimized_system from best_snapshot, use baseline (for eval comparison)\n    if not optimized_system:\n        print(\"NOTE: Could not extract optimized prompt from job details, using baseline for comparison\")\n        optimized_system = BASELINE_SYSTEM_PROMPT\n    \n    print('=' * 60)\n    print('BASELINE SYSTEM PROMPT')\n    print('=' * 60)\n    print(BASELINE_SYSTEM_PROMPT)\n    \n    print('\\n' + '=' * 60)\n    print('OPTIMIZED SYSTEM PROMPT (from GEPA)')\n    print('=' * 60)\n    print(optimized_system[:800] + \"...\" if len(optimized_system) > 800 else optimized_system)\n    \n    print('\\n' + '=' * 60)\n    print('GEPA TRAINING RESULTS')\n    print('=' * 60)\n    if baseline_train:\n        print(f\"Baseline Train:  {baseline_train:.1%}\")\n    if optimized_train:\n        print(f\"Optimized Train: {optimized_train:.1%}\")\n    if baseline_train and optimized_train:\n        print(f\"Training Lift:   {optimized_train - baseline_train:+.1%}\")\n    \n    print('\\n' + '=' * 60)\n    print(f'FORMAL EVAL JOBS (test split, seeds {EVAL_SEEDS[0]}-{EVAL_SEEDS[-1]})')\n    print('=' * 60)\n    \n    # Start optimized task app on different port using SDK helpers\n    print(f'\\nStarting optimized task app on port {OPTIMIZED_TASK_APP_PORT}...')\n    optimized_app = create_banking77_task_app(optimized_system, ENVIRONMENT_API_KEY)\n    \n    kill_port(OPTIMIZED_TASK_APP_PORT)\n    run_server_background(optimized_app, OPTIMIZED_TASK_APP_PORT)\n    await wait_for_health_check(\"localhost\", OPTIMIZED_TASK_APP_PORT, ENVIRONMENT_API_KEY, timeout=30.0)\n    print('Optimized task app ready!')\n    \n    # Get tunnel for optimized using SDK\n    print('Provisioning Cloudflare tunnel for optimized...')\n    optimized_tunnel = await rotate_tunnel(API_KEY, OPTIMIZED_TASK_APP_PORT, reason=\"optimized_notebook\")\n    OPTIMIZED_TUNNEL_HOSTNAME = optimized_tunnel['hostname']\n    OPTIMIZED_TASK_APP_URL = f'https://{OPTIMIZED_TUNNEL_HOSTNAME}'\n    print(f'Optimized tunnel: {OPTIMIZED_TUNNEL_HOSTNAME}')\n    \n    # Start cloudflared with tracking (auto-cleanup on exit)\n    track_process(open_managed_tunnel(optimized_tunnel['tunnel_token']))\n    await verify_tunnel_dns_resolution(OPTIMIZED_TASK_APP_URL, name=\"optimized\", api_key=ENVIRONMENT_API_KEY)\n    \n    # Run baseline eval\n    print('\\nRunning BASELINE eval job...')\n    baseline_results = await run_eval_job(\n        task_app_url=BASELINE_TASK_APP_URL,\n        task_app_api_key=ENVIRONMENT_API_KEY,\n        seeds=EVAL_SEEDS,\n        mode='baseline'\n    )\n    \n    baseline_summary = baseline_results.get('summary', {})\n    baseline_eval_score = baseline_summary.get('mean_score')\n    print(f'  Baseline eval: {baseline_eval_score:.1%}' if baseline_eval_score is not None else '  Baseline eval: N/A')\n    \n    # Run optimized eval\n    print('\\nRunning OPTIMIZED eval job...')\n    optimized_results = await run_eval_job(\n        task_app_url=OPTIMIZED_TASK_APP_URL,\n        task_app_api_key=ENVIRONMENT_API_KEY,\n        seeds=EVAL_SEEDS,\n        mode='optimized'\n    )\n    \n    optimized_summary = optimized_results.get('summary', {})\n    optimized_eval_score = optimized_summary.get('mean_score')\n    print(f'  Optimized eval: {optimized_eval_score:.1%}' if optimized_eval_score is not None else '  Optimized eval: N/A')\n    \n    # Final comparison\n    print('\\n' + '=' * 60)\n    print('FINAL COMPARISON')\n    print('=' * 60)\n    print(f\"Training:\")\n    if baseline_train:\n        print(f\"  Baseline:  {baseline_train:.1%}\")\n    if optimized_train:\n        print(f\"  Optimized: {optimized_train:.1%}\")\n    if baseline_train and optimized_train:\n        print(f\"  Lift:      {optimized_train - baseline_train:+.1%}\")\n    \n    print(f\"\\nEval (seeds {EVAL_SEEDS[0]}-{EVAL_SEEDS[-1]}, held-out):\")\n    if baseline_eval_score is not None:\n        print(f\"  Baseline:  {baseline_eval_score:.1%}\")\n    if optimized_eval_score is not None:\n        print(f\"  Optimized: {optimized_eval_score:.1%}\")\n    if baseline_eval_score is not None and optimized_eval_score is not None:\n        eval_lift = optimized_eval_score - baseline_eval_score\n        print(f\"  Lift:      {eval_lift:+.1%}\")\n        \n        if eval_lift > 0:\n            print(\"\\n>>> OPTIMIZATION GENERALIZES TO HELD-OUT DATA!\")\n        elif eval_lift == 0:\n            print(\"\\n=== Same performance on held-out data\")\n        else:\n            print(\"\\n<<< Baseline better on held-out (possible overfitting)\")\nelse:\n    print(f\"Job did not succeed: {job.get('status')}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# Step 10: Cleanup\nfrom synth_ai.sdk.tunnels import cleanup_all\n\nprint('Cleaning up cloudflared processes...')\ncleanup_all()\nprint('Demo complete!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e125b0c0-b463-43d5-9529-1aac88e7e1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}