{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Synth GEPA Demo - Banking77 (Production)\n\nSelf-contained notebook for prompt optimization using GEPA against the **production backend**.\n\n**Run in Google Colab:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/synth-laboratories/synth-ai/blob/main/demos/gepa_banking77/demo_prod.ipynb)\n\n**What this demo does:**\n1. Spins up a local Banking77 classification task app\n2. Creates a Cloudflare tunnel to expose it to the internet\n3. Runs GEPA prompt optimization on the Synth backend\n4. Compares baseline vs optimized prompts on held-out data"
  },
  {
   "cell_type": "code",
   "id": "35ku8p0wn7",
   "source": "# Step 0: Install dependencies (run this first on Colab)\nimport sys\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"Running in Google Colab - installing dependencies...\")\n    !pip install -q httpx pynacl fastapi uvicorn datasets nest_asyncio\n    \n    # Install cloudflared\n    !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O /usr/local/bin/cloudflared\n    !chmod +x /usr/local/bin/cloudflared\n    !cloudflared --version\n    \n    print(\"Dependencies installed!\")\nelse:\n    print(\"Not in Colab - assuming dependencies are already installed\")\n    print(\"Required: pip install httpx pynacl fastapi uvicorn datasets nest_asyncio\")\n    print(\"Required: brew install cloudflare/cloudflare/cloudflared (macOS)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Imports and Config\nimport os, sys, time, secrets, base64, asyncio, json, threading, subprocess, signal, atexit\nfrom typing import Any, Optional\nfrom contextlib import asynccontextmanager\n\nimport httpx\nimport uvicorn\nfrom nacl.public import PublicKey, SealedBox\nfrom fastapi import FastAPI, Request, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom datasets import load_dataset\n\n# Production backend\nSYNTH_API_BASE = 'https://api.usesynth.ai'\nTASK_APP_PORT = 8001\nOPTIMIZED_TASK_APP_PORT = 8002\n\n# Track cloudflared processes for cleanup\n_cloudflared_processes = []\n\ndef cleanup_cloudflared():\n    for proc in _cloudflared_processes:\n        try:\n            proc.terminate()\n            proc.wait(timeout=5)\n        except:\n            try:\n                proc.kill()\n            except:\n                pass\n\natexit.register(cleanup_cloudflared)\n\nprint(f'Backend: {SYNTH_API_BASE}')\nprint(f'Task App Ports: {TASK_APP_PORT}, {OPTIMIZED_TASK_APP_PORT}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Check Backend Health\nr = httpx.get(f'{SYNTH_API_BASE}/health', timeout=30)\nif r.status_code == 200:\n    print(f'Backend health: {r.json()}')\nelse:\n    print(f'WARNING: Backend returned status {r.status_code}')\n    print(f'Response: {r.text[:200]}...' if len(r.text) > 200 else f'Response: {r.text}')\n    raise RuntimeError(f'Backend not healthy: status {r.status_code}')"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No SYNTH_API_KEY found, minting demo key...\n",
      "Demo API Key: sk_demo_a3T-ldJZ0ZqDJ0TE7...\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Get API Key (use env var or mint demo key)\n",
    "API_KEY = os.environ.get('SYNTH_API_KEY', '')\n",
    "\n",
    "if not API_KEY:\n",
    "    print('No SYNTH_API_KEY found, minting demo key...')\n",
    "    resp = httpx.post(f'{SYNTH_API_BASE}/api/demo/keys', json={'ttl_hours': 4}, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    API_KEY = resp.json()['api_key']\n",
    "    print(f'Demo API Key: {API_KEY[:25]}...')\n",
    "else:\n",
    "    print(f'Using SYNTH_API_KEY: {API_KEY[:20]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minted env key: 115860176ce8...47c3\n",
      "Uploaded env key: {'id': '88c0a188-d511-4f23-a59b-9e1fa46ceda9', 'name': 'ENVIRONMENT_API_KEY', 'updated_at': '2025-12-27T05:15:01.143970Z', 'org_id': 'e77ef3a8-677d-4ddd-92d6-0f114d6bbdaf', 'upserted': True}\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Mint and Upload Environment Key\n",
    "ENVIRONMENT_API_KEY = secrets.token_hex(32)\n",
    "print(f'Minted env key: {ENVIRONMENT_API_KEY[:12]}...{ENVIRONMENT_API_KEY[-4:]}')\n",
    "\n",
    "pub_resp = httpx.get(f'{SYNTH_API_BASE}/api/v1/crypto/public-key', \n",
    "                     headers={'Authorization': f'Bearer {API_KEY}'}, timeout=30)\n",
    "pub_resp.raise_for_status()\n",
    "pubkey_b64 = pub_resp.json()['public_key']\n",
    "\n",
    "key_bytes = base64.b64decode(pubkey_b64, validate=True)\n",
    "box = SealedBox(PublicKey(key_bytes))\n",
    "ciphertext = box.encrypt(ENVIRONMENT_API_KEY.encode('utf-8'))\n",
    "ciphertext_b64 = base64.b64encode(ciphertext).decode('ascii')\n",
    "\n",
    "upload_resp = httpx.post(\n",
    "    f'{SYNTH_API_BASE}/api/v1/env-keys',\n",
    "    headers={'Authorization': f'Bearer {API_KEY}', 'Content-Type': 'application/json'},\n",
    "    json={'name': 'ENVIRONMENT_API_KEY', 'ciphertext_b64': ciphertext_b64},\n",
    "    timeout=30\n",
    ")\n",
    "upload_resp.raise_for_status()\n",
    "print(f'Uploaded env key: {upload_resp.json()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Define Banking77 Task App\nfrom urllib.parse import urlparse, urlunparse\n\nBANKING77_LABELS = [\n    \"activate_my_card\", \"age_limit\", \"apple_pay_or_google_pay\", \"atm_support\", \"automatic_top_up\",\n    \"balance_not_updated_after_bank_transfer\", \"balance_not_updated_after_cheque_or_cash_deposit\",\n    \"beneficiary_not_allowed\", \"cancel_transfer\", \"card_about_to_expire\", \"card_acceptance\",\n    \"card_arrival\", \"card_delivery_estimate\", \"card_linking\", \"card_not_working\",\n    \"card_payment_fee_charged\", \"card_payment_not_recognised\", \"card_payment_wrong_exchange_rate\",\n    \"card_swallowed\", \"cash_withdrawal_charge\", \"cash_withdrawal_not_recognised\", \"change_pin\",\n    \"compromised_card\", \"contactless_not_working\", \"country_support\", \"declined_card_payment\",\n    \"declined_cash_withdrawal\", \"declined_transfer\", \"direct_debit_payment_not_recognised\",\n    \"disposable_card_limits\", \"edit_personal_details\", \"exchange_charge\", \"exchange_rate\",\n    \"exchange_via_app\", \"extra_charge_on_statement\", \"failed_transfer\", \"fiat_currency_support\",\n    \"get_disposable_virtual_card\", \"get_physical_card\", \"getting_spare_card\", \"getting_virtual_card\",\n    \"lost_or_stolen_card\", \"lost_or_stolen_phone\", \"order_physical_card\", \"passcode_forgotten\",\n    \"pending_card_payment\", \"pending_cash_withdrawal\", \"pending_top_up\", \"pending_transfer\",\n    \"pin_blocked\", \"receiving_money\", \"Refund_not_showing_up\", \"request_refund\",\n    \"reverted_card_payment?\", \"supported_cards_and_currencies\", \"terminate_account\",\n    \"top_up_by_bank_transfer_charge\", \"top_up_by_card_charge\", \"top_up_by_cash_or_cheque\",\n    \"top_up_failed\", \"top_up_limits\", \"top_up_reverted\", \"topping_up_by_card\",\n    \"transaction_charged_twice\", \"transfer_fee_charged\", \"transfer_into_account\",\n    \"transfer_not_received_by_recipient\", \"transfer_timing\", \"unable_to_verify_identity\",\n    \"verify_my_identity\", \"verify_source_of_funds\", \"verify_top_up\", \"virtual_card_not_working\",\n    \"visa_or_mastercard\", \"why_verify_identity\", \"wrong_amount_of_cash_received\",\n    \"wrong_exchange_rate_for_cash_withdrawal\",\n]\nTOOL_NAME = \"banking77_classify\"\nTOOL_SCHEMA = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": TOOL_NAME,\n        \"description\": \"Return the predicted banking77 intent label.\",\n        \"parameters\": {\"type\": \"object\", \"properties\": {\"intent\": {\"type\": \"string\"}}, \"required\": [\"intent\"]},\n    },\n}\n\n# Dataset cache\n_dataset_cache = {}\n_label_names = None\n\ndef load_dataset_split(split: str):\n    global _label_names\n    if split not in _dataset_cache:\n        ds = load_dataset(\"banking77\", split=split, trust_remote_code=False)\n        _dataset_cache[split] = ds\n        if _label_names is None and hasattr(ds.features.get(\"label\"), \"names\"):\n            _label_names = ds.features[\"label\"].names\n    return _dataset_cache[split]\n\ndef get_sample(split: str, index: int) -> dict:\n    ds = load_dataset_split(split)\n    idx = index % len(ds)\n    row = ds[idx]\n    label_idx = int(row.get(\"label\", 0))\n    label_text = _label_names[label_idx] if _label_names and label_idx < len(_label_names) else f\"label_{label_idx}\"\n    return {\"index\": idx, \"split\": split, \"text\": str(row.get(\"text\", \"\")), \"label\": label_text}\n\ndef format_intents() -> str:\n    return \"\\n\".join(f\"{i+1}. {l}\" for i, l in enumerate(BANKING77_LABELS))\n\ndef normalize_intent(intent: str) -> str:\n    return intent.lower().replace(\"_\", \" \").strip()\n\ndef score_prediction(predicted: str, expected: str) -> tuple:\n    is_correct = normalize_intent(predicted) == normalize_intent(expected)\n    return is_correct, 1.0 if is_correct else 0.0\n\ndef normalize_chat_completion_url(url: str) -> str:\n    u = (url or \"\").rstrip(\"/\")\n    if not u:\n        return \"/chat/completions\"\n    parsed = urlparse(u)\n    path = parsed.path.rstrip(\"/\")\n    if path.endswith(\"/chat/completions\"):\n        return u\n    new_path = f\"{path}/chat/completions\"\n    return urlunparse((parsed.scheme, parsed.netloc, new_path, parsed.params, parsed.query, parsed.fragment))\n\n# Pydantic models\nclass EnvConfig(BaseModel):\n    seed: int = 0\n    config: dict = {}\n\nclass PolicyConfig(BaseModel):\n    config: dict = {}\n\nclass RolloutRequest(BaseModel):\n    run_id: str = \"\"\n    env: EnvConfig = EnvConfig()\n    policy: PolicyConfig = PolicyConfig()\n    mode: str = \"rollout\"\n\nclass RolloutMetrics(BaseModel):\n    episode_returns: list = []\n    mean_return: float = 0.0\n    num_steps: int = 1\n    num_episodes: int = 1\n    outcome_score: float = 0.0\n    events_score: float = 0.0\n    details: dict = {}\n\nclass RolloutResponse(BaseModel):\n    run_id: str = \"\"\n    branches: dict = {}\n    metrics: RolloutMetrics = RolloutMetrics()\n    aborted: bool = False\n    trace_correlation_id: Optional[str] = None\n    trace: Optional[dict] = None\n    pipeline_metadata: dict = {}\n\ndef create_banking77_task_app(system_prompt: str, env_api_key: str):\n    \"\"\"Create a Banking77 task app with a specific system prompt.\"\"\"\n    _http_client_holder = {\"client\": None}\n    \n    @asynccontextmanager\n    async def lifespan(app: FastAPI):\n        _http_client_holder[\"client\"] = httpx.AsyncClient(timeout=120.0)\n        load_dataset_split(\"train\")\n        load_dataset_split(\"test\")\n        yield\n        await _http_client_holder[\"client\"].aclose()\n    \n    app = FastAPI(lifespan=lifespan)\n    app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n    \n    def check_auth(request: Request) -> bool:\n        if not env_api_key:\n            return True\n        auth = request.headers.get(\"Authorization\", \"\") or request.headers.get(\"X-API-Key\", \"\")\n        token = auth.replace(\"Bearer \", \"\").strip()\n        return token == env_api_key\n    \n    @app.get(\"/health\")\n    async def health(request: Request):\n        return {\"status\": \"healthy\", \"authorized\": check_auth(request)}\n    \n    @app.post(\"/rollout\")\n    async def rollout(request: Request, body: RolloutRequest):\n        if not check_auth(request):\n            raise HTTPException(status_code=401, detail=\"Unauthorized\")\n\n        policy_config = body.policy.config or {}\n        split = (body.env.config or {}).get(\"split\", \"train\")\n        seed = body.env.seed or 0\n\n        # DEBUG: Log received policy config\n        print(f\"[ROLLOUT] seed={seed}, split={split}, mode={body.mode}\", flush=True)\n        print(f\"[ROLLOUT] policy_config keys: {list(policy_config.keys())}\", flush=True)\n\n        sample = get_sample(split, seed)\n        intents_list = format_intents()\n\n        user_msg = f\"Customer Query: {sample['text']}\\n\\nAvailable Intents:\\n{intents_list}\\n\\nClassify this query into one of the above banking intents using the tool call.\"\n        messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_msg}]\n\n        inference_url_raw = policy_config.get(\"inference_url\", \"\")\n\n        # DEBUG: Log raw inference_url before normalization\n        print(f\"[ROLLOUT] RAW inference_url: {inference_url_raw}\", flush=True)\n\n        if not inference_url_raw:\n            print(f\"[ROLLOUT] ERROR: Missing inference_url!\", flush=True)\n            raise HTTPException(status_code=400, detail=\"Missing inference_url\")\n\n        inference_url = normalize_chat_completion_url(inference_url_raw)\n\n        # DEBUG: Log normalized inference_url\n        print(f\"[ROLLOUT] NORMALIZED inference_url: {inference_url}\", flush=True)\n\n        model = policy_config.get(\"model\", \"gpt-4.1-nano\")\n        payload = {\n            \"model\": model,\n            \"messages\": messages,\n            \"tools\": [TOOL_SCHEMA],\n            \"tool_choice\": \"required\",\n            \"max_completion_tokens\": policy_config.get(\"max_completion_tokens\", 256)\n        }\n        if policy_config.get(\"temperature\", 0.0) != 0.0:\n            payload[\"temperature\"] = policy_config[\"temperature\"]\n\n        headers = {\"Content-Type\": \"application/json\"}\n        auth_header = request.headers.get(\"Authorization\") or request.headers.get(\"X-API-Key\")\n        if auth_header:\n            headers[\"X-API-Key\"] = auth_header.replace(\"Bearer \", \"\").strip()\n\n        try:\n            print(f\"[ROLLOUT] POST to: {inference_url}\", flush=True)\n            resp = await _http_client_holder[\"client\"].post(inference_url, json=payload, headers=headers)\n            print(f\"[ROLLOUT] Response status: {resp.status_code}\", flush=True)\n            if resp.status_code != 200:\n                print(f\"[ROLLOUT] Error response: {resp.text[:300]}\", flush=True)\n                if resp.status_code == 307:\n                    print(f\"[ROLLOUT] 307 REDIRECT! Location: {resp.headers.get('location', 'N/A')}\", flush=True)\n                raise HTTPException(status_code=resp.status_code, detail=f\"LLM error: {resp.text[:500]}\")\n            response_json = resp.json()\n        except httpx.HTTPError as e:\n            print(f\"[ROLLOUT] HTTPError: {e}\", flush=True)\n            raise HTTPException(status_code=502, detail=f\"LLM call failed: {e}\")\n\n        predicted_intent = \"\"\n        choices = response_json.get(\"choices\", [])\n        if choices:\n            message = choices[0].get(\"message\", {})\n            tool_calls = message.get(\"tool_calls\", [])\n            if tool_calls:\n                for tc in tool_calls:\n                    if tc.get(\"function\", {}).get(\"name\") == TOOL_NAME:\n                        try:\n                            args = json.loads(tc[\"function\"].get(\"arguments\", \"{}\"))\n                            predicted_intent = args.get(\"intent\", \"\")\n                        except: pass\n            if not predicted_intent:\n                predicted_intent = message.get(\"content\", \"\").strip().split()[0] if message.get(\"content\") else \"\"\n        if not predicted_intent:\n            predicted_intent = \"__NO_PREDICTION__\"\n\n        expected_intent = sample[\"label\"]\n        is_correct, reward = score_prediction(predicted_intent, expected_intent)\n\n        trace_correlation_id = policy_config.get(\"trace_correlation_id\")\n        if not trace_correlation_id:\n            from urllib.parse import urlsplit, parse_qs\n            try:\n                parsed = urlsplit(policy_config.get(\"inference_url\", \"\"))\n                cid_vals = parse_qs(parsed.query or \"\").get(\"cid\", [])\n                if cid_vals: trace_correlation_id = cid_vals[0]\n            except: pass\n\n        llm_model = response_json.get(\"model\") if isinstance(response_json, dict) else None\n        trace = {\n            \"messages\": messages,\n            \"response\": response_json,\n            \"correlation_id\": trace_correlation_id,\n            \"model\": llm_model,\n            \"metadata\": {\"env\": \"banking77\", \"split\": sample[\"split\"], \"index\": sample[\"index\"], \"correct\": is_correct}\n        }\n        metrics = RolloutMetrics(\n            episode_returns=[reward], mean_return=reward, num_steps=1, num_episodes=1,\n            outcome_score=reward, events_score=reward, details={\"correct\": is_correct}\n        )\n\n        return RolloutResponse(\n            run_id=body.run_id, branches={}, metrics=metrics, aborted=False,\n            trace_correlation_id=trace_correlation_id, trace=trace,\n            pipeline_metadata={\"inference_url\": policy_config.get(\"inference_url\", \"\")}\n        )\n\n    return app\n\nprint('Task app defined (with debug logging)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# Step 6: Cloudflare Tunnel Helpers\nimport nest_asyncio\nimport socket\nimport platform\nnest_asyncio.apply()\n\n# Cloudflare proxy IPs for usesynth.ai (bypasses local DNS caching)\nCLOUDFLARE_IPS = ['104.21.12.57', '172.67.193.178']\n\ndef kill_port(port):\n    \"\"\"Kill any process using the given port (works on macOS and Linux).\"\"\"\n    try:\n        if platform.system() == 'Darwin':  # macOS\n            result = subprocess.run(['lsof', '-ti', f':{port}'], capture_output=True, text=True)\n            pids = result.stdout.strip().split('\\n')\n        else:  # Linux (Colab)\n            result = subprocess.run(['fuser', f'{port}/tcp'], capture_output=True, text=True)\n            pids = result.stdout.strip().split()\n        \n        for pid in pids:\n            if pid:\n                print(f'Killing process {pid} on port {port}')\n                try:\n                    os.kill(int(pid), signal.SIGKILL)\n                except (ProcessLookupError, ValueError):\n                    pass\n        time.sleep(1)\n    except Exception as e:\n        print(f'Note: Could not check port {port}: {e}')\n\nasync def rotate_tunnel(port: int, reason: str = \"notebook\") -> dict:\n    \"\"\"Get a fresh Cloudflare tunnel from the backend.\"\"\"\n    async with httpx.AsyncClient(timeout=60) as client:\n        resp = await client.post(\n            f'{SYNTH_API_BASE}/api/v1/tunnels/rotate',\n            json={'local_port': port, 'local_host': 'localhost', 'reason': reason},\n            headers={'Authorization': f'Bearer {API_KEY}'}\n        )\n        if resp.status_code != 201:\n            raise RuntimeError(f'Failed to rotate tunnel: {resp.status_code} - {resp.text[:300]}')\n        return resp.json()\n\ndef start_cloudflared(tunnel_token: str, name: str) -> subprocess.Popen:\n    \"\"\"Start cloudflared tunnel connector in the background with clean environment.\"\"\"\n    env = os.environ.copy()\n    env['HOME'] = '/tmp'\n    \n    log_file = f'/tmp/cloudflared_{name}.log'\n    proc = subprocess.Popen(\n        ['cloudflared', 'tunnel', 'run', '--token', tunnel_token],\n        stdout=open(log_file, 'w'),\n        stderr=subprocess.STDOUT,\n        env=env,\n    )\n    _cloudflared_processes.append(proc)\n    print(f'Started cloudflared for {name} (pid={proc.pid}, log={log_file})')\n    return proc\n\ndef start_task_app(app, port: int, name: str):\n    \"\"\"Start a task app in a background thread.\"\"\"\n    kill_port(port)\n    \n    def run_server():\n        config = uvicorn.Config(app, host=\"0.0.0.0\", port=port, log_level=\"warning\")\n        server = uvicorn.Server(config)\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        loop.run_until_complete(server.serve())\n    \n    thread = threading.Thread(target=run_server, daemon=True, name=name)\n    thread.start()\n    \n    print(f'Waiting for {name} on port {port}...')\n    for i in range(30):\n        time.sleep(1)\n        try:\n            health = httpx.get(\n                f'http://localhost:{port}/health',\n                headers={'Authorization': f'Bearer {ENVIRONMENT_API_KEY}'},\n                timeout=5\n            ).json()\n            if health.get('status') == 'healthy':\n                print(f'{name} ready: {health}')\n                return thread\n        except: pass\n    raise RuntimeError(f'{name} failed to start on port {port}')\n\nasync def wait_for_tunnel_dns(hostname: str, timeout_sec: int = 120):\n    \"\"\"Wait for tunnel to be reachable via HTTPS (bypasses local DNS cache).\"\"\"\n    print(f'Waiting for tunnel: {hostname} (timeout={timeout_sec}s)')\n    start = time.time()\n    attempt = 0\n    last_error = None\n    \n    while time.time() - start < timeout_sec:\n        attempt += 1\n        # Try each Cloudflare IP to bypass local DNS caching\n        for cf_ip in CLOUDFLARE_IPS:\n            try:\n                # Use curl with --resolve to bypass DNS\n                result = subprocess.run([\n                    'curl', '-s', '-m', '10', \n                    '--resolve', f'{hostname}:443:{cf_ip}',\n                    f'https://{hostname}/health',\n                    '-H', f'Authorization: Bearer {ENVIRONMENT_API_KEY}'\n                ], capture_output=True, text=True)\n                \n                if result.returncode == 0 and result.stdout:\n                    try:\n                        health = json.loads(result.stdout)\n                        if health.get('status') == 'healthy':\n                            print(f'Tunnel reachable: {hostname} (attempt={attempt})')\n                            return True\n                    except:\n                        pass\n                last_error = f'curl returned {result.returncode}'\n            except Exception as e:\n                last_error = str(type(e).__name__)\n        \n        if attempt % 5 == 0:\n            elapsed = int(time.time() - start)\n            print(f'  [{elapsed}s] attempt {attempt}: {last_error}')\n        \n        await asyncio.sleep(3)\n    \n    raise RuntimeError(f'Tunnel timeout after {timeout_sec}s: {hostname} (last: {last_error})')\n\nprint('Tunnel helpers defined')"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for baseline_task_app on port 8001...\n",
      "baseline_task_app ready: {'status': 'healthy', 'authorized': True}\n",
      "\n",
      "Provisioning Cloudflare tunnel for baseline...\n",
      "Baseline tunnel: task-8001-12511.usesynth.ai\n",
      "Started cloudflared for baseline (pid=47045, log=/tmp/cloudflared_baseline.log)\n",
      "Waiting for cloudflared registration...\n",
      "  Cloudflared registered all 4 connections\n",
      "Waiting for tunnel: task-8001-12511.usesynth.ai (timeout=120s)\n",
      "  [14s] attempt 5: curl returned 0\n",
      "Tunnel reachable: task-8001-12511.usesynth.ai (attempt=7)\n",
      "\n",
      "Baseline task app URL: https://task-8001-12511.usesynth.ai\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Start Baseline Task App with Cloudflare Tunnel\n",
    "\n",
    "BASELINE_SYSTEM_PROMPT = \"You are an expert banking assistant that classifies customer queries into banking intents. Given a customer message, respond with exactly one intent label from the provided list using the `banking77_classify` tool.\"\n",
    "USER_PROMPT = \"Customer Query: {query}\\n\\nAvailable Intents:\\n{available_intents}\\n\\nClassify this query into one of the above banking intents using the tool call.\"\n",
    "\n",
    "# Create and start baseline task app\n",
    "baseline_app = create_banking77_task_app(BASELINE_SYSTEM_PROMPT, ENVIRONMENT_API_KEY)\n",
    "baseline_thread = start_task_app(baseline_app, TASK_APP_PORT, \"baseline_task_app\")\n",
    "\n",
    "# Get tunnel for baseline\n",
    "print('\\nProvisioning Cloudflare tunnel for baseline...')\n",
    "baseline_tunnel = await rotate_tunnel(TASK_APP_PORT, \"baseline_notebook\")\n",
    "BASELINE_TUNNEL_HOSTNAME = baseline_tunnel['hostname']\n",
    "BASELINE_TASK_APP_URL = f'https://{BASELINE_TUNNEL_HOSTNAME}'\n",
    "print(f'Baseline tunnel: {BASELINE_TUNNEL_HOSTNAME}')\n",
    "\n",
    "# Start cloudflared with clean environment\n",
    "baseline_cf_proc = start_cloudflared(baseline_tunnel['tunnel_token'], 'baseline')\n",
    "\n",
    "# Wait for cloudflared to register (check log)\n",
    "print('Waiting for cloudflared registration...')\n",
    "for i in range(20):\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        with open('/tmp/cloudflared_baseline.log') as f:\n",
    "            log = f.read()\n",
    "        if log.count('Registered tunnel connection') >= 4:\n",
    "            print(f'  Cloudflared registered all 4 connections')\n",
    "            break\n",
    "        elif i % 5 == 4:\n",
    "            print(f'  [{(i+1)*2}s] {log.count(\"Registered tunnel connection\")}/4 connections...')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Wait for tunnel to be reachable\n",
    "await wait_for_tunnel_dns(BASELINE_TUNNEL_HOSTNAME)\n",
    "\n",
    "print(f'\\nBaseline task app URL: {BASELINE_TASK_APP_URL}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating GEPA job (task_app_url=https://task-8001-12511.usesynth.ai)...\n",
      "Job ID: pl_2606908964e941c8\n",
      "Polling...\n",
      "    [0s] queued (best=None)\n",
      "[ROLLOUT] seed=0, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trace_correlation_id', 'api_base', 'base_url']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/baseline-0-517dac51?cid=trace_validation-0-b47fe941\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/baseline-0-517dac51/chat/completions?cid=trace_validation-0-b47fe941\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/baseline-0-517dac51/chat/completions?cid=trace_validation-0-b47fe941\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=4, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s4?cid=trace_pl-2606908964e941c8-i0000-t0000-s4\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s4\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s4\n",
      "[ROLLOUT] seed=27, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s27?cid=trace_pl-2606908964e941c8-i0000-t0000-s27\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s27\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s27\n",
      "[ROLLOUT] seed=1, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s1?cid=trace_pl-2606908964e941c8-i0000-t0000-s1\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s1\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s1\n",
      "[ROLLOUT] seed=28, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s28?cid=trace_pl-2606908964e941c8-i0000-t0000-s28\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s28\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s28\n",
      "[ROLLOUT] seed=16, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s16?cid=trace_pl-2606908964e941c8-i0000-t0000-s16\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s16\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0000-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0000-s16\n",
      "    [3s] running (best=None)\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=20, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s20?cid=trace_pl-2606908964e941c8-i0000-t0001-s20\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s20\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s20\n",
      "[ROLLOUT] seed=1, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s1?cid=trace_pl-2606908964e941c8-i0000-t0001-s1\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s1\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s1\n",
      "[ROLLOUT] seed=8, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s8?cid=trace_pl-2606908964e941c8-i0000-t0001-s8\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s8\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s8\n",
      "[ROLLOUT] seed=6, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s6?cid=trace_pl-2606908964e941c8-i0000-t0001-s6\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s6\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s6\n",
      "[ROLLOUT] seed=28, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s28?cid=trace_pl-2606908964e941c8-i0000-t0001-s28\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s28\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s28\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=4, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s4?cid=trace_pl-2606908964e941c8-i0000-t0001-s4\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s4\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s4\n",
      "[ROLLOUT] seed=16, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s16?cid=trace_pl-2606908964e941c8-i0000-t0001-s16\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s16\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s16\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=27, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s27?cid=trace_pl-2606908964e941c8-i0000-t0001-s27\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s27\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s27\n",
      "[ROLLOUT] seed=0, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s0?cid=trace_pl-2606908964e941c8-i0000-t0001-s0\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s0\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s0\n",
      "[ROLLOUT] seed=13, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s13?cid=trace_pl-2606908964e941c8-i0000-t0001-s13\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s13\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0001-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0001-s13\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=6, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s6?cid=trace_pl-2606908964e941c8-i0000-t0002-s6\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s6\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s6\n",
      "[ROLLOUT] seed=4, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s4?cid=trace_pl-2606908964e941c8-i0000-t0002-s4\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s4\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s4\n",
      "[ROLLOUT] seed=1, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s1?cid=trace_pl-2606908964e941c8-i0000-t0002-s1\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s1\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s1\n",
      "[ROLLOUT] seed=8, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s8?cid=trace_pl-2606908964e941c8-i0000-t0002-s8\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s8\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s8\n",
      "[ROLLOUT] seed=20, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s20?cid=trace_pl-2606908964e941c8-i0000-t0002-s20\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s20\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s20\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=28, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s28?cid=trace_pl-2606908964e941c8-i0000-t0002-s28\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s28\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s28\n",
      "[ROLLOUT] seed=16, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s16?cid=trace_pl-2606908964e941c8-i0000-t0002-s16\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s16\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s16\n",
      "[ROLLOUT] seed=27, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s27?cid=trace_pl-2606908964e941c8-i0000-t0002-s27\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s27\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s27\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=13, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s13?cid=trace_pl-2606908964e941c8-i0000-t0002-s13\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s13\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s13\n",
      "[ROLLOUT] seed=0, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s0?cid=trace_pl-2606908964e941c8-i0000-t0002-s0\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s0\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0002-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0002-s0\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=16, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s16?cid=trace_pl-2606908964e941c8-i0000-t0003-s16\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s16\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s16\n",
      "[ROLLOUT] seed=1, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s1?cid=trace_pl-2606908964e941c8-i0000-t0003-s1\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s1\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s1\n",
      "[ROLLOUT] seed=20, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s20?cid=trace_pl-2606908964e941c8-i0000-t0003-s20\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s20\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s20\n",
      "[ROLLOUT] seed=28, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s28?cid=trace_pl-2606908964e941c8-i0000-t0003-s28\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s28\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s28\n",
      "[ROLLOUT] seed=4, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s4?cid=trace_pl-2606908964e941c8-i0000-t0003-s4\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s4\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s4\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=8, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s8?cid=trace_pl-2606908964e941c8-i0000-t0003-s8\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s8\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s8\n",
      "[ROLLOUT] seed=6, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s6?cid=trace_pl-2606908964e941c8-i0000-t0003-s6\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s6\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s6\n",
      "[ROLLOUT] seed=27, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s27?cid=trace_pl-2606908964e941c8-i0000-t0003-s27\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s27\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s27\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=0, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s0?cid=trace_pl-2606908964e941c8-i0000-t0003-s0\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s0\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s0\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=13, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s13?cid=trace_pl-2606908964e941c8-i0000-t0003-s13\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s13\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0003-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0003-s13\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=20, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s20?cid=trace_pl-2606908964e941c8-i0000-t0004-s20\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s20\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s20\n",
      "[ROLLOUT] seed=4, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s4?cid=trace_pl-2606908964e941c8-i0000-t0004-s4\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s4\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s4\n",
      "[ROLLOUT] seed=8, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s8?cid=trace_pl-2606908964e941c8-i0000-t0004-s8\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s8\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s8\n",
      "[ROLLOUT] seed=27, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s27?cid=trace_pl-2606908964e941c8-i0000-t0004-s27\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s27\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s27\n",
      "[ROLLOUT] seed=28, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s28?cid=trace_pl-2606908964e941c8-i0000-t0004-s28\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s28\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s28\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=6, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s6?cid=trace_pl-2606908964e941c8-i0000-t0004-s6\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s6\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s6\n",
      "[ROLLOUT] seed=1, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s1?cid=trace_pl-2606908964e941c8-i0000-t0004-s1\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s1\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s1\n",
      "[ROLLOUT] Response status: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/fastapi/applications.py\", line 1134, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 125, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 111, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 391, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/joshpurtell/Documents/GitHub/monorepo/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 290, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/7b/96hbyfld35zflr_plpgvrdxm0000gn/T/ipykernel_47005/2179503155.py\", line 223, in rollout\n",
      "    is_correct, reward = score_prediction(predicted_intent, expected_intent)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/7b/96hbyfld35zflr_plpgvrdxm0000gn/T/ipykernel_47005/2179503155.py\", line 66, in score_prediction\n",
      "    is_correct = normalize_intent(predicted) == normalize_intent(expected)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/7b/96hbyfld35zflr_plpgvrdxm0000gn/T/ipykernel_47005/2179503155.py\", line 63, in normalize_intent\n",
      "    return intent.lower().replace(\"_\", \" \").strip()\n",
      "           ^^^^^^^^^^^^\n",
      "AttributeError: 'list' object has no attribute 'lower'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ROLLOUT] seed=16, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s16?cid=trace_pl-2606908964e941c8-i0000-t0004-s16\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s16\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s16\n",
      "[ROLLOUT] seed=0, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s0?cid=trace_pl-2606908964e941c8-i0000-t0004-s0\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s0\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s0\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=13, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id', 'version_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s13?cid=trace_pl-2606908964e941c8-i0000-t0004-s13\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s13\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0004-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0004-s13\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=7, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s7?cid=trace_pl-2606908964e941c8-i0000-t0006-s7\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s7/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s7\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s7/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s7\n",
      "[ROLLOUT] seed=21, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s21?cid=trace_pl-2606908964e941c8-i0000-t0005-s21\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s21/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s21\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s21/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s21\n",
      "[ROLLOUT] seed=19, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s19?cid=trace_pl-2606908964e941c8-i0000-t0006-s19\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s19/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s19\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s19/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s19\n",
      "[ROLLOUT] seed=7, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s7?cid=trace_pl-2606908964e941c8-i0000-t0005-s7\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s7/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s7\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s7/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s7\n",
      "[ROLLOUT] seed=18, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s18?cid=trace_pl-2606908964e941c8-i0000-t0005-s18\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s18/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s18\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s18/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s18\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=21, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s21?cid=trace_pl-2606908964e941c8-i0000-t0006-s21\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s21/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s21\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s21/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s21\n",
      "[ROLLOUT] seed=18, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s18?cid=trace_pl-2606908964e941c8-i0000-t0006-s18\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s18/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s18\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s18/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s18\n",
      "[ROLLOUT] seed=5, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s5?cid=trace_pl-2606908964e941c8-i0000-t0005-s5\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s5/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s5\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s5/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s5\n",
      "[ROLLOUT] seed=19, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s19?cid=trace_pl-2606908964e941c8-i0000-t0005-s19\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s19/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s19\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0005-s19/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0005-s19\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=7, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s7?cid=trace_pl-2606908964e941c8-i0000-t0008-s7\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s7/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s7\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s7/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s7\n",
      "[ROLLOUT] seed=5, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s5?cid=trace_pl-2606908964e941c8-i0000-t0006-s5\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s5/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s5\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0006-s5/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0006-s5\n",
      "[ROLLOUT] seed=21, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s21?cid=trace_pl-2606908964e941c8-i0000-t0008-s21\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s21/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s21\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s21/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s21\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=5, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s5?cid=trace_pl-2606908964e941c8-i0000-t0008-s5\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s5/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s5\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s5/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s5\n",
      "[ROLLOUT] seed=19, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s19?cid=trace_pl-2606908964e941c8-i0000-t0008-s19\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s19/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s19\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s19/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s19\n",
      "[ROLLOUT] seed=18, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s18?cid=trace_pl-2606908964e941c8-i0000-t0008-s18\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s18/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s18\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0008-s18/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0008-s18\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=21, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s21?cid=trace_pl-2606908964e941c8-i0000-t0007-s21\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s21/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s21\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s21/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s21\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=7, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s7?cid=trace_pl-2606908964e941c8-i0000-t0007-s7\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s7/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s7\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s7/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s7\n",
      "[ROLLOUT] seed=5, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s5?cid=trace_pl-2606908964e941c8-i0000-t0007-s5\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s5/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s5\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s5/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s5\n",
      "[ROLLOUT] seed=19, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s19?cid=trace_pl-2606908964e941c8-i0000-t0007-s19\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s19/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s19\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s19/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s19\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=18, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s18?cid=trace_pl-2606908964e941c8-i0000-t0007-s18\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s18/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s18\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0007-s18/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0007-s18\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=28, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s28?cid=trace_pl-2606908964e941c8-i0000-t0009-s28\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s28\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s28\n",
      "[ROLLOUT] seed=16, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s16?cid=trace_pl-2606908964e941c8-i0000-t0009-s16\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s16\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s16\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=1, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s1?cid=trace_pl-2606908964e941c8-i0000-t0009-s1\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s1\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s1\n",
      "[ROLLOUT] seed=8, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s8?cid=trace_pl-2606908964e941c8-i0000-t0009-s8\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s8\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s8\n",
      "[ROLLOUT] seed=20, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s20?cid=trace_pl-2606908964e941c8-i0000-t0009-s20\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s20\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s20\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=4, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s4?cid=trace_pl-2606908964e941c8-i0000-t0009-s4\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s4\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s4\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=6, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s6?cid=trace_pl-2606908964e941c8-i0000-t0009-s6\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s6\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s6\n",
      "[ROLLOUT] seed=13, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s13?cid=trace_pl-2606908964e941c8-i0000-t0009-s13\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s13\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s13\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=27, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s27?cid=trace_pl-2606908964e941c8-i0000-t0009-s27\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s27\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s27\n",
      "[ROLLOUT] seed=0, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s0?cid=trace_pl-2606908964e941c8-i0000-t0009-s0\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s0\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0009-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0009-s0\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=0, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s0?cid=trace_pl-2606908964e941c8-i0000-t0010-s0\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s0\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s0/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s0\n",
      "[ROLLOUT] seed=8, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s8?cid=trace_pl-2606908964e941c8-i0000-t0010-s8\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s8\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s8/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s8\n",
      "[ROLLOUT] seed=16, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s16?cid=trace_pl-2606908964e941c8-i0000-t0010-s16\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s16\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s16/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s16\n",
      "[ROLLOUT] seed=27, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s27?cid=trace_pl-2606908964e941c8-i0000-t0010-s27\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s27\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s27/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s27\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=28, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s28?cid=trace_pl-2606908964e941c8-i0000-t0010-s28\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s28\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s28/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s28\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=6, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s6?cid=trace_pl-2606908964e941c8-i0000-t0010-s6\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s6\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s6/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s6\n",
      "[ROLLOUT] seed=20, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s20?cid=trace_pl-2606908964e941c8-i0000-t0010-s20\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s20\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s20/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s20\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=1, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s1?cid=trace_pl-2606908964e941c8-i0000-t0010-s1\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s1\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s1/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s1\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=13, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s13?cid=trace_pl-2606908964e941c8-i0000-t0010-s13\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s13\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s13/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s13\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=4, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s4?cid=trace_pl-2606908964e941c8-i0000-t0010-s4\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s4\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0010-s4/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0010-s4\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=50, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s50?cid=trace_pl-2606908964e941c8-i0000-t0011-s50\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s50/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s50\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s50/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s50\n",
      "[ROLLOUT] seed=52, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s52?cid=trace_pl-2606908964e941c8-i0000-t0011-s52\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s52/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s52\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s52/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s52\n",
      "[ROLLOUT] seed=53, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s53?cid=trace_pl-2606908964e941c8-i0000-t0011-s53\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s53/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s53\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s53/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s53\n",
      "[ROLLOUT] seed=54, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s54?cid=trace_pl-2606908964e941c8-i0000-t0011-s54\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s54/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s54\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s54/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s54\n",
      "[ROLLOUT] seed=55, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s55?cid=trace_pl-2606908964e941c8-i0000-t0011-s55\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s55/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s55\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s55/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s55\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=51, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s51?cid=trace_pl-2606908964e941c8-i0000-t0011-s51\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s51/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s51\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0000-t0011-s51/chat/completions?cid=trace_pl-2606908964e941c8-i0000-t0011-s51\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=53, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s53?cid=trace_pl-2606908964e941c8-i0001-t0012-s53\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s53/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s53\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s53/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s53\n",
      "[ROLLOUT] seed=52, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s52?cid=trace_pl-2606908964e941c8-i0001-t0012-s52\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s52/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s52\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s52/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s52\n",
      "[ROLLOUT] seed=54, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s54?cid=trace_pl-2606908964e941c8-i0001-t0012-s54\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s54/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s54\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s54/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s54\n",
      "[ROLLOUT] seed=50, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s50?cid=trace_pl-2606908964e941c8-i0001-t0012-s50\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s50/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s50\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s50/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s50\n",
      "[ROLLOUT] seed=55, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s55?cid=trace_pl-2606908964e941c8-i0001-t0012-s55\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s55/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s55\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s55/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s55\n",
      "    [45s] running (best=None)\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=51, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s51?cid=trace_pl-2606908964e941c8-i0001-t0012-s51\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s51/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s51\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0012-s51/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0012-s51\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=55, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s55?cid=trace_pl-2606908964e941c8-i0001-t0013-s55\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s55/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s55\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s55/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s55\n",
      "[ROLLOUT] seed=50, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s50?cid=trace_pl-2606908964e941c8-i0001-t0013-s50\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s50/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s50\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s50/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s50\n",
      "[ROLLOUT] seed=54, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s54?cid=trace_pl-2606908964e941c8-i0001-t0013-s54\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s54/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s54\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s54/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s54\n",
      "[ROLLOUT] seed=53, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s53?cid=trace_pl-2606908964e941c8-i0001-t0013-s53\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s53/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s53\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s53/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s53\n",
      "[ROLLOUT] seed=52, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s52?cid=trace_pl-2606908964e941c8-i0001-t0013-s52\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s52/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s52\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s52/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s52\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=51, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s51?cid=trace_pl-2606908964e941c8-i0001-t0013-s51\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s51/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s51\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0013-s51/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0013-s51\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=53, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s53?cid=trace_pl-2606908964e941c8-i0001-t0014-s53\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s53/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s53\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s53/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s53\n",
      "[ROLLOUT] seed=54, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s54?cid=trace_pl-2606908964e941c8-i0001-t0014-s54\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s54/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s54\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s54/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s54\n",
      "[ROLLOUT] seed=52, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s52?cid=trace_pl-2606908964e941c8-i0001-t0014-s52\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s52/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s52\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s52/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s52\n",
      "[ROLLOUT] seed=55, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s55?cid=trace_pl-2606908964e941c8-i0001-t0014-s55\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s55/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s55\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s55/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s55\n",
      "[ROLLOUT] seed=51, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s51?cid=trace_pl-2606908964e941c8-i0001-t0014-s51\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s51/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s51\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s51/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s51\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=50, split=train, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'temperature', 'max_completion_tokens', 'inference_url', 'trial_id', 'trace_correlation_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s50?cid=trace_pl-2606908964e941c8-i0001-t0014-s50\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s50/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s50\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/pl-2606908964e941c8-i0001-t0014-s50/chat/completions?cid=trace_pl-2606908964e941c8-i0001-t0014-s50\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "    [59s] succeeded (best=0.8)\n",
      "\n",
      "FINAL: succeeded\n",
      "BEST SCORE: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Run GEPA Optimization (using tunnel URL)\n",
    "async def run_gepa():\n",
    "    config_body = {\n",
    "        'prompt_learning': {\n",
    "            'algorithm': 'gepa',\n",
    "            'run_local': False,  # Run on remote backend\n",
    "            'task_app_url': BASELINE_TASK_APP_URL,\n",
    "            'task_app_api_key': ENVIRONMENT_API_KEY,\n",
    "            'env_name': 'banking77',\n",
    "            'initial_prompt': {\n",
    "                'messages': [\n",
    "                    {'role': 'system', 'order': 0, 'pattern': BASELINE_SYSTEM_PROMPT},\n",
    "                    {'role': 'user', 'order': 1, 'pattern': USER_PROMPT},\n",
    "                ],\n",
    "                'wildcards': {'query': 'REQUIRED', 'available_intents': 'OPTIONAL'},\n",
    "            },\n",
    "            'policy': {'model': 'gpt-4.1-nano', 'provider': 'openai', 'temperature': 0.0, 'max_completion_tokens': 256},\n",
    "            'gepa': {\n",
    "                'env_name': 'banking77',\n",
    "                'evaluation': {'seeds': list(range(30)), 'validation_seeds': list(range(50, 56))},\n",
    "                'rollout': {'budget': 50, 'max_concurrent': 5, 'minibatch_size': 5},\n",
    "                'mutation': {'rate': 0.3, 'llm_model': 'gpt-4.1-nano'},\n",
    "                'population': {'initial_size': 3, 'num_generations': 2, 'children_per_generation': 2},\n",
    "                'archive': {'size': 5, 'pareto_set_size': 10},\n",
    "                'token': {'counting_model': 'gpt-4'},\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(f'Creating GEPA job (task_app_url={BASELINE_TASK_APP_URL})...')\n",
    "    async with httpx.AsyncClient(timeout=30) as client:\n",
    "        resp = await client.post(\n",
    "            f'{SYNTH_API_BASE}/api/prompt-learning/online/jobs',\n",
    "            json={'algorithm': 'gepa', 'config_body': config_body},\n",
    "            headers={'Authorization': f'Bearer {API_KEY}'}\n",
    "        )\n",
    "        if resp.status_code != 200:\n",
    "            print(f'ERROR: {resp.status_code} - {resp.text[:500]}')\n",
    "            resp.raise_for_status()\n",
    "        job_id = resp.json()['job_id']\n",
    "    print(f'Job ID: {job_id}')\n",
    "\n",
    "    print('Polling...')\n",
    "    start = time.time()\n",
    "    last_status = None\n",
    "    job = None\n",
    "    \n",
    "    while True:\n",
    "        async with httpx.AsyncClient(timeout=30) as client:\n",
    "            resp = await client.get(\n",
    "                f'{SYNTH_API_BASE}/api/prompt-learning/online/jobs/{job_id}',\n",
    "                headers={'Authorization': f'Bearer {API_KEY}'}\n",
    "            )\n",
    "            resp.raise_for_status()\n",
    "            job = resp.json()\n",
    "        \n",
    "        status = job['status']\n",
    "        elapsed = int(time.time() - start)\n",
    "        best = job.get('best_train_score') or job.get('best_score')\n",
    "        \n",
    "        if status != last_status or elapsed % 15 == 0:\n",
    "            print(f'    [{elapsed}s] {status} (best={best})')\n",
    "            last_status = status\n",
    "        \n",
    "        if status in ['succeeded', 'failed', 'cancelled']:\n",
    "            break\n",
    "        await asyncio.sleep(3)\n",
    "\n",
    "    print(f'\\nFINAL: {status}')\n",
    "    if status == 'succeeded':\n",
    "        best = job.get('best_score') or job.get('best_train_score')\n",
    "        print(f'BEST SCORE: {best}')\n",
    "    elif status == 'failed':\n",
    "        print(f'ERROR: {job.get(\"error\")}')\n",
    "    \n",
    "    return job\n",
    "\n",
    "job = await run_gepa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEPA Job Succeeded!\n",
      "\n",
      "============================================================\n",
      "BASELINE SYSTEM PROMPT\n",
      "============================================================\n",
      "You are an expert banking assistant that classifies customer queries into banking intents. Given a customer message, respond with exactly one intent label from the provided list using the `banking77_classify` tool.\n",
      "\n",
      "============================================================\n",
      "OPTIMIZED SYSTEM PROMPT (from GEPA)\n",
      "============================================================\n",
      "[1. Input Understanding]\n",
      "- You will receive a single input field named `Customer Query` containing a user's banking-related question, issue, or concern.\n",
      "- You will also be provided with a fixed list of 77 detailed banking intent labels named `Available Intents`.\n",
      "\n",
      "[2. Core Task Description]\n",
      "- Your task is to classify the `Customer Query` into exactly one intent label from `Available Intents`.\n",
      "- Select the single intent that best captures the primary purpose or concern expressed in the query.\n",
      "- Return only the chosen intent label as plain text, implicitly using the `banking77_classify` tool call format.\n",
      "\n",
      "[3. Premises]\n",
      "- Each customer query expresses precisely one primary banking-related intent or issue.\n",
      "- The `Available Intents` list includes specific and granulated banking topics such as ca...\n",
      "\n",
      "============================================================\n",
      "GEPA TRAINING RESULTS\n",
      "============================================================\n",
      "Baseline Train:  40.0%\n",
      "Optimized Train: 80.0%\n",
      "Training Lift:   +40.0%\n",
      "\n",
      "============================================================\n",
      "FORMAL EVAL JOBS (test split, seeds 100-119)\n",
      "============================================================\n",
      "\n",
      "Starting optimized task app on port 8002...\n",
      "Waiting for optimized_task_app on port 8002...\n",
      "optimized_task_app ready: {'status': 'healthy', 'authorized': True}\n",
      "Provisioning Cloudflare tunnel for optimized...\n",
      "Optimized tunnel: task-8002-12613.usesynth.ai\n",
      "Started cloudflared for optimized (pid=47178, log=/tmp/cloudflared_optimized.log)\n",
      "Waiting for tunnel: task-8002-12613.usesynth.ai (timeout=120s)\n",
      "Tunnel reachable: task-8002-12613.usesynth.ai (attempt=3)\n",
      "\n",
      "Running BASELINE eval job...\n",
      "  baseline eval job: eval_d9b2b672cae84c7e\n",
      "[ROLLOUT] seed=108, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0108-s108/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed108_760d3662\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0108-s108/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed108_760d3662\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0108-s108/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed108_760d3662\n",
      "[ROLLOUT] seed=104, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0104-s104/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed104_2aa76359\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0104-s104/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed104_2aa76359\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0104-s104/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed104_2aa76359\n",
      "[ROLLOUT] seed=102, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0102-s102/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed102_a69dcd0a\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0102-s102/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed102_a69dcd0a\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0102-s102/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed102_a69dcd0a\n",
      "[ROLLOUT] seed=107, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0107-s107/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed107_771fccd8\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0107-s107/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed107_771fccd8\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0107-s107/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed107_771fccd8\n",
      "[ROLLOUT] seed=101, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0101-s101/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed101_a1d20640\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0101-s101/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed101_a1d20640\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0101-s101/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed101_a1d20640\n",
      "    [0s] running (0/20)\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=109, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0109-s109/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed109_b6281874\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0109-s109/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed109_b6281874\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0109-s109/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed109_b6281874\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=100, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0100-s100/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed100_47787aac\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0100-s100/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed100_47787aac\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0100-s100/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed100_47787aac\n",
      "[ROLLOUT] seed=105, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0105-s105/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed105_e38bf86f\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0105-s105/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed105_e38bf86f\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0105-s105/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed105_e38bf86f\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=106, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0106-s106/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed106_9c7cf84f\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0106-s106/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed106_9c7cf84f\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0106-s106/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed106_9c7cf84f\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=103, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0103-s103/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed103_2ec7035b\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0103-s103/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed103_2ec7035b\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0103-s103/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed103_2ec7035b\n",
      "[ROLLOUT] seed=110, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0110-s110/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed110_fd654bb5\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0110-s110/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed110_fd654bb5\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0110-s110/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed110_fd654bb5\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=111, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0111-s111/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed111_d0e5375d\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0111-s111/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed111_d0e5375d\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0111-s111/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed111_d0e5375d\n",
      "[ROLLOUT] seed=112, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0112-s112/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed112_5bc914a4\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0112-s112/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed112_5bc914a4\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0112-s112/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed112_5bc914a4\n",
      "[ROLLOUT] seed=113, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0113-s113/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed113_2d8957a9\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0113-s113/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed113_2d8957a9\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0113-s113/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed113_2d8957a9\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=114, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0114-s114/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed114_188d1f35\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0114-s114/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed114_188d1f35\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0114-s114/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed114_188d1f35\n",
      "[ROLLOUT] seed=116, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0116-s116/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed116_2d28285d\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0116-s116/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed116_2d28285d\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0116-s116/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed116_2d28285d\n",
      "[ROLLOUT] seed=115, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0115-s115/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed115_4fdc5563\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0115-s115/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed115_4fdc5563\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0115-s115/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed115_4fdc5563\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=117, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0117-s117/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed117_532a2a3c\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0117-s117/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed117_532a2a3c\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0117-s117/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed117_532a2a3c\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=119, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0119-s119/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed119_c6dc2e61\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0119-s119/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed119_c6dc2e61\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0119-s119/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed119_c6dc2e61\n",
      "[ROLLOUT] seed=118, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0118-s118/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed118_1976a2ae\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0118-s118/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed118_1976a2ae\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-d9b2b672cae84c7e-i0000-t0118-s118/chat/completions?cid=eval_eval_d9b2b672cae84c7e_seed118_1976a2ae\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "  Baseline eval: 60.0%\n",
      "\n",
      "Running OPTIMIZED eval job...\n",
      "  optimized eval job: eval_5bbda7ddcb314d3d\n",
      "[ROLLOUT] seed=104, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0104-s104/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed104_97ce0b0f\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0104-s104/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed104_97ce0b0f\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0104-s104/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed104_97ce0b0f\n",
      "[ROLLOUT] seed=105, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0105-s105/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed105_92080393\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0105-s105/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed105_92080393\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0105-s105/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed105_92080393\n",
      "[ROLLOUT] seed=103, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0103-s103/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed103_68b0ea13\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0103-s103/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed103_68b0ea13\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0103-s103/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed103_68b0ea13\n",
      "[ROLLOUT] seed=100, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0100-s100/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed100_b424f0ad\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0100-s100/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed100_b424f0ad\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0100-s100/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed100_b424f0ad\n",
      "[ROLLOUT] seed=109, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0109-s109/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed109_a4371e15\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0109-s109/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed109_a4371e15\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0109-s109/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed109_a4371e15\n",
      "    [0s] running (0/20)\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=108, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0108-s108/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed108_14236dae\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0108-s108/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed108_14236dae\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0108-s108/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed108_14236dae\n",
      "[ROLLOUT] seed=107, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0107-s107/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed107_bfafaad3\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0107-s107/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed107_bfafaad3\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0107-s107/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed107_bfafaad3\n",
      "[ROLLOUT] seed=101, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0101-s101/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed101_14a16706\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0101-s101/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed101_14a16706\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0101-s101/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed101_14a16706\n",
      "[ROLLOUT] seed=106, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0106-s106/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed106_d369791b\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0106-s106/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed106_d369791b\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0106-s106/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed106_d369791b\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=102, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0102-s102/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed102_413ec3e9\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0102-s102/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed102_413ec3e9\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0102-s102/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed102_413ec3e9\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=110, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0110-s110/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed110_696a0df1\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0110-s110/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed110_696a0df1\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0110-s110/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed110_696a0df1\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=111, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0111-s111/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed111_d8acb177\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0111-s111/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed111_d8acb177\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0111-s111/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed111_d8acb177\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=113, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0113-s113/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed113_7af9e0aa\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0113-s113/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed113_7af9e0aa\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0113-s113/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed113_7af9e0aa\n",
      "[ROLLOUT] seed=112, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0112-s112/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed112_8f683c4c\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0112-s112/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed112_8f683c4c\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0112-s112/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed112_8f683c4c\n",
      "[ROLLOUT] seed=114, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0114-s114/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed114_4caa6940\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0114-s114/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed114_4caa6940\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0114-s114/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed114_4caa6940\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=115, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0115-s115/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed115_d999d792\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0115-s115/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed115_d999d792\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0115-s115/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed115_d999d792\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] seed=117, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0117-s117/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed117_6d0b8b81\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0117-s117/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed117_6d0b8b81\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0117-s117/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed117_6d0b8b81\n",
      "[ROLLOUT] seed=116, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0116-s116/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed116_8293c3ac\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0116-s116/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed116_8293c3ac\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0116-s116/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed116_8293c3ac\n",
      "[ROLLOUT] seed=118, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0118-s118/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed118_312b624d\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0118-s118/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed118_312b624d\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0118-s118/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed118_312b624d\n",
      "[ROLLOUT] seed=119, split=test, mode=eval\n",
      "[ROLLOUT] policy_config keys: ['model', 'provider', 'inference_url', 'trace_correlation_id', 'trial_id']\n",
      "[ROLLOUT] RAW inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0119-s119/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed119_218733c3\n",
      "[ROLLOUT] NORMALIZED inference_url: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0119-s119/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed119_218733c3\n",
      "[ROLLOUT] POST to: https://synth-backend-dev-docker.onrender.com/api/interceptor/v1/eval-5bbda7ddcb314d3d-i0000-t0119-s119/chat/completions?cid=eval_eval_5bbda7ddcb314d3d_seed119_218733c3\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "[ROLLOUT] Response status: 200\n",
      "  Optimized eval: 85.0%\n",
      "\n",
      "============================================================\n",
      "FINAL COMPARISON\n",
      "============================================================\n",
      "Training:\n",
      "  Baseline:  40.0%\n",
      "  Optimized: 80.0%\n",
      "  Lift:      +40.0%\n",
      "\n",
      "Eval (seeds 100-119, held-out):\n",
      "  Baseline:  60.0%\n",
      "  Optimized: 85.0%\n",
      "  Lift:      +25.0%\n",
      "\n",
      ">>> OPTIMIZATION GENERALIZES TO HELD-OUT DATA!\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Run Formal Eval Jobs (Baseline vs Optimized)\n",
    "\n",
    "EVAL_SEEDS = list(range(100, 120))  # 20 held-out test samples\n",
    "\n",
    "async def run_eval_job(task_app_url: str, task_app_api_key: str, seeds: list, mode: str) -> dict:\n",
    "    \"\"\"Run an eval job and wait for completion.\"\"\"\n",
    "    async with httpx.AsyncClient(timeout=30) as client:\n",
    "        resp = await client.post(\n",
    "            f'{SYNTH_API_BASE}/api/eval/jobs',\n",
    "            json={\n",
    "                'task_app_url': task_app_url,\n",
    "                'task_app_api_key': task_app_api_key,\n",
    "                'env_name': 'banking77',\n",
    "                'seeds': seeds,\n",
    "                'env_config': {'split': 'test'},\n",
    "                'policy': {'model': 'gpt-4.1-nano', 'provider': 'openai'},\n",
    "                'mode': mode,\n",
    "                'max_concurrent': 10,\n",
    "            },\n",
    "            headers={'Authorization': f'Bearer {API_KEY}'}\n",
    "        )\n",
    "        if resp.status_code != 200:\n",
    "            print(f'ERROR creating {mode} eval job: {resp.status_code} - {resp.text[:300]}')\n",
    "            return {'status': 'failed', 'error': resp.text}\n",
    "        \n",
    "        job_id = resp.json()['job_id']\n",
    "        print(f'  {mode} eval job: {job_id}')\n",
    "    \n",
    "    start = time.time()\n",
    "    while True:\n",
    "        async with httpx.AsyncClient(timeout=30) as client:\n",
    "            resp = await client.get(\n",
    "                f'{SYNTH_API_BASE}/api/eval/jobs/{job_id}',\n",
    "                headers={'Authorization': f'Bearer {API_KEY}'}\n",
    "            )\n",
    "            if resp.status_code != 200:\n",
    "                print(f'  Error polling: {resp.status_code}')\n",
    "                await asyncio.sleep(2)\n",
    "                continue\n",
    "            job = resp.json()\n",
    "        \n",
    "        status = job.get('status', '')\n",
    "        elapsed = int(time.time() - start)\n",
    "        \n",
    "        if status in ['completed', 'failed']:\n",
    "            break\n",
    "        \n",
    "        if elapsed % 10 == 0:\n",
    "            results = job.get('results') or {}\n",
    "            completed = results.get('completed', 0)\n",
    "            total = results.get('total', len(seeds))\n",
    "            print(f'    [{elapsed}s] {status} ({completed}/{total})')\n",
    "        \n",
    "        await asyncio.sleep(2)\n",
    "    \n",
    "    async with httpx.AsyncClient(timeout=30) as client:\n",
    "        resp = await client.get(\n",
    "            f'{SYNTH_API_BASE}/api/eval/jobs/{job_id}/results',\n",
    "            headers={'Authorization': f'Bearer {API_KEY}'}\n",
    "        )\n",
    "        if resp.status_code != 200:\n",
    "            return {'status': 'failed', 'error': f'Failed to get results: {resp.status_code}'}\n",
    "        return resp.json()\n",
    "\n",
    "if job['status'] == 'succeeded':\n",
    "    print(\"GEPA Job Succeeded!\\n\")\n",
    "    \n",
    "    # Get job details with error handling\n",
    "    async with httpx.AsyncClient(timeout=30) as client:\n",
    "        resp = await client.get(\n",
    "            f'{SYNTH_API_BASE}/api/prompt-learning/online/jobs/{job[\"job_id\"]}',\n",
    "            headers={'Authorization': f'Bearer {API_KEY}'}\n",
    "        )\n",
    "        if resp.status_code != 200:\n",
    "            print(f'ERROR getting job details: {resp.status_code} - {resp.text[:200]}')\n",
    "            job_details = {}\n",
    "        else:\n",
    "            job_details = resp.json()\n",
    "    \n",
    "    best_snapshot = job_details.get('best_snapshot') or {}\n",
    "    \n",
    "    # Try to get optimized prompt from job_details or from the job itself\n",
    "    baseline_score_info = best_snapshot.get('baseline_score_info') or {}\n",
    "    best_score_info = best_snapshot.get('best_score_info') or {}\n",
    "    baseline_train = baseline_score_info.get('mean_score') or baseline_score_info.get('score')\n",
    "    optimized_train = best_score_info.get('mean_score') or best_score_info.get('score') or job.get('best_score')\n",
    "    \n",
    "    best_prompt = best_snapshot.get('best_prompt', {})\n",
    "    best_prompt_messages = best_prompt.get('messages', [])\n",
    "    \n",
    "    # Extract optimized system prompt\n",
    "    optimized_system = None\n",
    "    for msg in best_prompt_messages:\n",
    "        if msg.get('role') == 'system':\n",
    "            optimized_system = msg.get('pattern') or msg.get('content')\n",
    "            break\n",
    "    \n",
    "    # If we didn't get optimized_system from best_snapshot, use baseline (for eval comparison)\n",
    "    if not optimized_system:\n",
    "        print(\"NOTE: Could not extract optimized prompt from job details, using baseline for comparison\")\n",
    "        optimized_system = BASELINE_SYSTEM_PROMPT\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('BASELINE SYSTEM PROMPT')\n",
    "    print('=' * 60)\n",
    "    print(BASELINE_SYSTEM_PROMPT)\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print('OPTIMIZED SYSTEM PROMPT (from GEPA)')\n",
    "    print('=' * 60)\n",
    "    print(optimized_system[:800] + \"...\" if len(optimized_system) > 800 else optimized_system)\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print('GEPA TRAINING RESULTS')\n",
    "    print('=' * 60)\n",
    "    if baseline_train:\n",
    "        print(f\"Baseline Train:  {baseline_train:.1%}\")\n",
    "    if optimized_train:\n",
    "        print(f\"Optimized Train: {optimized_train:.1%}\")\n",
    "    if baseline_train and optimized_train:\n",
    "        print(f\"Training Lift:   {optimized_train - baseline_train:+.1%}\")\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print(f'FORMAL EVAL JOBS (test split, seeds {EVAL_SEEDS[0]}-{EVAL_SEEDS[-1]})')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Start optimized task app on different port\n",
    "    print(f'\\nStarting optimized task app on port {OPTIMIZED_TASK_APP_PORT}...')\n",
    "    optimized_app = create_banking77_task_app(optimized_system, ENVIRONMENT_API_KEY)\n",
    "    optimized_thread = start_task_app(optimized_app, OPTIMIZED_TASK_APP_PORT, \"optimized_task_app\")\n",
    "    \n",
    "    # Get tunnel for optimized\n",
    "    print('Provisioning Cloudflare tunnel for optimized...')\n",
    "    optimized_tunnel = await rotate_tunnel(OPTIMIZED_TASK_APP_PORT, \"optimized_notebook\")\n",
    "    OPTIMIZED_TUNNEL_HOSTNAME = optimized_tunnel['hostname']\n",
    "    OPTIMIZED_TASK_APP_URL = f'https://{OPTIMIZED_TUNNEL_HOSTNAME}'\n",
    "    print(f'Optimized tunnel: {OPTIMIZED_TUNNEL_HOSTNAME}')\n",
    "    \n",
    "    # Start cloudflared for optimized\n",
    "    optimized_cf_proc = start_cloudflared(optimized_tunnel['tunnel_token'], 'optimized')\n",
    "    await wait_for_tunnel_dns(OPTIMIZED_TUNNEL_HOSTNAME)\n",
    "    \n",
    "    # Run baseline eval\n",
    "    print('\\nRunning BASELINE eval job...')\n",
    "    baseline_results = await run_eval_job(\n",
    "        task_app_url=BASELINE_TASK_APP_URL,\n",
    "        task_app_api_key=ENVIRONMENT_API_KEY,\n",
    "        seeds=EVAL_SEEDS,\n",
    "        mode='baseline'\n",
    "    )\n",
    "    \n",
    "    baseline_summary = baseline_results.get('summary', {})\n",
    "    baseline_eval_score = baseline_summary.get('mean_score')\n",
    "    print(f'  Baseline eval: {baseline_eval_score:.1%}' if baseline_eval_score is not None else '  Baseline eval: N/A')\n",
    "    \n",
    "    # Run optimized eval\n",
    "    print('\\nRunning OPTIMIZED eval job...')\n",
    "    optimized_results = await run_eval_job(\n",
    "        task_app_url=OPTIMIZED_TASK_APP_URL,\n",
    "        task_app_api_key=ENVIRONMENT_API_KEY,\n",
    "        seeds=EVAL_SEEDS,\n",
    "        mode='optimized'\n",
    "    )\n",
    "    \n",
    "    optimized_summary = optimized_results.get('summary', {})\n",
    "    optimized_eval_score = optimized_summary.get('mean_score')\n",
    "    print(f'  Optimized eval: {optimized_eval_score:.1%}' if optimized_eval_score is not None else '  Optimized eval: N/A')\n",
    "    \n",
    "    # Final comparison\n",
    "    print('\\n' + '=' * 60)\n",
    "    print('FINAL COMPARISON')\n",
    "    print('=' * 60)\n",
    "    print(f\"Training:\")\n",
    "    if baseline_train:\n",
    "        print(f\"  Baseline:  {baseline_train:.1%}\")\n",
    "    if optimized_train:\n",
    "        print(f\"  Optimized: {optimized_train:.1%}\")\n",
    "    if baseline_train and optimized_train:\n",
    "        print(f\"  Lift:      {optimized_train - baseline_train:+.1%}\")\n",
    "    \n",
    "    print(f\"\\nEval (seeds {EVAL_SEEDS[0]}-{EVAL_SEEDS[-1]}, held-out):\")\n",
    "    if baseline_eval_score is not None:\n",
    "        print(f\"  Baseline:  {baseline_eval_score:.1%}\")\n",
    "    if optimized_eval_score is not None:\n",
    "        print(f\"  Optimized: {optimized_eval_score:.1%}\")\n",
    "    if baseline_eval_score is not None and optimized_eval_score is not None:\n",
    "        eval_lift = optimized_eval_score - baseline_eval_score\n",
    "        print(f\"  Lift:      {eval_lift:+.1%}\")\n",
    "        \n",
    "        if eval_lift > 0:\n",
    "            print(\"\\n>>> OPTIMIZATION GENERALIZES TO HELD-OUT DATA!\")\n",
    "        elif eval_lift == 0:\n",
    "            print(\"\\n=== Same performance on held-out data\")\n",
    "        else:\n",
    "            print(\"\\n<<< Baseline better on held-out (possible overfitting)\")\n",
    "else:\n",
    "    print(f\"Job did not succeed: {job.get('status')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Cleanup\n",
    "print('Cleaning up cloudflared processes...')\n",
    "cleanup_cloudflared()\n",
    "print('Demo complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e125b0c0-b463-43d5-9529-1aac88e7e1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}