{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Synth GEPA Demo - Banking77 (Production)\n",
    "\n",
    "Self-contained notebook for prompt optimization using GEPA.\n",
    "\n",
    "Banking77 is a task where an AI needs to label a customer service request with one of 77 fixed possible intents.\n",
    "\n",
    "**Run in Google Colab:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/synth-laboratories/synth-ai/blob/main/demos/gepa_banking77/demo_prod.ipynb)\n",
    "\n",
    "**What this demo does:**\n",
    "1. Spins up a local api that runs the banking77 classification pipeline\n",
    "2. Creates a Cloudflare tunnel to expose it to the internet\n",
    "3. Runs GEPA prompt optimization via Synth\n",
    "4. Compares baseline vs optimized prompts on held-out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ku8p0wn7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Install dependencies (run this first on Colab)\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab - installing dependencies...\")\n",
    "    !pip install -q synth-ai httpx fastapi uvicorn datasets nest_asyncio\n",
    "    \n",
    "    # Install cloudflared\n",
    "    !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O /usr/local/bin/cloudflared\n",
    "    !chmod +x /usr/local/bin/cloudflared\n",
    "    !cloudflared --version\n",
    "    \n",
    "    print(\"Dependencies installed!\")\n",
    "else:\n",
    "    print(\"Not in Colab - assuming dependencies are already installed\")\n",
    "    print(\"Required: pip install synth-ai httpx fastapi uvicorn datasets nest_asyncio\")\n",
    "    print(\"Required: brew install cloudflare/cloudflare/cloudflared (macOS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0n3ycc08f4tq",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "To complete prompt optimization, we'll need to spin up services that expose our business logic to the optimizer via http. Because we'll evaluate the optimized prompt against the baseline prompt concurrently at the end, we'll select two unique ports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Imports, Config, and Backend Health Check\n",
    "import os, sys, time, asyncio, json, threading\n",
    "from typing import Any, Optional\n",
    "\n",
    "import httpx\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, Request, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Production backend\n",
    "SYNTH_API_BASE = 'https://api.usesynth.ai'\n",
    "LOCAL_API_PORT = 8001\n",
    "OPTIMIZED_LOCAL_API_PORT = 8002\n",
    "\n",
    "print(f'Backend: {SYNTH_API_BASE}')\n",
    "print(f'Local API Ports: {LOCAL_API_PORT}, {OPTIMIZED_LOCAL_API_PORT}')\n",
    "\n",
    "# Check backend health\n",
    "r = httpx.get(f'{SYNTH_API_BASE}/health', timeout=30)\n",
    "if r.status_code == 200:\n",
    "    print(f'Backend health: {r.json()}')\n",
    "else:\n",
    "    print(f'WARNING: Backend returned status {r.status_code}')\n",
    "    print(f'Response: {r.text[:200]}...' if len(r.text) > 200 else f'Response: {r.text}')\n",
    "    raise RuntimeError(f'Backend not healthy: status {r.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v9dkmn241l",
   "metadata": {},
   "source": [
    "## Step 2: Authentication\n",
    "\n",
    "To kick off any job via Synth, we need an API key. Typically, you'll use your own. But here, for simplicity's sake, we'll mint a demo API key which is scoped to these demos and should not be used in other instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Get API Key (use env var or mint demo key)\n",
    "API_KEY = os.environ.get('SYNTH_API_KEY', '')\n",
    "\n",
    "if not API_KEY:\n",
    "    print('No SYNTH_API_KEY found, minting demo key...')\n",
    "    resp = httpx.post(f'{SYNTH_API_BASE}/api/demo/keys', json={'ttl_hours': 4}, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    API_KEY = resp.json()['api_key']\n",
    "    print(f'Demo API Key: {API_KEY[:25]}...')\n",
    "else:\n",
    "    print(f'Using SYNTH_API_KEY: {API_KEY[:20]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7jqt92io4",
   "metadata": {},
   "source": [
    "## Step 3: Environment Key\n",
    "\n",
    "Because your business logic's integrity is important, you should require authentication to the local api you serve during optimization. The SDK has functions to help you mint and register your key, which we'll then use to authenticate requests from the Synth optimization job to our local API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Mint and Upload Environment Key (using SDK helpers)\n",
    "from synth_ai.sdk.learning.rl import mint_environment_api_key, setup_environment_api_key\n",
    "\n",
    "ENVIRONMENT_API_KEY = mint_environment_api_key()\n",
    "print(f'Minted env key: {ENVIRONMENT_API_KEY[:12]}...{ENVIRONMENT_API_KEY[-4:]}')\n",
    "\n",
    "result = setup_environment_api_key(SYNTH_API_BASE, API_KEY, token=ENVIRONMENT_API_KEY)\n",
    "print(f'Uploaded env key: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5hgu087g",
   "metadata": {},
   "source": [
    "## Step 4: Local API Definition\n",
    "\n",
    "Here we define a FastAPI service that serves our Banking77 classification pipeline.\n",
    "\n",
    "**Key Design: Clean Separation**\n",
    "\n",
    "The code is split into two parts:\n",
    "\n",
    "1. **`classify_banking77_query()`** - A clean classification pipeline using standard OpenAI Python client. This is normal business logic with NO Synth-specific code. It could be used anywhere.\n",
    "\n",
    "2. **`create_banking77_local_api()`** - A thin wrapper that:\n",
    "   - Sets `OPENAI_BASE_URL` to route calls through Synth's inference proxy\n",
    "   - Calls the clean classification pipeline\n",
    "   - Computes rewards\n",
    "   - Returns `RolloutResponse` with just metrics (no trace)\n",
    "\n",
    "**Why no trace?** Synth's inference proxy intercepts all OpenAI calls and reconstructs traces server-side. Your code doesn't need to manually build trace objects.\n",
    "\n",
    "**Required Routes:**\n",
    "\n",
    "| Route | Method | Description |\n",
    "|-------|--------|-------------|\n",
    "| `/health` | GET | Health check |\n",
    "| `/info` | GET | Static metadata |\n",
    "| `/task_info` | GET | Task instances for seeds |\n",
    "| `/rollout` | POST | Execute classification and return reward |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: Define Banking77 Local API\nimport json\nimport os\n\nfrom datasets import load_dataset\nfrom openai import AsyncOpenAI\n\n# Synth-AI SDK LocalAPI imports\nfrom synth_ai.sdk.localapi import LocalAPIConfig, create_local_api\nfrom synth_ai.sdk.task.contracts import RolloutMetrics, RolloutRequest, RolloutResponse, TaskInfo\n\n# ============================================================================\n# PART 1: Core Classification Pipeline (NO Synth logic)\n# ============================================================================\n# This is your business logic - a normal OpenAI-based classification pipeline.\n# It uses the standard openai library and has no knowledge of Synth.\n# The only \"magic\" is that OPENAI_BASE_URL can be set externally to route\n# calls through Synth's inference proxy for trace reconstruction.\n\nAPP_ID = \"banking77\"\nAPP_NAME = \"Banking77 Intent Classification\"\n\nBANKING77_LABELS = [\n    \"activate_my_card\", \"age_limit\", \"apple_pay_or_google_pay\", \"atm_support\", \"automatic_top_up\",\n    \"balance_not_updated_after_bank_transfer\", \"balance_not_updated_after_cheque_or_cash_deposit\",\n    \"beneficiary_not_allowed\", \"cancel_transfer\", \"card_about_to_expire\", \"card_acceptance\",\n    \"card_arrival\", \"card_delivery_estimate\", \"card_linking\", \"card_not_working\",\n    \"card_payment_fee_charged\", \"card_payment_not_recognised\", \"card_payment_wrong_exchange_rate\",\n    \"card_swallowed\", \"cash_withdrawal_charge\", \"cash_withdrawal_not_recognised\", \"change_pin\",\n    \"compromised_card\", \"contactless_not_working\", \"country_support\", \"declined_card_payment\",\n    \"declined_cash_withdrawal\", \"declined_transfer\", \"direct_debit_payment_not_recognised\",\n    \"disposable_card_limits\", \"edit_personal_details\", \"exchange_charge\", \"exchange_rate\",\n    \"exchange_via_app\", \"extra_charge_on_statement\", \"failed_transfer\", \"fiat_currency_support\",\n    \"get_disposable_virtual_card\", \"get_physical_card\", \"getting_spare_card\", \"getting_virtual_card\",\n    \"lost_or_stolen_card\", \"lost_or_stolen_phone\", \"order_physical_card\", \"passcode_forgotten\",\n    \"pending_card_payment\", \"pending_cash_withdrawal\", \"pending_top_up\", \"pending_transfer\",\n    \"pin_blocked\", \"receiving_money\", \"Refund_not_showing_up\", \"request_refund\",\n    \"reverted_card_payment?\", \"supported_cards_and_currencies\", \"terminate_account\",\n    \"top_up_by_bank_transfer_charge\", \"top_up_by_card_charge\", \"top_up_by_cash_or_cheque\",\n    \"top_up_failed\", \"top_up_limits\", \"top_up_reverted\", \"topping_up_by_card\",\n    \"transaction_charged_twice\", \"transfer_fee_charged\", \"transfer_into_account\",\n    \"transfer_not_received_by_recipient\", \"transfer_timing\", \"unable_to_verify_identity\",\n    \"verify_my_identity\", \"verify_source_of_funds\", \"verify_top_up\", \"virtual_card_not_working\",\n    \"visa_or_mastercard\", \"why_verify_identity\", \"wrong_amount_of_cash_received\",\n    \"wrong_exchange_rate_for_cash_withdrawal\",\n]\n\nTOOL_NAME = \"banking77_classify\"\nTOOL_SCHEMA = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": TOOL_NAME,\n        \"description\": \"Return the predicted banking77 intent label.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\"intent\": {\"type\": \"string\"}},\n            \"required\": [\"intent\"],\n        },\n    },\n}\n\n\ndef format_available_intents(label_names: list) -> str:\n    \"\"\"Format the list of available intents for the prompt.\"\"\"\n    return \"\\n\".join(f\"{i+1}. {l}\" for i, l in enumerate(label_names))\n\n\n# =============================================================================\n# CORE PIPELINE: This is the main classification logic.\n# =============================================================================\n# This function is completely independent of Synth. It's just a normal\n# async function that calls OpenAI and returns a prediction. You could\n# use this exact code in any application.\n\nasync def classify_banking77_query(\n    query: str,\n    system_prompt: str,\n    available_intents: str,\n    model: str = \"gpt-4o-mini\",\n    api_key: str | None = None,\n) -> str:\n    \"\"\"Classify a banking query into an intent using OpenAI.\n    \n    This is the CORE PIPELINE - clean async code with NO Synth-specific logic.\n    It uses whatever OPENAI_BASE_URL is set in the environment, allowing\n    calls to be routed through Synth's inference proxy for trace reconstruction.\n    \n    Args:\n        query: The customer query to classify\n        system_prompt: System prompt for the model\n        available_intents: Formatted list of available intents\n        model: Model to use (e.g., \"gpt-4o-mini\")\n        api_key: Optional API key (uses OPENAI_API_KEY env var if not provided)\n    \n    Returns:\n        The predicted intent label\n    \"\"\"\n    # Standard async OpenAI client - uses OPENAI_BASE_URL from environment\n    client = AsyncOpenAI(api_key=api_key) if api_key else AsyncOpenAI()\n    \n    user_msg = (\n        f\"Customer Query: {query}\\n\\n\"\n        f\"Available Intents:\\n{available_intents}\\n\\n\"\n        f\"Classify this query into one of the above banking intents using the tool call.\"\n    )\n    \n    response = await client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_msg},\n        ],\n        tools=[TOOL_SCHEMA],\n        tool_choice={\"type\": \"function\", \"function\": {\"name\": TOOL_NAME}},\n    )\n    \n    # Extract intent from tool call\n    tool_call = response.choices[0].message.tool_calls[0]\n    args = json.loads(tool_call.function.arguments)\n    return args[\"intent\"]\n\n\n# ============================================================================\n# PART 2: Dataset Loader\n# ============================================================================\n\nclass Banking77Dataset:\n    \"\"\"Lazy Hugging Face dataset loader for Banking77.\"\"\"\n    def __init__(self):\n        self._cache = {}\n        self._label_names = None\n\n    def _load_split(self, split: str):\n        if split not in self._cache:\n            ds = load_dataset(\"banking77\", split=split, trust_remote_code=False)\n            self._cache[split] = ds\n            if self._label_names is None and hasattr(ds.features.get(\"label\"), \"names\"):\n                self._label_names = ds.features[\"label\"].names\n        return self._cache[split]\n\n    def ensure_ready(self, splits):\n        for split in splits:\n            self._load_split(split)\n\n    def size(self, split: str) -> int:\n        return len(self._load_split(split))\n\n    def sample(self, *, split: str, index: int) -> dict:\n        ds = self._load_split(split)\n        idx = index % len(ds)\n        row = ds[idx]\n        label_idx = int(row.get(\"label\", 0))\n        label_text = self._label_names[label_idx] if self._label_names and label_idx < len(self._label_names) else f\"label_{label_idx}\"\n        return {\"index\": idx, \"split\": split, \"text\": str(row.get(\"text\", \"\")), \"label\": label_text}\n\n    @property\n    def label_names(self) -> list:\n        if self._label_names is None:\n            self._load_split(\"train\")\n        return self._label_names or []\n\n\n# ============================================================================\n# PART 3: Local API (thin Synth integration layer)\n# ============================================================================\n# This is the wrapper that:\n# 1. Sets OPENAI_BASE_URL to route calls through Synth's inference proxy\n# 2. Calls the clean classification pipeline\n# 3. Computes rewards\n# 4. Returns RolloutResponse (NO trace - Synth reconstructs it server-side)\n\ndef create_banking77_local_api(system_prompt: str, env_api_key: str):\n    \"\"\"Create a Banking77 local API.\n    \n    The classification pipeline is completely clean - no Synth logic.\n    Synth integration happens via OPENAI_BASE_URL routing.\n    \"\"\"\n    from fastapi import Request\n    \n    # Set ENVIRONMENT_API_KEY so SDK auth middleware can validate it\n    os.environ[\"ENVIRONMENT_API_KEY\"] = env_api_key\n    \n    # Initialize dataset\n    dataset = Banking77Dataset()\n    dataset.ensure_ready([\"train\", \"test\"])\n    \n    async def run_rollout(request: RolloutRequest, fastapi_request: Request) -> RolloutResponse:\n        \"\"\"Execute a rollout for the banking77 classification task.\"\"\"\n        split = request.env.config.get(\"split\", \"train\")\n        seed = request.env.seed\n        \n        # Get sample from dataset\n        sample = dataset.sample(split=split, index=seed)\n        \n        # Route OpenAI calls through Synth's inference proxy for trace reconstruction.\n        # The inference_url is a base URL in the format:\n        #   {interceptor_base}/v1/{trial_id}/{correlation_id}\n        # The OpenAI SDK automatically appends /chat/completions to this base URL.\n        os.environ[\"OPENAI_BASE_URL\"] = request.policy.config.get(\"inference_url\")\n        \n        # Extract API key from request (Synth provides this)\n        api_key = request.policy.config.get(\"api_key\")\n        \n        # Call the CORE PIPELINE (no Synth logic inside)\n        predicted_intent = await classify_banking77_query(\n            query=sample[\"text\"],\n            system_prompt=system_prompt,\n            available_intents=format_available_intents(dataset.label_names),\n            model=request.policy.config.get(\"model\", \"gpt-4o-mini\"),\n            api_key=api_key,\n        )\n        \n        # Compute reward (simple exact match)\n        expected_intent = sample[\"label\"]\n        is_correct = (\n            predicted_intent.lower().replace(\"_\", \" \").strip() \n            == expected_intent.lower().replace(\"_\", \" \").strip()\n        )\n        reward = 1.0 if is_correct else 0.0\n        \n        # Return just metrics - NO trace (Synth reconstructs from intercepted calls)\n        return RolloutResponse(\n            run_id=request.run_id,\n            metrics=RolloutMetrics(outcome_reward=reward),\n            trace=None,  # Synth server reconstructs traces from inference proxy calls\n            trace_correlation_id=request.policy.config.get(\"trace_correlation_id\"),\n        )\n    \n    def provide_taskset_description():\n        \"\"\"Returns static metadata about the dataset.\"\"\"\n        return {\n            \"splits\": [\"train\", \"test\"],\n            \"sizes\": {\"train\": dataset.size(\"train\"), \"test\": dataset.size(\"test\")},\n        }\n    \n    def provide_task_instances(seeds):\n        \"\"\"Returns per-seed task metadata.\"\"\"\n        for seed in seeds:\n            sample = dataset.sample(split=\"train\", index=seed)\n            yield TaskInfo(\n                task={\"id\": APP_ID, \"name\": APP_NAME},\n                dataset={\"id\": APP_ID, \"split\": sample[\"split\"], \"index\": sample[\"index\"]},\n                inference={\"tool\": TOOL_NAME},\n                limits={\"max_turns\": 1},\n                task_metadata={\"query\": sample[\"text\"], \"expected_intent\": sample[\"label\"]},\n            )\n    \n    return create_local_api(LocalAPIConfig(\n        app_id=APP_ID,\n        name=APP_NAME,\n        description=f\"{APP_NAME} local API for classifying customer queries into banking intents.\",\n        provide_taskset_description=provide_taskset_description,\n        provide_task_instances=provide_task_instances,\n        rollout=run_rollout,\n        cors_origins=[\"*\"],\n    ))\n\nprint('Banking77 local API defined')\nprint('  - classify_banking77_query(): Core pipeline (async), no Synth logic')\nprint('  - create_banking77_local_api(): Thin wrapper that routes calls via OPENAI_BASE_URL')"
  },
  {
   "cell_type": "markdown",
   "id": "y0l3voghzj",
   "metadata": {},
   "source": [
    "## Step 5: Start Baseline Local API\n",
    "\n",
    "Now we'll spin up the baseline local API and expose it via a Cloudflare tunnel. The SDK provides `TunneledLocalAPI` for clean one-liner tunnel setup that handles rotation, cloudflared process management, and DNS verification automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Start Baseline Local API with Cloudflare Tunnel\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Import tunnel helpers from SDK - TunneledLocalAPI handles rotation, cloudflared, and DNS verification\n",
    "from synth_ai.sdk.tunnels import TunneledLocalAPI, TunnelBackend, kill_port, wait_for_health_check\n",
    "from synth_ai.sdk.task import run_server_background\n",
    "\n",
    "BASELINE_SYSTEM_PROMPT = \"You are an expert banking assistant that classifies customer queries into banking intents. Given a customer message, respond with exactly one intent label from the provided list using the `banking77_classify` tool.\"\n",
    "USER_PROMPT = \"Customer Query: {query}\\n\\nAvailable Intents:\\n{available_intents}\\n\\nClassify this query into one of the above banking intents using the tool call.\"\n",
    "\n",
    "# Create baseline local API using SDK abstractions\n",
    "baseline_app = create_banking77_local_api(BASELINE_SYSTEM_PROMPT, ENVIRONMENT_API_KEY)\n",
    "\n",
    "# Kill port if in use, start server in background\n",
    "kill_port(LOCAL_API_PORT)\n",
    "run_server_background(baseline_app, LOCAL_API_PORT)\n",
    "\n",
    "# Wait for local health check\n",
    "print(f'Waiting for baseline local API on port {LOCAL_API_PORT}...')\n",
    "await wait_for_health_check(\"localhost\", LOCAL_API_PORT, ENVIRONMENT_API_KEY, timeout=30.0)\n",
    "print('Baseline local API ready!')\n",
    "\n",
    "# Create managed tunnel - handles rotation, cloudflared process, and DNS verification\n",
    "print('\\nProvisioning Cloudflare tunnel for baseline...')\n",
    "baseline_tunnel = await TunneledLocalAPI.create(\n",
    "    local_port=LOCAL_API_PORT,\n",
    "    backend=TunnelBackend.CloudflareManagedTunnel,\n",
    "    api_key=API_KEY,\n",
    "    env_api_key=ENVIRONMENT_API_KEY,\n",
    "    reason=\"baseline_notebook\",\n",
    "    backend_url=SYNTH_API_BASE,\n",
    "    progress=True,\n",
    ")\n",
    "BASELINE_LOCAL_API_URL = baseline_tunnel.url\n",
    "\n",
    "print(f'\\nBaseline local API URL: {BASELINE_LOCAL_API_URL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "btowuuijz1w",
   "metadata": {},
   "source": [
    "## Step 6: Run GEPA Optimization\n",
    "\n",
    "This kicks off the GEPA prompt optimization job on the Synth backend. GEPA will evolve the system prompt over multiple generations, evaluating each candidate against the training seeds via our tunneled local API. Because evolutionary prompt optimization requires multiple sequential rounds of evals, expect the process to take roughly as long as (time to eval) x (number of generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Run GEPA Optimization (using SDK)\n",
    "from synth_ai.sdk.api.train.prompt_learning import PromptLearningJob\n",
    "\n",
    "def run_gepa():\n",
    "    config_body = {\n",
    "        'prompt_learning': {\n",
    "            'algorithm': 'gepa',\n",
    "            'run_local': False,  # Run on remote backend\n",
    "            'local_api_url': BASELINE_LOCAL_API_URL,\n",
    "            'local_api_key': ENVIRONMENT_API_KEY,\n",
    "            'env_name': 'banking77',\n",
    "            'initial_prompt': {\n",
    "                'messages': [\n",
    "                    {'role': 'system', 'order': 0, 'pattern': BASELINE_SYSTEM_PROMPT},\n",
    "                    {'role': 'user', 'order': 1, 'pattern': USER_PROMPT},\n",
    "                ],\n",
    "                'wildcards': {'query': 'REQUIRED', 'available_intents': 'OPTIONAL'},\n",
    "            },\n",
    "            'policy': {'model': 'gpt-4.1-nano', 'provider': 'openai', 'temperature': 0.0, 'max_completion_tokens': 256},\n",
    "            'gepa': {\n",
    "                'env_name': 'banking77',\n",
    "                'evaluation': {'seeds': list(range(30)), 'validation_seeds': list(range(50, 56))},\n",
    "                'rollout': {'budget': 50, 'max_concurrent': 5, 'minibatch_size': 5},\n",
    "                'mutation': {'rate': 0.3, 'llm_model': 'gpt-4.1-nano'},\n",
    "                'population': {'initial_size': 3, 'num_generations': 2, 'children_per_generation': 2},\n",
    "                'archive': {'size': 5, 'pareto_set_size': 10},\n",
    "                'token': {'counting_model': 'gpt-4'},\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(f'Creating GEPA job (local_api_url={BASELINE_LOCAL_API_URL})...')\n",
    "    \n",
    "    # Use SDK instead of raw httpx - both file-based and dict-based configs\n",
    "    # go through the same PromptLearningConfig Pydantic validation\n",
    "    pl_job = PromptLearningJob.from_dict(\n",
    "        config_dict=config_body,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        api_key=API_KEY,\n",
    "        local_api_key=ENVIRONMENT_API_KEY,\n",
    "        skip_health_check=True,  # Tunnel DNS may not have propagated yet\n",
    "    )\n",
    "    \n",
    "    job_id = pl_job.submit()\n",
    "    print(f'Job ID: {job_id}')\n",
    "\n",
    "    # SDK handles polling with built-in progress printing\n",
    "    # Returns typed PromptLearningResult with .succeeded, .best_score, etc.\n",
    "    result = pl_job.poll_until_complete(timeout=3600.0, interval=3.0, progress=True)\n",
    "    \n",
    "    print(f'\\nFINAL: {result.status.value}')\n",
    "    \n",
    "    if result.succeeded:\n",
    "        print(f'BEST SCORE: {result.best_score}')\n",
    "    elif result.failed:\n",
    "        print(f'ERROR: {result.error}')\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Sync function - no await needed\n",
    "result = run_gepa()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fgaj2jzfvr",
   "metadata": {},
   "source": [
    "## Step 7: Evaluation\n",
    "\n",
    "Now, we'll run an evaluation for both the baseline prompt we started with, and the optimized prompt. This will be on a separate, but still randomly drawn dataset, so it will tell us if the optimizer in fact improved the prompt generally, or if it overfit to the trainset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Run Formal Eval Jobs (Baseline vs Optimized)\n",
    "from synth_ai.sdk.api.eval import EvalJob, EvalJobConfig, EvalResult\n",
    "from synth_ai.sdk.learning.prompt_learning_client import PromptLearningClient\n",
    "from synth_ai.sdk.tunnels import TunneledLocalAPI, TunnelBackend, kill_port, wait_for_health_check\n",
    "from synth_ai.sdk.task import run_server_background\n",
    "\n",
    "EVAL_SEEDS = list(range(100, 120))  # 20 held-out test samples\n",
    "\n",
    "def run_eval_job(local_api_url: str, local_api_key: str, seeds: list[int], mode: str) -> EvalResult:\n",
    "    \"\"\"Run an eval job using the SDK and wait for completion.\"\"\"\n",
    "    config = EvalJobConfig(\n",
    "        local_api_url=local_api_url,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        api_key=API_KEY,\n",
    "        local_api_key=local_api_key,\n",
    "        env_name='banking77',\n",
    "        seeds=seeds,\n",
    "        policy_config={'model': 'gpt-4.1-nano', 'provider': 'openai'},\n",
    "        env_config={'split': 'test'},\n",
    "        concurrency=10,\n",
    "    )\n",
    "\n",
    "    job = EvalJob(config)\n",
    "    job_id = job.submit()\n",
    "    print(f'  {mode} eval job: {job_id}')\n",
    "\n",
    "    # SDK handles polling with built-in progress printing\n",
    "    # Returns typed EvalResult with .succeeded, .mean_score, etc.\n",
    "    return job.poll_until_complete(timeout=600.0, interval=2.0, progress=True)\n",
    "\n",
    "\n",
    "def extract_system_prompt(prompt_results) -> str:\n",
    "    \"\"\"Extract system prompt from the best optimized prompt.\"\"\"\n",
    "    sections = prompt_results.top_prompts[0]['template']['sections']\n",
    "    return next(s['content'] for s in sections if s['role'] == 'system')\n",
    "\n",
    "\n",
    "# Use typed result from cell-8 (PromptLearningResult with .succeeded, .job_id, etc.)\n",
    "if result.succeeded:\n",
    "    print(\"GEPA Job Succeeded!\\n\")\n",
    "    \n",
    "    # Use PromptLearningClient to retrieve the optimized prompt\n",
    "    pl_client = PromptLearningClient(SYNTH_API_BASE, API_KEY)\n",
    "    prompt_results = await pl_client.get_prompts(result.job_id)\n",
    "    \n",
    "    # Extract optimized system prompt from top_prompts\n",
    "    optimized_system = extract_system_prompt(prompt_results)\n",
    "    best_train_reward = prompt_results.best_score\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('BASELINE SYSTEM PROMPT')\n",
    "    print('=' * 60)\n",
    "    print(BASELINE_SYSTEM_PROMPT)\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print('OPTIMIZED SYSTEM PROMPT (from GEPA)')\n",
    "    print('=' * 60)\n",
    "    print(optimized_system[:800] + \"...\" if len(optimized_system) > 800 else optimized_system)\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print('GEPA TRAINING RESULTS')\n",
    "    print('=' * 60)\n",
    "    print(f\"Best Train Reward: {best_train_reward:.1%}\")\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print(f'FORMAL EVAL JOBS (test split, seeds {EVAL_SEEDS[0]}-{EVAL_SEEDS[-1]})')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Start optimized local API on different port using SDK abstractions\n",
    "    print(f'\\nStarting optimized local API on port {OPTIMIZED_LOCAL_API_PORT}...')\n",
    "    optimized_app = create_banking77_local_api(optimized_system, ENVIRONMENT_API_KEY)\n",
    "    \n",
    "    kill_port(OPTIMIZED_LOCAL_API_PORT)\n",
    "    run_server_background(optimized_app, OPTIMIZED_LOCAL_API_PORT)\n",
    "    await wait_for_health_check(\"localhost\", OPTIMIZED_LOCAL_API_PORT, ENVIRONMENT_API_KEY, timeout=30.0)\n",
    "    print('Optimized local API ready!')\n",
    "    \n",
    "    # Create managed tunnel - TunneledLocalAPI handles rotation, cloudflared, and DNS verification\n",
    "    print('\\nProvisioning Cloudflare tunnel for optimized...')\n",
    "    optimized_tunnel = await TunneledLocalAPI.create(\n",
    "        local_port=OPTIMIZED_LOCAL_API_PORT,\n",
    "        backend=TunnelBackend.CloudflareManagedTunnel,\n",
    "        api_key=API_KEY,\n",
    "        env_api_key=ENVIRONMENT_API_KEY,\n",
    "        reason=\"optimized_notebook\",\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        progress=True,\n",
    "    )\n",
    "    OPTIMIZED_LOCAL_API_URL = optimized_tunnel.url\n",
    "    \n",
    "    # Run baseline eval - SDK handles progress printing\n",
    "    print('\\nRunning BASELINE eval job...')\n",
    "    baseline_result = run_eval_job(\n",
    "        local_api_url=BASELINE_LOCAL_API_URL,\n",
    "        local_api_key=ENVIRONMENT_API_KEY,\n",
    "        seeds=EVAL_SEEDS,\n",
    "        mode='baseline'\n",
    "    )\n",
    "    \n",
    "    if baseline_result.succeeded:\n",
    "        print(f'  Baseline eval reward: {baseline_result.mean_score:.1%}')\n",
    "    else:\n",
    "        print(f'  Baseline eval failed: {baseline_result.error}')\n",
    "    \n",
    "    # Run optimized eval - SDK handles progress printing\n",
    "    print('\\nRunning OPTIMIZED eval job...')\n",
    "    optimized_result = run_eval_job(\n",
    "        local_api_url=OPTIMIZED_LOCAL_API_URL,\n",
    "        local_api_key=ENVIRONMENT_API_KEY,\n",
    "        seeds=EVAL_SEEDS,\n",
    "        mode='optimized'\n",
    "    )\n",
    "    \n",
    "    if optimized_result.succeeded:\n",
    "        print(f'  Optimized eval reward: {optimized_result.mean_score:.1%}')\n",
    "    else:\n",
    "        print(f'  Optimized eval failed: {optimized_result.error}')\n",
    "    \n",
    "    # Final comparison (only if both succeeded)\n",
    "    if baseline_result.succeeded and optimized_result.succeeded:\n",
    "        print('\\n' + '=' * 60)\n",
    "        print('FINAL COMPARISON')\n",
    "        print('=' * 60)\n",
    "        print(f\"Training:\")\n",
    "        print(f\"  Best Train Reward: {best_train_reward:.1%}\")\n",
    "        \n",
    "        print(f\"\\nEval (seeds {EVAL_SEEDS[0]}-{EVAL_SEEDS[-1]}, held-out):\")\n",
    "        print(f\"  Baseline Reward:  {baseline_result.mean_score:.1%}\")\n",
    "        print(f\"  Optimized Reward: {optimized_result.mean_score:.1%}\")\n",
    "        \n",
    "        eval_lift = optimized_result.mean_score - baseline_result.mean_score\n",
    "        print(f\"  Lift:             {eval_lift:+.1%}\")\n",
    "        \n",
    "        if eval_lift > 0:\n",
    "            print(\"\\n>>> OPTIMIZATION GENERALIZES TO HELD-OUT DATA!\")\n",
    "        elif eval_lift == 0:\n",
    "            print(\"\\n=== Same performance on held-out data\")\n",
    "        else:\n",
    "            print(\"\\n<<< Baseline better on held-out (possible overfitting)\")\n",
    "else:\n",
    "    print(f\"Job failed: {result.status.value}\")\n",
    "    if result.error:\n",
    "        print(f\"Error: {result.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0smmbd65001",
   "metadata": {},
   "source": [
    "## Step 8: Cleanup\n",
    "\n",
    "Terminate cloudflared tunnel processes to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Cleanup\n",
    "from synth_ai.sdk.tunnels import cleanup_all\n",
    "\n",
    "print('Cleaning up cloudflared processes...')\n",
    "cleanup_all()\n",
    "print('Demo complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}