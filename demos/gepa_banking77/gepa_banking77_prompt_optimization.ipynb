{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Synth GEPA Demo - Banking77\n",
    "\n",
    "Prompt optimization using Synth's GEPA algorithm on the Banking77 intent classification task.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/synth-laboratories/synth-ai/blob/main/demos/gepa_banking77/gepa_banking77_prompt_optimization.ipynb)\n",
    "\n",
    "**Structure:**\n",
    "1. **Setup** - Install dependencies and configure\n",
    "2. **Task Definition** - Banking77 classification task\n",
    "3. **Local API** - Expose the task for optimization\n",
    "4. **Optimize** - Run GEPA to discover better prompts\n",
    "5. **Evaluate** - Formal eval on held-out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Install dependencies (Colab only)\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    import os\n",
    "\n",
    "    _INSTALLED_MARKER = \"/content/.synth_deps_v2\"\n",
    "\n",
    "    if os.path.exists(_INSTALLED_MARKER):\n",
    "        print(\"Dependencies ready.\")\n",
    "    else:\n",
    "        print(\"Installing dependencies...\")\n",
    "        \n",
    "        # Simple install - pip handles versioning\n",
    "        !pip install -q \"synth-ai>=0.6.3\" httpx fastapi uvicorn datasets nest_asyncio\n",
    "\n",
    "        # Install cloudflared\n",
    "        !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O /usr/local/bin/cloudflared 2>/dev/null\n",
    "        !chmod +x /usr/local/bin/cloudflared\n",
    "\n",
    "        with open(_INSTALLED_MARKER, 'w') as f:\n",
    "            f.write(\"ok\")\n",
    "\n",
    "        print(\"Done! Restarting runtime...\")\n",
    "        os.kill(os.getpid(), 9)\n",
    "else:\n",
    "    print(\"Not in Colab - assuming dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setup - All imports, config, and API keys\n",
    "import os, sys, json, asyncio\n",
    "import httpx\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from datasets import load_dataset\n",
    "from openai import AsyncOpenAI\n",
    "from synth_ai.core.utils.env import mint_demo_api_key\n",
    "from synth_ai.core.utils.urls import BACKEND_URL_BASE\n",
    "\n",
    "# Production backend\n",
    "SYNTH_API_BASE = BACKEND_URL_BASE\n",
    "# Ports are optional - will auto-find available ports if not specified\n",
    "LOCAL_API_PORT = 8001  # Optional: specify a port, or None to auto-select\n",
    "OPTIMIZED_LOCAL_API_PORT = 8002  # Optional: specify a port, or None to auto-select\n",
    "\n",
    "# Always mint a demo key for this notebook\n",
    "print(\"\\nMinting demo SYNTH_API_KEY for this demo...\")\n",
    "API_KEY = mint_demo_api_key()\n",
    "print(f\"Demo API Key: {API_KEY[:25]}...\")\n",
    "\n",
    "# Set API key in environment for SDK to use\n",
    "os.environ[\"SYNTH_API_KEY\"] = API_KEY\n",
    "\n",
    "# Synth inference URL - all LLM calls go through Synth's hosted inference\n",
    "# Uses the inference proxy endpoint (OpenAI-compatible)\n",
    "SYNTH_INFERENCE_URL = f\"{SYNTH_API_BASE}/api/inference/v1\"\n",
    "print(f\"\\nUsing Synth hosted inference: {SYNTH_INFERENCE_URL}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SETUP COMPLETE\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Step 2: Task Definition\n",
    "\n",
    "Banking77 is an intent classification task with 77 possible intents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BANKING77_LABELS = [\n",
    "    \"activate_my_card\",\n",
    "    \"age_limit\",\n",
    "    \"apple_pay_or_google_pay\",\n",
    "    \"atm_support\",\n",
    "    \"automatic_top_up\",\n",
    "    \"balance_not_updated_after_bank_transfer\",\n",
    "    \"balance_not_updated_after_cheque_or_cash_deposit\",\n",
    "    \"beneficiary_not_allowed\",\n",
    "    \"cancel_transfer\",\n",
    "    \"card_about_to_expire\",\n",
    "    \"card_acceptance\",\n",
    "    \"card_arrival\",\n",
    "    \"card_delivery_estimate\",\n",
    "    \"card_linking\",\n",
    "    \"card_not_working\",\n",
    "    \"card_payment_fee_charged\",\n",
    "    \"card_payment_not_recognised\",\n",
    "    \"card_payment_wrong_exchange_rate\",\n",
    "    \"card_swallowed\",\n",
    "    \"cash_withdrawal_charge\",\n",
    "    \"cash_withdrawal_not_recognised\",\n",
    "    \"change_pin\",\n",
    "    \"compromised_card\",\n",
    "    \"contactless_not_working\",\n",
    "    \"country_support\",\n",
    "    \"declined_card_payment\",\n",
    "    \"declined_cash_withdrawal\",\n",
    "    \"declined_transfer\",\n",
    "    \"direct_debit_payment_not_recognised\",\n",
    "    \"disposable_card_limits\",\n",
    "    \"edit_personal_details\",\n",
    "    \"exchange_charge\",\n",
    "    \"exchange_rate\",\n",
    "    \"exchange_via_app\",\n",
    "    \"extra_charge_on_statement\",\n",
    "    \"failed_transfer\",\n",
    "    \"fiat_currency_support\",\n",
    "    \"get_disposable_virtual_card\",\n",
    "    \"get_physical_card\",\n",
    "    \"getting_spare_card\",\n",
    "    \"getting_virtual_card\",\n",
    "    \"lost_or_stolen_card\",\n",
    "    \"lost_or_stolen_phone\",\n",
    "    \"order_physical_card\",\n",
    "    \"passcode_forgotten\",\n",
    "    \"pending_card_payment\",\n",
    "    \"pending_cash_withdrawal\",\n",
    "    \"pending_top_up\",\n",
    "    \"pending_transfer\",\n",
    "    \"pin_blocked\",\n",
    "    \"receiving_money\",\n",
    "    \"Refund_not_showing_up\",\n",
    "    \"request_refund\",\n",
    "    \"reverted_card_payment?\",\n",
    "    \"supported_cards_and_currencies\",\n",
    "    \"terminate_account\",\n",
    "    \"top_up_by_bank_transfer_charge\",\n",
    "    \"top_up_by_card_charge\",\n",
    "    \"top_up_by_cash_or_cheque\",\n",
    "    \"top_up_failed\",\n",
    "    \"top_up_limits\",\n",
    "    \"top_up_reverted\",\n",
    "    \"topping_up_by_card\",\n",
    "    \"transaction_charged_twice\",\n",
    "    \"transfer_fee_charged\",\n",
    "    \"transfer_into_account\",\n",
    "    \"transfer_not_received_by_recipient\",\n",
    "    \"transfer_timing\",\n",
    "    \"unable_to_verify_identity\",\n",
    "    \"verify_my_identity\",\n",
    "    \"verify_source_of_funds\",\n",
    "    \"verify_top_up\",\n",
    "    \"virtual_card_not_working\",\n",
    "    \"visa_or_mastercard\",\n",
    "    \"why_verify_identity\",\n",
    "    \"wrong_amount_of_cash_received\",\n",
    "    \"wrong_exchange_rate_for_cash_withdrawal\",\n",
    "]\n",
    "\n",
    "TOOL_NAME = \"banking77_classify\"\n",
    "TOOL_SCHEMA = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": TOOL_NAME,\n",
    "        \"description\": \"Return the predicted banking77 intent label.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"intent\": {\"type\": \"string\"}},\n",
    "            \"required\": [\"intent\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def format_available_intents(label_names: list) -> str:\n",
    "    \"\"\"Format the list of available intents for the prompt.\"\"\"\n",
    "    return \"\\n\".join(f\"{i + 1}. {l}\" for i, l in enumerate(label_names))\n",
    "\n",
    "\n",
    "async def classify_banking77_query(\n",
    "    query: str,\n",
    "    system_prompt: str,\n",
    "    available_intents: str | None = None,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    api_key: str | None = None,\n",
    "    inference_url: str | None = None,\n",
    ") -> str:\n",
    "    \"\"\"Classify a banking query into an intent.\n",
    "\n",
    "    Args:\n",
    "        query: The customer query to classify\n",
    "        system_prompt: System prompt for the model\n",
    "        available_intents: Formatted list of available intents\n",
    "        model: Model to use (e.g., \"gpt-4o-mini\")\n",
    "        api_key: API key for authentication\n",
    "        inference_url: Inference URL (interceptor URL during optimization, regular URL otherwise)\n",
    "\n",
    "    Returns:\n",
    "        The predicted intent label\n",
    "    \"\"\"\n",
    "    if available_intents is None:\n",
    "        available_intents = format_available_intents(BANKING77_LABELS)\n",
    "\n",
    "    user_msg = (\n",
    "        f\"Customer Query: {query}\\n\\n\"\n",
    "        f\"Available Intents:\\n{available_intents}\\n\\n\"\n",
    "        f\"Classify this query into one of the above banking intents using the tool call.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_msg},\n",
    "    ]\n",
    "\n",
    "    if inference_url:\n",
    "        # Use interceptor URL - pass Synth API key via X-API-Key header\n",
    "        default_headers = {\"X-API-Key\": api_key} if api_key else {}\n",
    "        client = AsyncOpenAI(\n",
    "            base_url=inference_url,\n",
    "            api_key=\"synth-interceptor\",  # Dummy - interceptor uses its own key\n",
    "            default_headers=default_headers,\n",
    "        )\n",
    "    else:\n",
    "        # Fallback to Synth's hosted inference for standalone usage\n",
    "        client = AsyncOpenAI(\n",
    "            base_url=SYNTH_INFERENCE_URL,\n",
    "            api_key=API_KEY,\n",
    "        )\n",
    "\n",
    "    response = await client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=[TOOL_SCHEMA],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": TOOL_NAME}},\n",
    "    )\n",
    "\n",
    "    tool_call = response.choices[0].message.tool_calls[0]\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "    return args[\"intent\"]\n",
    "\n",
    "\n",
    "# Load Banking77 from source CSV (HuggingFace dataset scripts no longer supported)\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=\"https://raw.githubusercontent.com/PolyAI-LDN/task-specific-datasets/master/banking_data/test.csv\",\n",
    "    split=\"train\"\n",
    ")\n",
    "# CSV has 'category' column (string labels) instead of 'label' (int)\n",
    "label_names = sorted(set(dataset[\"category\"]))\n",
    "print(f\"Loaded {len(dataset)} test samples with {len(label_names)} intent labels\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"BUSINESS LOGIC READY\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nclassify_banking77_query(query, system_prompt) -> intent\")\n",
    "print(\"\\nThis is the core app. Now let's see how prompts affect performance...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Step 3: Before/After Preview\n",
    "\n",
    "Compare a **baseline prompt** (78%) vs an **optimized prompt** (92%) on 50 test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Before/After Comparison\n",
    "#\n",
    "# Compare baseline vs optimized prompts on 50 test samples.\n",
    "# The optimized prompt was discovered by GEPA - it achieves ~92% vs ~78% baseline.\n",
    "# Uses Synth's hosted inference - no OPENAI_API_KEY needed!\n",
    "\n",
    "BASELINE_SYSTEM_PROMPT = \"\"\"You are an expert banking assistant that classifies customer queries into banking intents. Given a customer message, respond with exactly one intent label from the provided list using the `banking77_classify` tool.\"\"\"\n",
    "\n",
    "# This optimized prompt was discovered by GEPA - it adds classification strategy and key distinctions\n",
    "OPTIMIZED_SYSTEM_PROMPT = \"\"\"You are a precise banking intent classifier. Analyze customer queries and classify them into exactly one of the 77 predefined banking intents.\n",
    "\n",
    "Classification Strategy:\n",
    "1. IDENTIFY THE PRIMARY ACTION: What does the customer want to DO? (activate, cancel, check, transfer, verify, etc.)\n",
    "2. IDENTIFY THE SUBJECT: What is it about? (card, transfer, payment, account, etc.)\n",
    "3. IDENTIFY THE STATE: Is it about something pending, failed, declined, or completed?\n",
    "\n",
    "Key Intent Distinctions:\n",
    "- \"card_arrival\" vs \"card_delivery_estimate\": Both about card delivery. Use \"card_arrival\" for \"where is my card?\" and \"card_delivery_estimate\" for \"how long will it take?\"\n",
    "- \"get_physical_card\" vs \"order_physical_card\": Use \"order_physical_card\" for placing an order, \"get_physical_card\" for asking HOW to get one\n",
    "- \"pending_*\" intents: Transaction is IN PROGRESS, not yet complete\n",
    "- \"failed_*\" or \"declined_*\" intents: Transaction was REJECTED\n",
    "- \"*_not_recognised\" intents: Customer doesn't recognize a transaction on their statement\n",
    "- \"verify_*\" intents: About verification/authentication processes\n",
    "- \"top_up_*\" intents: About adding money TO the account\n",
    "- \"transfer_*\" intents: About moving money between accounts\n",
    "\n",
    "Output the single most appropriate intent using the banking77_classify tool.\"\"\"\n",
    "\n",
    "# Test on 50 held-out samples\n",
    "TEST_INDICES = list(range(100, 150))\n",
    "\n",
    "async def score_prompt(system_prompt: str, indices: list[int], prompt_name: str) -> float:\n",
    "    \"\"\"Score a prompt on a set of test samples.\"\"\"\n",
    "    correct = 0\n",
    "    total = len(indices)\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        sample = dataset[idx]\n",
    "        query = sample[\"text\"]\n",
    "        expected = sample[\"category\"]  # CSV has string labels directly\n",
    "\n",
    "        predicted = await classify_banking77_query(\n",
    "            query=query,\n",
    "            system_prompt=system_prompt,\n",
    "            model=\"gpt-4o-mini\",\n",
    "        )\n",
    "\n",
    "        # Normalize for comparison\n",
    "        pred_norm = predicted.lower().replace(\"_\", \" \").strip()\n",
    "        exp_norm = expected.lower().replace(\"_\", \" \").strip()\n",
    "        is_correct = pred_norm == exp_norm\n",
    "\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"  {prompt_name}: {i + 1}/{total} done, {correct}/{i + 1} correct ({correct / (i + 1):.0%})\"\n",
    "            )\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "print(\n",
    "    f\"Testing on {len(TEST_INDICES)} samples (indices {TEST_INDICES[0]}-{TEST_INDICES[-1]})...\\n\"\n",
    ")\n",
    "\n",
    "print(\"Scoring BASELINE prompt...\")\n",
    "baseline_score = await score_prompt(BASELINE_SYSTEM_PROMPT, TEST_INDICES, \"Baseline\")\n",
    "\n",
    "print(\"\\nScoring OPTIMIZED prompt...\")\n",
    "optimized_score = await score_prompt(OPTIMIZED_SYSTEM_PROMPT, TEST_INDICES, \"Optimized\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BEFORE/AFTER COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBASELINE PROMPT:\")\n",
    "print(f'  \"{BASELINE_SYSTEM_PROMPT[:80]}...\"')\n",
    "print(\n",
    "    f\"  Accuracy: {baseline_score:.0%} ({int(baseline_score * len(TEST_INDICES))}/{len(TEST_INDICES)})\"\n",
    ")\n",
    "\n",
    "print(f\"\\nOPTIMIZED PROMPT (from GEPA):\")\n",
    "print(f'  \"{OPTIMIZED_SYSTEM_PROMPT[:80]}...\"')\n",
    "print(\n",
    "    f\"  Accuracy: {optimized_score:.0%} ({int(optimized_score * len(TEST_INDICES))}/{len(TEST_INDICES)})\"\n",
    ")\n",
    "\n",
    "lift = optimized_score - baseline_score\n",
    "print(f\"\\nLIFT: {lift:+.0%}\")\n",
    "\n",
    "if lift > 0:\n",
    "    print(\"\\n>>> Better prompts = better results!\")\n",
    "    print(\">>> Now let's see how Synth finds these optimized prompts...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Step 3: Local API\n",
    "\n",
    "Expose the task via HTTP so Synth can run optimization against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synth_ai.sdk.localapi import LocalAPIConfig, create_local_api\n",
    "from synth_ai.sdk.localapi._impl.contracts import RolloutMetrics, RolloutRequest, RolloutResponse, TaskInfo\n",
    "from synth_ai.sdk.localapi._impl.trace_correlation_helpers import extract_trace_correlation_id\n",
    "from synth_ai.core.tunnels import TunnelBackend, TunneledLocalAPI\n",
    "from synth_ai.data.enums import SuccessStatus\n",
    "\n",
    "APP_ID = \"banking77\"\n",
    "APP_NAME = \"Banking77 Intent Classification\"\n",
    "\n",
    "BASELINE_SYSTEM_PROMPT = \"\"\"You are an expert banking assistant that classifies customer queries into banking intents. Given a customer message, respond with exactly one intent label from the provided list using the `banking77_classify` tool.\"\"\"\n",
    "\n",
    "USER_PROMPT = \"Customer Query: {query}\\n\\nAvailable Intents:\\n{available_intents}\\n\\nClassify this query into one of the above banking intents using the tool call.\"\n",
    "\n",
    "\n",
    "class Banking77Dataset:\n",
    "    \"\"\"Lazy dataset loader for Banking77.\"\"\"\n",
    "    # Load directly from GitHub CSV (HuggingFace dataset scripts no longer supported)\n",
    "    _DATA_URLS = {\n",
    "        \"train\": \"https://raw.githubusercontent.com/PolyAI-LDN/task-specific-datasets/master/banking_data/train.csv\",\n",
    "        \"test\": \"https://raw.githubusercontent.com/PolyAI-LDN/task-specific-datasets/master/banking_data/test.csv\",\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self._cache = {}\n",
    "        self._label_names = None\n",
    "\n",
    "    def _load_split(self, split: str):\n",
    "        if split not in self._cache:\n",
    "            url = self._DATA_URLS.get(split)\n",
    "            if not url:\n",
    "                raise ValueError(f\"Unknown split: {split}. Available: {list(self._DATA_URLS.keys())}\")\n",
    "            ds = load_dataset(\"csv\", data_files=url, split=\"train\")\n",
    "            self._cache[split] = ds\n",
    "            # Build label names from unique categories\n",
    "            if self._label_names is None:\n",
    "                self._label_names = sorted(set(ds[\"category\"]))\n",
    "        return self._cache[split]\n",
    "\n",
    "    def ensure_ready(self, splits):\n",
    "        \"\"\"Ensure specified dataset splits are loaded.\"\"\"\n",
    "        for split in splits:\n",
    "            self._load_split(split)\n",
    "\n",
    "    def size(self, split: str) -> int:\n",
    "        return len(self._load_split(split))\n",
    "\n",
    "    def sample(self, *, split: str, index: int) -> dict:\n",
    "        ds = self._load_split(split)\n",
    "        idx = index % len(ds)\n",
    "        row = ds[idx]\n",
    "        # CSV has 'category' field with string labels\n",
    "        label_text = row.get(\"category\", \"unknown\")\n",
    "        return {\"index\": idx, \"split\": split, \"text\": str(row.get(\"text\", \"\")), \"label\": label_text}\n",
    "\n",
    "    @property\n",
    "    def label_names(self) -> list:\n",
    "        if self._label_names is None:\n",
    "            self._load_split(\"train\")\n",
    "        return self._label_names or []\n",
    "\n",
    "\n",
    "def create_banking77_local_api(system_prompt: str):\n",
    "    \"\"\"Create a Banking77 local API for optimization.\"\"\"\n",
    "\n",
    "    dataset = Banking77Dataset()\n",
    "    dataset.ensure_ready([\"train\", \"test\"])\n",
    "\n",
    "    async def run_rollout(request: RolloutRequest, fastapi_request) -> RolloutResponse:\n",
    "        split = request.env.config.get(\"split\", \"train\")\n",
    "        seed = request.env.seed\n",
    "        sample = dataset.sample(split=split, index=seed)\n",
    "\n",
    "        # Extract inference_url and api_key from policy config\n",
    "        # During GEPA optimization, inference_url points to the interceptor\n",
    "        policy_config = request.policy.config or {}\n",
    "        inference_url = policy_config.get(\"inference_url\")\n",
    "        api_key = policy_config.get(\"api_key\")\n",
    "\n",
    "        # Call the classifier with the interceptor URL\n",
    "        predicted_intent = await classify_banking77_query(\n",
    "            query=sample[\"text\"],\n",
    "            system_prompt=system_prompt,\n",
    "            available_intents=format_available_intents(dataset.label_names),\n",
    "            model=policy_config.get(\"model\", \"gpt-4o-mini\"),\n",
    "            api_key=api_key,\n",
    "            inference_url=inference_url,\n",
    "        )\n",
    "\n",
    "        expected_intent = sample[\"label\"]\n",
    "        is_correct = (\n",
    "            predicted_intent.lower().replace(\"_\", \" \").strip()\n",
    "            == expected_intent.lower().replace(\"_\", \" \").strip()\n",
    "        )\n",
    "        reward = 1.0 if is_correct else 0.0\n",
    "\n",
    "        # Extract trace correlation ID from policy config/inference_url\n",
    "        policy_cfg_for_trace = {\n",
    "            key: value\n",
    "            for key, value in policy_config.items()\n",
    "            if key not in {\"trace_correlation_id\", \"trace\"}\n",
    "        }\n",
    "        trace_correlation_id = extract_trace_correlation_id(\n",
    "            policy_config=policy_cfg_for_trace,\n",
    "            inference_url=str(inference_url or \"\"),\n",
    "        )\n",
    "\n",
    "        return RolloutResponse(\n",
    "            trace_correlation_id=trace_correlation_id,\n",
    "            reward_info=RolloutMetrics(outcome_reward=reward),\n",
    "            trace=None,\n",
    "            inference_url=str(inference_url or \"\"),\n",
    "            success_status=SuccessStatus.SUCCESS,\n",
    "        )\n",
    "\n",
    "    def provide_taskset_description():\n",
    "        return {\n",
    "            \"splits\": [\"train\", \"test\"],\n",
    "            \"sizes\": {\"train\": dataset.size(\"train\"), \"test\": dataset.size(\"test\")},\n",
    "        }\n",
    "\n",
    "    def provide_task_instances(seeds):\n",
    "        for seed in seeds:\n",
    "            sample = dataset.sample(split=\"train\", index=seed)\n",
    "            yield TaskInfo(\n",
    "                task={\"id\": APP_ID, \"name\": APP_NAME},\n",
    "                dataset={\"id\": APP_ID, \"split\": sample[\"split\"], \"index\": sample[\"index\"]},\n",
    "                inference={\"tool\": TOOL_NAME},\n",
    "                limits={\"max_turns\": 1},\n",
    "                task_metadata={\"query\": sample[\"text\"], \"expected_intent\": sample[\"label\"]},\n",
    "            )\n",
    "\n",
    "    return create_local_api(\n",
    "        LocalAPIConfig(\n",
    "            app_id=APP_ID,\n",
    "            name=APP_NAME,\n",
    "            description=f\"{APP_NAME} local API for classifying customer queries into banking intents.\",\n",
    "            provide_taskset_description=provide_taskset_description,\n",
    "            provide_task_instances=provide_task_instances,\n",
    "            rollout=run_rollout,\n",
    "            cors_origins=[\"*\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Starting local API...\")\n",
    "baseline_app = create_banking77_local_api(BASELINE_SYSTEM_PROMPT)\n",
    "\n",
    "# Create tunnel - handles server startup, health check, and tunnel creation automatically\n",
    "print(\"\\nStarting server and provisioning Cloudflare tunnel...\")\n",
    "baseline_tunnel = await TunneledLocalAPI.create_for_app(\n",
    "    app=baseline_app,\n",
    "    local_port=None,\n",
    "    backend=TunnelBackend.CloudflareQuickTunnel,\n",
    "    progress=True,\n",
    ")\n",
    "BASELINE_LOCAL_API_URL = baseline_tunnel.url\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(\"LOCAL API READY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"URL: {BASELINE_LOCAL_API_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step 4: Run GEPA\n",
    "\n",
    "GEPA evolves prompts over multiple generations, selecting the best performers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from synth_ai.sdk.optimization.internal.prompt_learning import PromptLearningJob\n",
    "\n",
    "# Banking77 train split has 10,003 samples - sample randomly for better coverage\n",
    "random.seed(42)\n",
    "DATASET_SIZE = 10003\n",
    "all_indices = list(range(DATASET_SIZE))\n",
    "random.shuffle(all_indices)\n",
    "\n",
    "# 100 train seeds, 200 validation seeds - randomly sampled, non-overlapping\n",
    "TRAIN_SEEDS = all_indices[:100]\n",
    "VALIDATION_SEEDS = all_indices[100:300]\n",
    "\n",
    "\n",
    "async def run_gepa():\n",
    "    config_body = {\n",
    "        \"prompt_learning\": {\n",
    "            \"algorithm\": \"gepa\",\n",
    "            \"task_app_url\": BASELINE_LOCAL_API_URL,\n",
    "            \"env_name\": \"banking77\",\n",
    "            \"initial_prompt\": {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"order\": 0, \"pattern\": BASELINE_SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"order\": 1, \"pattern\": USER_PROMPT},\n",
    "                ],\n",
    "                \"wildcards\": {\"query\": \"REQUIRED\", \"available_intents\": \"OPTIONAL\"},\n",
    "            },\n",
    "            \"policy\": {\n",
    "                \"model\": \"gpt-4.1-nano\",\n",
    "                \"provider\": \"openai\",\n",
    "                \"inference_mode\": \"synth_hosted\",\n",
    "                \"temperature\": 0.0,\n",
    "                \"max_completion_tokens\": 256,\n",
    "            },\n",
    "            \"gepa\": {\n",
    "                \"env_name\": \"banking77\",\n",
    "                \"evaluation\": {\n",
    "                    \"seeds\": TRAIN_SEEDS,  # 100 random training seeds\n",
    "                    \"validation_seeds\": VALIDATION_SEEDS,  # 200 random validation seeds\n",
    "                },\n",
    "                \"rollout\": {\n",
    "                    \"budget\": 300,  # Higher budget for more samples\n",
    "                    \"max_concurrent\": 32,  # High parallelism for speed\n",
    "                    \"minibatch_size\": 32,\n",
    "                },\n",
    "                \"proposer_effort\": \"MEDIUM\",\n",
    "                \"proposer_output_tokens\": \"FAST\",\n",
    "                \"mutation\": {\"rate\": 0.3},\n",
    "                \"population\": {\n",
    "                    \"initial_size\": 4,\n",
    "                    \"num_generations\": 3,\n",
    "                    \"children_per_generation\": 3,\n",
    "                },\n",
    "                \"archive\": {\"size\": 5, \"pareto_set_size\": 10},\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(f\"Creating GEPA job...\")\n",
    "    print(f\"  Train seeds: {len(TRAIN_SEEDS)} (randomly sampled)\")\n",
    "    print(f\"  Validation seeds: {len(VALIDATION_SEEDS)} (randomly sampled)\")\n",
    "    print(f\"  Parallelism: 32 concurrent rollouts\")\n",
    "\n",
    "    pl_job = PromptLearningJob.from_dict(\n",
    "        config_dict=config_body,\n",
    "        skip_health_check=True,\n",
    "    )\n",
    "\n",
    "    job_id = pl_job.submit()\n",
    "    print(f\"Job ID: {job_id}\")\n",
    "    print()\n",
    "\n",
    "    # Use streaming for real-time progress updates\n",
    "    result = await pl_job.stream_until_complete_async(timeout=3600.0)\n",
    "\n",
    "    print(f\"\\nFINAL: {result.status.value}\")\n",
    "\n",
    "    if result.succeeded:\n",
    "        print(f\"BEST SCORE: {result.best_score}\")\n",
    "    elif result.failed:\n",
    "        print(f\"ERROR: {result.error}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = await run_gepa()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate\n",
    "\n",
    "Compare baseline vs optimized prompts on held-out test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synth_ai.sdk.eval.job import EvalJob, EvalJobConfig, EvalResult\n",
    "\n",
    "EVAL_SEEDS = list(range(100, 150))  # Held-out test samples\n",
    "\n",
    "\n",
    "def run_eval_job(local_api_url: str, seeds: list[int], mode: str) -> EvalResult:\n",
    "    \"\"\"Run an eval job and wait for completion.\"\"\"\n",
    "    config = EvalJobConfig(\n",
    "        local_api_url=local_api_url,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        api_key=API_KEY,\n",
    "        env_name=\"banking77\",\n",
    "        seeds=seeds,\n",
    "        policy_config={\"model\": \"gpt-4.1-nano\", \"provider\": \"openai\"},\n",
    "        env_config={\"split\": \"test\"},\n",
    "        concurrency=10,\n",
    "    )\n",
    "    job = EvalJob(config)\n",
    "    job.submit()\n",
    "    return job.poll_until_complete(timeout=600.0, interval=2.0, progress=True)\n",
    "\n",
    "\n",
    "if result.succeeded:\n",
    "    # Get the optimized system prompt - one simple method call\n",
    "    optimized_prompt = result.get_system_prompt()\n",
    "    best_score = result.best_score\n",
    "\n",
    "    if optimized_prompt:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"OPTIMIZATION RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nBest Train Reward: {best_score:.1%}\")\n",
    "        print(f\"\\nOptimized Prompt:\")\n",
    "        print(optimized_prompt[:400] + \"...\" if len(optimized_prompt) > 400 else optimized_prompt)\n",
    "\n",
    "        # Create optimized API and run final evaluation\n",
    "        optimized_app = create_banking77_local_api(optimized_prompt)\n",
    "        optimized_tunnel = await TunneledLocalAPI.create_for_app(\n",
    "            app=optimized_app,\n",
    "            local_port=None,\n",
    "            backend=TunnelBackend.CloudflareQuickTunnel,\n",
    "            progress=True,\n",
    "        )\n",
    "        OPTIMIZED_LOCAL_API_URL = optimized_tunnel.url\n",
    "\n",
    "        baseline_result = run_eval_job(BASELINE_LOCAL_API_URL, EVAL_SEEDS, \"baseline\")\n",
    "        optimized_result = run_eval_job(OPTIMIZED_LOCAL_API_URL, EVAL_SEEDS, \"optimized\")\n",
    "\n",
    "        # Final results\n",
    "        if baseline_result.succeeded and optimized_result.succeeded:\n",
    "            eval_lift = optimized_result.mean_reward - baseline_result.mean_reward\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"FINAL EVALUATION\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"Training Score:  {best_score:.1%}\")\n",
    "            print(\n",
    "                f\"Held-Out Test:   {optimized_result.mean_reward:.1%} (baseline: {baseline_result.mean_reward:.1%})\"\n",
    "            )\n",
    "            print(f\"Improvement:     {eval_lift:+.1%}\")\n",
    "\n",
    "            if eval_lift > 0:\n",
    "                print(\"\\n✓ Optimization generalizes to held-out data!\")\n",
    "            elif eval_lift == 0:\n",
    "                print(\"\\n= Same performance on held-out data\")\n",
    "            else:\n",
    "                print(\"\\n⚠ Possible overfitting (baseline better on held-out)\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"OPTIMIZATION COMPLETED\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nBest Score: {result.best_score:.1%}\")\n",
    "        print(\"\\nNote: Could not retrieve the optimized prompt text.\")\n",
    "else:\n",
    "    print(f\"Optimization failed: {result.error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synth_ai.core.tunnels import cleanup_all\n",
    "\n",
    "print(\"Cleaning up cloudflared processes...\")\n",
    "cleanup_all()\n",
    "print(\"Demo complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synth-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
