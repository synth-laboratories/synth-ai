{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Synth GEPA Demo - Banking77 (Production)\n",
    "\n",
    "This notebook demonstrates prompt optimization using Synth's GEPA algorithm.\n",
    "\n",
    "**Banking77** is a task where an AI classifies customer service requests into one of 77 banking intents.\n",
    "\n",
    "**Run in Google Colab:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/synth-laboratories/synth-ai/blob/main/demos/gepa_banking77/gepa_banking77_prompt_optimization.ipynb)\n",
    "\n",
    "**Structure:**\n",
    "1. **Setup** - Install dependencies and configure API keys\n",
    "2. **Business Logic** - A simple Banking77 classification app\n",
    "3. **Before/After** - Preview: 78% baseline \u2192 92% optimized\n",
    "4. **Optimize** - Run GEPA to discover better prompts\n",
    "5. **Evaluate** - Formal eval on held-out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ku8p0wn7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Install dependencies (run this first on Colab)\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab - installing dependencies...\")\n",
    "    !pip install -q \"numpy<2\" synth-ai httpx fastapi uvicorn datasets nest_asyncio\n",
    "    \n",
    "    # Install cloudflared\n",
    "    !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O /usr/local/bin/cloudflared\n",
    "    !chmod +x /usr/local/bin/cloudflared\n",
    "    !cloudflared --version\n",
    "    \n",
    "    print(\"Dependencies installed!\")\n",
    "else:\n",
    "    print(\"Not in Colab - assuming dependencies are already installed\")\n",
    "    print(\"Required: pip install synth-ai httpx fastapi uvicorn datasets nest_asyncio\")\n",
    "    print(\"Required: brew install cloudflare/cloudflare/cloudflared (macOS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-md",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Configure all imports, API keys, and environment keys in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setup - All imports, config, and API keys\n",
    "import os, sys, json, asyncio\n",
    "import httpx\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from datasets import load_dataset\n",
    "from openai import AsyncOpenAI\n",
    "from synth_ai.core.env import PROD_BASE_URL, mint_demo_api_key\n",
    "\n",
    "# Production backend\n",
    "SYNTH_API_BASE = PROD_BASE_URL\n",
    "# Ports are optional - will auto-find available ports if not specified\n",
    "LOCAL_API_PORT = 8001  # Optional: specify a port, or None to auto-select\n",
    "OPTIMIZED_LOCAL_API_PORT = 8002  # Optional: specify a port, or None to auto-select\n",
    "\n",
    "# Get API Key (use env var or mint demo key)\n",
    "API_KEY = os.environ.get('SYNTH_API_KEY', '')\n",
    "if not API_KEY:\n",
    "    print('\\nNo SYNTH_API_KEY found, minting demo key...')\n",
    "    API_KEY = mint_demo_api_key()\n",
    "    print(f'Demo API Key: {API_KEY[:25]}...')\n",
    "else:\n",
    "    print(f'\\nUsing SYNTH_API_KEY: {API_KEY[:20]}...')\n",
    "\n",
    "# Set API keys in environment for SDK to use\n",
    "os.environ['SYNTH_API_KEY'] = API_KEY\n",
    "\n",
    "# Set OpenAI API key if available (optional - Step 3 will be skipped if not set)\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', '')\n",
    "if OPENAI_API_KEY:\n",
    "    os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "    print(f'\\nOpenAI API key found: {OPENAI_API_KEY[:20]}...')\n",
    "else:\n",
    "    print('\\nNo OPENAI_API_KEY found - Step 3 (preview) will be skipped')\n",
    "\n",
    "# Note: Environment API key is automatically handled by create_local_api()\n",
    "# No need to call ensure_localapi_auth() manually - it happens on app startup\n",
    "\n",
    "\n",
    "print('\\n' + '=' * 50)\n",
    "print('SETUP COMPLETE')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biz-logic-md",
   "metadata": {},
   "source": [
    "## Step 2: Business Logic - A Simple Banking77 Classifier\n",
    "\n",
    "Here's a simple prompt-based app that classifies customer queries into banking intents.\n",
    "\n",
    "This is **normal business logic** - just an async function that calls OpenAI. No Synth-specific code here. You could use this exact code in any application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-biz-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Business Logic - Banking77 Classification Pipeline\n",
    "#\n",
    "# This is a simple prompt app for Banking77 intent classification.\n",
    "# Run this cell to see the core business logic - no Synth dependencies.\n",
    "\n",
    "BANKING77_LABELS = [\n",
    "    \"activate_my_card\", \"age_limit\", \"apple_pay_or_google_pay\", \"atm_support\", \"automatic_top_up\",\n",
    "    \"balance_not_updated_after_bank_transfer\", \"balance_not_updated_after_cheque_or_cash_deposit\",\n",
    "    \"beneficiary_not_allowed\", \"cancel_transfer\", \"card_about_to_expire\", \"card_acceptance\",\n",
    "    \"card_arrival\", \"card_delivery_estimate\", \"card_linking\", \"card_not_working\",\n",
    "    \"card_payment_fee_charged\", \"card_payment_not_recognised\", \"card_payment_wrong_exchange_rate\",\n",
    "    \"card_swallowed\", \"cash_withdrawal_charge\", \"cash_withdrawal_not_recognised\", \"change_pin\",\n",
    "    \"compromised_card\", \"contactless_not_working\", \"country_support\", \"declined_card_payment\",\n",
    "    \"declined_cash_withdrawal\", \"declined_transfer\", \"direct_debit_payment_not_recognised\",\n",
    "    \"disposable_card_limits\", \"edit_personal_details\", \"exchange_charge\", \"exchange_rate\",\n",
    "    \"exchange_via_app\", \"extra_charge_on_statement\", \"failed_transfer\", \"fiat_currency_support\",\n",
    "    \"get_disposable_virtual_card\", \"get_physical_card\", \"getting_spare_card\", \"getting_virtual_card\",\n",
    "    \"lost_or_stolen_card\", \"lost_or_stolen_phone\", \"order_physical_card\", \"passcode_forgotten\",\n",
    "    \"pending_card_payment\", \"pending_cash_withdrawal\", \"pending_top_up\", \"pending_transfer\",\n",
    "    \"pin_blocked\", \"receiving_money\", \"Refund_not_showing_up\", \"request_refund\",\n",
    "    \"reverted_card_payment?\", \"supported_cards_and_currencies\", \"terminate_account\",\n",
    "    \"top_up_by_bank_transfer_charge\", \"top_up_by_card_charge\", \"top_up_by_cash_or_cheque\",\n",
    "    \"top_up_failed\", \"top_up_limits\", \"top_up_reverted\", \"topping_up_by_card\",\n",
    "    \"transaction_charged_twice\", \"transfer_fee_charged\", \"transfer_into_account\",\n",
    "    \"transfer_not_received_by_recipient\", \"transfer_timing\", \"unable_to_verify_identity\",\n",
    "    \"verify_my_identity\", \"verify_source_of_funds\", \"verify_top_up\", \"virtual_card_not_working\",\n",
    "    \"visa_or_mastercard\", \"why_verify_identity\", \"wrong_amount_of_cash_received\",\n",
    "    \"wrong_exchange_rate_for_cash_withdrawal\",\n",
    "]\n",
    "\n",
    "TOOL_NAME = \"banking77_classify\"\n",
    "TOOL_SCHEMA = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": TOOL_NAME,\n",
    "        \"description\": \"Return the predicted banking77 intent label.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"intent\": {\"type\": \"string\"}},\n",
    "            \"required\": [\"intent\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def format_available_intents(label_names: list) -> str:\n",
    "    \"\"\"Format the list of available intents for the prompt.\"\"\"\n",
    "    return \"\\n\".join(f\"{i+1}. {l}\" for i, l in enumerate(label_names))\n",
    "\n",
    "\n",
    "async def classify_banking77_query(\n",
    "    query: str,\n",
    "    system_prompt: str,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    ") -> str:\n",
    "    \"\"\"Classify a banking query into an intent using OpenAI.\n",
    "    \n",
    "    This is the CORE PIPELINE - clean async code with NO Synth-specific logic.\n",
    "    \n",
    "    Args:\n",
    "        query: The customer query to classify\n",
    "        system_prompt: System prompt for the model\n",
    "        model: Model to use (e.g., \"gpt-4o-mini\")\n",
    "    \n",
    "    Returns:\n",
    "        The predicted intent label\n",
    "    \"\"\"\n",
    "    client = AsyncOpenAI()  # Uses OPENAI_API_KEY from environment\n",
    "    available_intents = format_available_intents(BANKING77_LABELS)\n",
    "    \n",
    "    user_msg = (\n",
    "        f\"Customer Query: {query}\\n\\n\"\n",
    "        f\"Available Intents:\\n{available_intents}\\n\\n\"\n",
    "        f\"Classify this query into one of the above banking intents using the tool call.\"\n",
    "    )\n",
    "    \n",
    "    response = await client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ],\n",
    "        tools=[TOOL_SCHEMA],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": TOOL_NAME}},\n",
    "    )\n",
    "    \n",
    "    tool_call = response.choices[0].message.tool_calls[0]\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "    return args[\"intent\"]\n",
    "\n",
    "\n",
    "# Load Banking77 dataset\n",
    "print(\"Loading Banking77 dataset...\")\n",
    "dataset = load_dataset(\"banking77\", split=\"test\", trust_remote_code=False)\n",
    "label_names = dataset.features[\"label\"].names\n",
    "print(f\"Loaded {len(dataset)} test samples with {len(label_names)} intent labels\")\n",
    "\n",
    "print('\\n' + '=' * 50)\n",
    "print('BUSINESS LOGIC READY')\n",
    "print('=' * 50)\n",
    "print('\\nclassify_banking77_query(query, system_prompt) -> intent')\n",
    "print('\\nThis is the core app. Now let\\'s see how prompts affect performance...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "before-after-md",
   "metadata": {},
   "source": [
    "## Step 3: Before/After Preview\n",
    "\n",
    "Compare a **baseline prompt** (78%) vs an **optimized prompt** (92%) on 50 test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-before-after",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Before/After Comparison\n",
    "#\n",
    "# Compare baseline vs optimized prompts on 50 test samples.\n",
    "# The optimized prompt was discovered by GEPA - it achieves ~92% vs ~78% baseline.\n",
    "\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "if not OPENAI_API_KEY:\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\") or \"\"\n",
    "    except Exception:\n",
    "        OPENAI_API_KEY = \"\"\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY not set; skipping Step 3 eval.\")\n",
    "\n",
    "BASELINE_SYSTEM_PROMPT = \"\"\"You are an expert banking assistant that classifies customer queries into banking intents. Given a customer message, respond with exactly one intent label from the provided list using the `banking77_classify` tool.\"\"\"\n",
    "\n",
    "# This optimized prompt was discovered by GEPA - it adds classification strategy and key distinctions\n",
    "OPTIMIZED_SYSTEM_PROMPT = \"\"\"You are a precise banking intent classifier. Analyze customer queries and classify them into exactly one of the 77 predefined banking intents.\n",
    "\n",
    "Classification Strategy:\n",
    "1. IDENTIFY THE PRIMARY ACTION: What does the customer want to DO? (activate, cancel, check, transfer, verify, etc.)\n",
    "2. IDENTIFY THE SUBJECT: What is it about? (card, transfer, payment, account, etc.)\n",
    "3. IDENTIFY THE STATE: Is it about something pending, failed, declined, or completed?\n",
    "\n",
    "Key Intent Distinctions:\n",
    "- \"card_arrival\" vs \"card_delivery_estimate\": Both about card delivery. Use \"card_arrival\" for \"where is my card?\" and \"card_delivery_estimate\" for \"how long will it take?\"\n",
    "- \"get_physical_card\" vs \"order_physical_card\": Use \"order_physical_card\" for placing an order, \"get_physical_card\" for asking HOW to get one\n",
    "- \"pending_*\" intents: Transaction is IN PROGRESS, not yet complete\n",
    "- \"failed_*\" or \"declined_*\" intents: Transaction was REJECTED\n",
    "- \"*_not_recognised\" intents: Customer doesn't recognize a transaction on their statement\n",
    "- \"verify_*\" intents: About verification/authentication processes\n",
    "- \"top_up_*\" intents: About adding money TO the account\n",
    "- \"transfer_*\" intents: About moving money between accounts\n",
    "\n",
    "Output the single most appropriate intent using the banking77_classify tool.\"\"\"\n",
    "\n",
    "# Test on 50 held-out samples\n",
    "if OPENAI_API_KEY:\n",
    "    TEST_INDICES = list(range(100, 150))\n",
    "    \n",
    "    \n",
    "    async def score_prompt(system_prompt: str, indices: list[int], prompt_name: str) -> float:\n",
    "        \"\"\"Score a prompt on a set of test samples.\"\"\"\n",
    "        correct = 0\n",
    "        total = len(indices)\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            sample = dataset[idx]\n",
    "            query = sample[\"text\"]\n",
    "            expected = label_names[sample[\"label\"]]\n",
    "            \n",
    "            predicted = await classify_banking77_query(\n",
    "                query=query,\n",
    "                system_prompt=system_prompt,\n",
    "                model=\"gpt-4o-mini\",\n",
    "            )\n",
    "            \n",
    "            # Normalize for comparison\n",
    "            pred_norm = predicted.lower().replace(\"_\", \" \").strip()\n",
    "            exp_norm = expected.lower().replace(\"_\", \" \").strip()\n",
    "            is_correct = pred_norm == exp_norm\n",
    "            \n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f'  {prompt_name}: {i+1}/{total} done, {correct}/{i+1} correct ({correct/(i+1):.0%})')\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    print(f'Testing on {len(TEST_INDICES)} samples (indices {TEST_INDICES[0]}-{TEST_INDICES[-1]})...\\n')\n",
    "    \n",
    "    print('Scoring BASELINE prompt...')\n",
    "    baseline_score = await score_prompt(BASELINE_SYSTEM_PROMPT, TEST_INDICES, \"Baseline\")\n",
    "    \n",
    "    print('\\nScoring OPTIMIZED prompt...')\n",
    "    optimized_score = await score_prompt(OPTIMIZED_SYSTEM_PROMPT, TEST_INDICES, \"Optimized\")\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print('BEFORE/AFTER COMPARISON')\n",
    "    print('=' * 60)\n",
    "    print(f'\\nBASELINE PROMPT:')\n",
    "    print(f'  \"{BASELINE_SYSTEM_PROMPT[:80]}...\"')\n",
    "    print(f'  Accuracy: {baseline_score:.0%} ({int(baseline_score * len(TEST_INDICES))}/{len(TEST_INDICES)})')\n",
    "    \n",
    "    print(f'\\nOPTIMIZED PROMPT (from GEPA):')\n",
    "    print(f'  \"{OPTIMIZED_SYSTEM_PROMPT[:80]}...\"')\n",
    "    print(f'  Accuracy: {optimized_score:.0%} ({int(optimized_score * len(TEST_INDICES))}/{len(TEST_INDICES)})')\n",
    "    \n",
    "    lift = optimized_score - baseline_score\n",
    "    print(f'\\nLIFT: {lift:+.0%}')\n",
    "    \n",
    "    if lift > 0:\n",
    "        print('\\n>>> Better prompts = better results!')\n",
    "        print('>>> Now let\\'s see how Synth finds these optimized prompts...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-api-md",
   "metadata": {},
   "source": [
    "## Step 4: Setup Local API for GEPA\n",
    "\n",
    "To run GEPA, we need to expose our classification pipeline via HTTP. This cell:\n",
    "1. Creates a FastAPI wrapper around our business logic\n",
    "2. Starts it on a local port\n",
    "3. Creates a Cloudflare tunnel so Synth can reach it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-local-api",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Setup Local API for Optimization\n",
    "from synth_ai.sdk.localapi import LocalAPIConfig, create_local_api\n",
    "from synth_ai.sdk.task.contracts import RolloutMetrics, RolloutRequest, RolloutResponse, TaskInfo\n",
    "from synth_ai.sdk.tunnels import TunneledLocalAPI, TunnelBackend\n",
    "\n",
    "APP_ID = \"banking77\"\n",
    "APP_NAME = \"Banking77 Intent Classification\"\n",
    "USER_PROMPT = \"Customer Query: {query}\\n\\nAvailable Intents:\\n{available_intents}\\n\\nClassify this query into one of the above banking intents using the tool call.\"\n",
    "\n",
    "\n",
    "class Banking77Dataset:\n",
    "    \"\"\"Lazy dataset loader for Banking77.\"\"\"\n",
    "    def __init__(self):\n",
    "        self._cache = {}\n",
    "        self._label_names = None\n",
    "\n",
    "    def _load_split(self, split: str):\n",
    "        if split not in self._cache:\n",
    "            ds = load_dataset(\"banking77\", split=split, trust_remote_code=False)\n",
    "            self._cache[split] = ds\n",
    "            if self._label_names is None and hasattr(ds.features.get(\"label\"), \"names\"):\n",
    "                self._label_names = ds.features[\"label\"].names\n",
    "        return self._cache[split]\n",
    "\n",
    "    def ensure_ready(self, splits):\n",
    "        for split in splits:\n",
    "            self._load_split(split)\n",
    "\n",
    "    def size(self, split: str) -> int:\n",
    "        return len(self._load_split(split))\n",
    "\n",
    "    def sample(self, *, split: str, index: int) -> dict:\n",
    "        ds = self._load_split(split)\n",
    "        idx = index % len(ds)\n",
    "        row = ds[idx]\n",
    "        label_idx = int(row.get(\"label\", 0))\n",
    "        label_text = self._label_names[label_idx] if self._label_names and label_idx < len(self._label_names) else f\"label_{label_idx}\"\n",
    "        return {\"index\": idx, \"split\": split, \"text\": str(row.get(\"text\", \"\")), \"label\": label_text}\n",
    "\n",
    "    @property\n",
    "    def label_names(self) -> list:\n",
    "        if self._label_names is None:\n",
    "            self._load_split(\"train\")\n",
    "        return self._label_names or []\n",
    "\n",
    "\n",
    "def create_banking77_local_api(system_prompt: str):\n",
    "    \"\"\"Create a Banking77 local API for optimization.\"\"\"\n",
    "    \n",
    "    dataset = Banking77Dataset()\n",
    "    dataset.ensure_ready([\"train\", \"test\"])\n",
    "    \n",
    "    async def run_rollout(request: RolloutRequest, fastapi_request) -> RolloutResponse:\n",
    "        split = request.env.config.get(\"split\", \"train\")\n",
    "        seed = request.env.seed\n",
    "        sample = dataset.sample(split=split, index=seed)\n",
    "        \n",
    "        # Use model from policy config (OPENAI_API_KEY is already set in environment)\n",
    "        predicted_intent = await classify_banking77_query(\n",
    "            query=sample[\"text\"],\n",
    "            system_prompt=system_prompt,\n",
    "            model=request.policy.config.get(\"model\", \"gpt-4o-mini\"),\n",
    "        )\n",
    "        \n",
    "        expected_intent = sample[\"label\"]\n",
    "        is_correct = (\n",
    "            predicted_intent.lower().replace(\"_\", \" \").strip() \n",
    "            == expected_intent.lower().replace(\"_\", \" \").strip()\n",
    "        )\n",
    "        reward = 1.0 if is_correct else 0.0\n",
    "        \n",
    "        return RolloutResponse(\n",
    "            run_id=request.run_id,\n",
    "            metrics=RolloutMetrics(outcome_reward=reward),\n",
    "            trace=None,\n",
    "            trace_correlation_id=request.policy.config.get(\"trace_correlation_id\"),\n",
    "        )\n",
    "    \n",
    "    def provide_taskset_description():\n",
    "        return {\n",
    "            \"splits\": [\"train\", \"test\"],\n",
    "            \"sizes\": {\"train\": dataset.size(\"train\"), \"test\": dataset.size(\"test\")},\n",
    "        }\n",
    "    \n",
    "    def provide_task_instances(seeds):\n",
    "        for seed in seeds:\n",
    "            sample = dataset.sample(split=\"train\", index=seed)\n",
    "            yield TaskInfo(\n",
    "                task={\"id\": APP_ID, \"name\": APP_NAME},\n",
    "                dataset={\"id\": APP_ID, \"split\": sample[\"split\"], \"index\": sample[\"index\"]},\n",
    "                inference={\"tool\": TOOL_NAME},\n",
    "                limits={\"max_turns\": 1},\n",
    "                task_metadata={\"query\": sample[\"text\"], \"expected_intent\": sample[\"label\"]},\n",
    "            )\n",
    "    \n",
    "    return create_local_api(LocalAPIConfig(\n",
    "        app_id=APP_ID,\n",
    "        name=APP_NAME,\n",
    "        description=f\"{APP_NAME} local API for classifying customer queries into banking intents.\",\n",
    "        provide_taskset_description=provide_taskset_description,\n",
    "        provide_task_instances=provide_task_instances,\n",
    "        rollout=run_rollout,\n",
    "        cors_origins=[\"*\"],\n",
    "    ))\n",
    "\n",
    "\n",
    "# Create and start the local API with tunnel\n",
    "print(\"Creating baseline local API...\")\n",
    "baseline_app = create_banking77_local_api(BASELINE_SYSTEM_PROMPT)\n",
    "\n",
    "# Create tunnel - handles server startup, health check, and tunnel creation automatically\n",
    "print('\\nStarting server and provisioning Cloudflare tunnel...')\n",
    "baseline_tunnel = await TunneledLocalAPI.create_for_app(\n",
    "    app=baseline_app,\n",
    "    local_port=LOCAL_API_PORT,  # Optional: None to auto-select port\n",
    "    backend=TunnelBackend.CloudflareManagedTunnel,\n",
    "    progress=True,\n",
    ")\n",
    "BASELINE_LOCAL_API_URL = baseline_tunnel.url\n",
    "\n",
    "print(f'\\n' + '=' * 50)\n",
    "print('LOCAL API READY')\n",
    "print('=' * 50)\n",
    "print(f'URL: {BASELINE_LOCAL_API_URL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gepa-md",
   "metadata": {},
   "source": [
    "## Step 5: Run GEPA Optimization\n",
    "\n",
    "GEPA (Genetic Evolutionary Prompt Algorithm) evolves prompts over multiple generations.\n",
    "Each generation evaluates candidates on training samples and selects the best performers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-gepa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Run GEPA Optimization\n",
    "from synth_ai.sdk.api.train.prompt_learning import PromptLearningJob\n",
    "\n",
    "def run_gepa():\n",
    "    config_body = {\n",
    "        'prompt_learning': {\n",
    "            'algorithm': 'gepa',\n",
    "            'task_app_url': BASELINE_LOCAL_API_URL,\n",
    "            'env_name': 'banking77',\n",
    "            'initial_prompt': {\n",
    "                'messages': [\n",
    "                    {'role': 'system', 'order': 0, 'pattern': BASELINE_SYSTEM_PROMPT},\n",
    "                    {'role': 'user', 'order': 1, 'pattern': USER_PROMPT},\n",
    "                ],\n",
    "                'wildcards': {'query': 'REQUIRED', 'available_intents': 'OPTIONAL'},\n",
    "            },\n",
    "            'policy': {\n",
    "                'model': 'gpt-4.1-nano',\n",
    "                'provider': 'openai',\n",
    "                'inference_mode': 'synth_hosted',\n",
    "                'temperature': 0.0,\n",
    "                'max_completion_tokens': 256,\n",
    "            },\n",
    "            'gepa': {\n",
    "                'env_name': 'banking77',\n",
    "                'evaluation': {\n",
    "                    'seeds': list(range(50)),  # Training seeds (used during optimization)\n",
    "                    'validation_seeds': list(range(50, 60)),  # Validation seeds (held-out, checked during optimization)\n",
    "                },\n",
    "                'rollout': {'budget': 80, 'max_concurrent': 8, 'minibatch_size': 8},\n",
    "                'proposer_effort': 'MEDIUM',  # Controls mutation model: LOW_CONTEXT, LOW, MEDIUM, HIGH\n",
    "                'proposer_output_tokens': 'FAST',  # Controls mutation length: RAPID, FAST, SLOW\n",
    "                'mutation': {'rate': 0.3},  # llm_model is deprecated - use proposer_effort instead\n",
    "                'population': {'initial_size': 4, 'num_generations': 3, 'children_per_generation': 3},\n",
    "                'archive': {'size': 5, 'pareto_set_size': 10},\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(f'Creating GEPA job...')\n",
    "    \n",
    "    pl_job = PromptLearningJob.from_dict(\n",
    "        config_dict=config_body,\n",
    "        skip_health_check=True,\n",
    "    )\n",
    "    \n",
    "    job_id = pl_job.submit()\n",
    "    print(f'Job ID: {job_id}')\n",
    "\n",
    "    result = pl_job.poll_until_complete(timeout=3600.0, interval=3.0, progress=True)\n",
    "    \n",
    "    print(f'\\nFINAL: {result.status.value}')\n",
    "    \n",
    "    if result.succeeded:\n",
    "        print(f'BEST SCORE: {result.best_score}')\n",
    "    elif result.failed:\n",
    "        print(f'ERROR: {result.error}')\n",
    "    \n",
    "    return result\n",
    "\n",
    "result = run_gepa()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-md",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate on Held-Out Data\n",
    "\n",
    "Run formal eval jobs comparing baseline vs optimized prompts on 50 held-out test samples.\n",
    "This validates that optimization generalizes beyond the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Evaluate on Held-Out Data\n",
    "from synth_ai.sdk.api.eval import EvalJob, EvalJobConfig, EvalResult\n",
    "from synth_ai.sdk.learning.prompt_learning_client import PromptLearningClient\n",
    "from synth_ai.sdk.learning.prompt_learning_types import PromptResults\n",
    "\n",
    "EVAL_SEEDS = list(range(100, 150))  # Held-out test samples\n",
    "\n",
    "def run_eval_job(local_api_url: str, seeds: list[int], mode: str) -> EvalResult:\n",
    "    \"\"\"Run an eval job and wait for completion.\"\"\"\n",
    "    config = EvalJobConfig(\n",
    "        local_api_url=local_api_url,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        api_key=API_KEY,\n",
    "        env_name='banking77',\n",
    "        seeds=seeds,\n",
    "        policy_config={'model': 'gpt-4.1-nano', 'provider': 'openai'},\n",
    "        env_config={'split': 'test'},\n",
    "        concurrency=10,\n",
    "    )\n",
    "    job = EvalJob(config)\n",
    "    job.submit()\n",
    "    return job.poll_until_complete(timeout=600.0, interval=2.0, progress=True)\n",
    "\n",
    "\n",
    "def extract_system_prompt(prompt_results: PromptResults) -> str:\n",
    "    \"\"\"Extract system prompt from the best optimized prompt.\"\"\"\n",
    "    sections = prompt_results.top_prompts[0]['template']['sections']\n",
    "    return next(s['content'] for s in sections if s['role'] == 'system')\n",
    "\n",
    "\n",
    "if result.succeeded:\n",
    "    # Retrieve optimized prompt\n",
    "    pl_client = PromptLearningClient()\n",
    "    prompt_results = await pl_client.get_prompts(result.job_id)\n",
    "    gepa_optimized_system = extract_system_prompt(prompt_results)\n",
    "    best_train_reward = prompt_results.best_score\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print('OPTIMIZATION RESULTS')\n",
    "    print('=' * 60)\n",
    "    print(f'\\nBest Train Reward: {best_train_reward:.1%}')\n",
    "    print(f'\\nOptimized Prompt:')\n",
    "    print(gepa_optimized_system[:400] + \"...\" if len(gepa_optimized_system) > 400 else gepa_optimized_system)\n",
    "    \n",
    "    # Create optimized API and run final evaluation\n",
    "    optimized_app = create_banking77_local_api(gepa_optimized_system)\n",
    "    optimized_tunnel = await TunneledLocalAPI.create_for_app(\n",
    "        app=optimized_app,\n",
    "        local_port=OPTIMIZED_LOCAL_API_PORT,\n",
    "        backend=TunnelBackend.CloudflareManagedTunnel,\n",
    "        progress=True,\n",
    "    )\n",
    "    OPTIMIZED_LOCAL_API_URL = optimized_tunnel.url\n",
    "    \n",
    "    baseline_result = run_eval_job(BASELINE_LOCAL_API_URL, EVAL_SEEDS, 'baseline')\n",
    "    optimized_result = run_eval_job(OPTIMIZED_LOCAL_API_URL, EVAL_SEEDS, 'optimized')\n",
    "    \n",
    "    # Final results\n",
    "    if baseline_result.succeeded and optimized_result.succeeded:\n",
    "        eval_lift = optimized_result.mean_score - baseline_result.mean_score\n",
    "        print('\\n' + '=' * 60)\n",
    "        print('FINAL EVALUATION')\n",
    "        print('=' * 60)\n",
    "        print(f'Training Score:  {best_train_reward:.1%}')\n",
    "        print(f'Held-Out Test:   {optimized_result.mean_score:.1%} (baseline: {baseline_result.mean_score:.1%})')\n",
    "        print(f'Improvement:     {eval_lift:+.1%}')\n",
    "        \n",
    "        if eval_lift > 0:\n",
    "            print('\\n\u2713 Optimization generalizes to held-out data!')\n",
    "        elif eval_lift == 0:\n",
    "            print('\\n= Same performance on held-out data')\n",
    "        else:\n",
    "            print('\\n\u26a0 Possible overfitting (baseline better on held-out)')\n",
    "else:\n",
    "    print(f\"Job failed: {result.status.value}\")\n",
    "    if result.error:\n",
    "        print(f\"Error: {result.error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "from synth_ai.sdk.tunnels import cleanup_all\n",
    "\n",
    "print('Cleaning up cloudflared processes...')\n",
    "cleanup_all()\n",
    "print('Demo complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}