{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Synth GEPA Demo - Banking77\n",
    "\n",
    "Prompt optimization using Synth's GEPA algorithm on the Banking77 intent classification task.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/synth-laboratories/synth-ai/blob/main/demos/gepa_banking77/gepa_banking77_prompt_optimization.ipynb)\n",
    "\n",
    "**Structure:**\n",
    "1. **Setup** - Install dependencies and configure\n",
    "2. **Task Definition** - Banking77 classification task\n",
    "3. **Local API** - Expose the task for optimization\n",
    "4. **Optimize** - Run GEPA to discover better prompts\n",
    "5. **Evaluate** - Formal eval on held-out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Install dependencies (run this first on Colab)\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab - installing dependencies...\")\n",
    "    !pip install -q \"numpy<2\" synth-ai httpx fastapi uvicorn datasets nest_asyncio\n",
    "\n",
    "    # Install cloudflared\n",
    "    !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O /usr/local/bin/cloudflared\n",
    "    !chmod +x /usr/local/bin/cloudflared\n",
    "    !cloudflared --version\n",
    "\n",
    "    print(\"Dependencies installed!\")\n",
    "else:\n",
    "    print(\"Not in Colab - assuming dependencies are already installed\")\n",
    "    print(\"Required: pip install synth-ai httpx fastapi uvicorn datasets nest_asyncio\")\n",
    "    print(\"Required: brew install cloudflare/cloudflare/cloudflared (macOS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setup - All imports, config, and API keys\n",
    "import os, sys, json, asyncio\n",
    "import httpx\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from datasets import load_dataset\n",
    "from openai import AsyncOpenAI\n",
    "from synth_ai.core.utils.env import mint_demo_api_key\n",
    "from synth_ai.core.utils.urls import BACKEND_URL_BASE\n",
    "\n",
    "# Production backend\n",
    "SYNTH_API_BASE = BACKEND_URL_BASE\n",
    "# Ports are optional - will auto-find available ports if not specified\n",
    "LOCAL_API_PORT = 8001  # Optional: specify a port, or None to auto-select\n",
    "OPTIMIZED_LOCAL_API_PORT = 8002  # Optional: specify a port, or None to auto-select\n",
    "\n",
    "# Always mint a demo key for this notebook\n",
    "print(\"\\nMinting demo SYNTH_API_KEY for this demo...\")\n",
    "API_KEY = mint_demo_api_key()\n",
    "print(f\"Demo API Key: {API_KEY[:25]}...\")\n",
    "\n",
    "# Set API key in environment for SDK to use\n",
    "os.environ[\"SYNTH_API_KEY\"] = API_KEY\n",
    "\n",
    "# Set OpenAI API key if available (optional - Step 3 will be skipped if not set)\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "if OPENAI_API_KEY:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "    print(f\"\\nOpenAI API key found: {OPENAI_API_KEY[:20]}...\")\n",
    "else:\n",
    "    print(\"\\nNo OPENAI_API_KEY found - Step 3 (preview) will be skipped\")\n",
    "\n",
    "# Note: Environment API key is automatically handled by create_local_api()\n",
    "# No need to call ensure_localapi_auth() manually - it happens on app startup\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SETUP COMPLETE\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Step 2: Task Definition\n",
    "\n",
    "Banking77 is an intent classification task with 77 possible intents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BANKING77_LABELS = [\n",
    "    \"activate_my_card\",\n",
    "    \"age_limit\",\n",
    "    \"apple_pay_or_google_pay\",\n",
    "    \"atm_support\",\n",
    "    \"automatic_top_up\",\n",
    "    \"balance_not_updated_after_bank_transfer\",\n",
    "    \"balance_not_updated_after_cheque_or_cash_deposit\",\n",
    "    \"beneficiary_not_allowed\",\n",
    "    \"cancel_transfer\",\n",
    "    \"card_about_to_expire\",\n",
    "    \"card_acceptance\",\n",
    "    \"card_arrival\",\n",
    "    \"card_delivery_estimate\",\n",
    "    \"card_linking\",\n",
    "    \"card_not_working\",\n",
    "    \"card_payment_fee_charged\",\n",
    "    \"card_payment_not_recognised\",\n",
    "    \"card_payment_wrong_exchange_rate\",\n",
    "    \"card_swallowed\",\n",
    "    \"cash_withdrawal_charge\",\n",
    "    \"cash_withdrawal_not_recognised\",\n",
    "    \"change_pin\",\n",
    "    \"compromised_card\",\n",
    "    \"contactless_not_working\",\n",
    "    \"country_support\",\n",
    "    \"declined_card_payment\",\n",
    "    \"declined_cash_withdrawal\",\n",
    "    \"declined_transfer\",\n",
    "    \"direct_debit_payment_not_recognised\",\n",
    "    \"disposable_card_limits\",\n",
    "    \"edit_personal_details\",\n",
    "    \"exchange_charge\",\n",
    "    \"exchange_rate\",\n",
    "    \"exchange_via_app\",\n",
    "    \"extra_charge_on_statement\",\n",
    "    \"failed_transfer\",\n",
    "    \"fiat_currency_support\",\n",
    "    \"get_disposable_virtual_card\",\n",
    "    \"get_physical_card\",\n",
    "    \"getting_spare_card\",\n",
    "    \"getting_virtual_card\",\n",
    "    \"lost_or_stolen_card\",\n",
    "    \"lost_or_stolen_phone\",\n",
    "    \"order_physical_card\",\n",
    "    \"passcode_forgotten\",\n",
    "    \"pending_card_payment\",\n",
    "    \"pending_cash_withdrawal\",\n",
    "    \"pending_top_up\",\n",
    "    \"pending_transfer\",\n",
    "    \"pin_blocked\",\n",
    "    \"receiving_money\",\n",
    "    \"Refund_not_showing_up\",\n",
    "    \"request_refund\",\n",
    "    \"reverted_card_payment?\",\n",
    "    \"supported_cards_and_currencies\",\n",
    "    \"terminate_account\",\n",
    "    \"top_up_by_bank_transfer_charge\",\n",
    "    \"top_up_by_card_charge\",\n",
    "    \"top_up_by_cash_or_cheque\",\n",
    "    \"top_up_failed\",\n",
    "    \"top_up_limits\",\n",
    "    \"top_up_reverted\",\n",
    "    \"topping_up_by_card\",\n",
    "    \"transaction_charged_twice\",\n",
    "    \"transfer_fee_charged\",\n",
    "    \"transfer_into_account\",\n",
    "    \"transfer_not_received_by_recipient\",\n",
    "    \"transfer_timing\",\n",
    "    \"unable_to_verify_identity\",\n",
    "    \"verify_my_identity\",\n",
    "    \"verify_source_of_funds\",\n",
    "    \"verify_top_up\",\n",
    "    \"virtual_card_not_working\",\n",
    "    \"visa_or_mastercard\",\n",
    "    \"why_verify_identity\",\n",
    "    \"wrong_amount_of_cash_received\",\n",
    "    \"wrong_exchange_rate_for_cash_withdrawal\",\n",
    "]\n",
    "\n",
    "TOOL_NAME = \"banking77_classify\"\n",
    "TOOL_SCHEMA = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": TOOL_NAME,\n",
    "        \"description\": \"Return the predicted banking77 intent label.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"intent\": {\"type\": \"string\"}},\n",
    "            \"required\": [\"intent\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def format_available_intents(label_names: list) -> str:\n",
    "    \"\"\"Format the list of available intents for the prompt.\"\"\"\n",
    "    return \"\\n\".join(f\"{i + 1}. {l}\" for i, l in enumerate(label_names))\n",
    "\n",
    "\n",
    "async def classify_banking77_query(\n",
    "    query: str,\n",
    "    system_prompt: str,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    ") -> str:\n",
    "    \"\"\"Classify a banking query into an intent using OpenAI.\n",
    "\n",
    "    This is the CORE PIPELINE - clean async code with NO Synth-specific logic.\n",
    "\n",
    "    Args:\n",
    "        query: The customer query to classify\n",
    "        system_prompt: System prompt for the model\n",
    "        model: Model to use (e.g., \"gpt-4o-mini\")\n",
    "\n",
    "    Returns:\n",
    "        The predicted intent label\n",
    "    \"\"\"\n",
    "    client = AsyncOpenAI()  # Uses OPENAI_API_KEY from environment\n",
    "    available_intents = format_available_intents(BANKING77_LABELS)\n",
    "\n",
    "    user_msg = (\n",
    "        f\"Customer Query: {query}\\n\\n\"\n",
    "        f\"Available Intents:\\n{available_intents}\\n\\n\"\n",
    "        f\"Classify this query into one of the above banking intents using the tool call.\"\n",
    "    )\n",
    "\n",
    "    response = await client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ],\n",
    "        tools=[TOOL_SCHEMA],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": TOOL_NAME}},\n",
    "    )\n",
    "\n",
    "    tool_call = response.choices[0].message.tool_calls[0]\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "    return args[\"intent\"]\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"banking77\", split=\"test\", trust_remote_code=False)\n",
    "label_names = dataset.features[\"label\"].names\n",
    "print(f\"Loaded {len(dataset)} test samples with {len(label_names)} intent labels\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"BUSINESS LOGIC READY\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nclassify_banking77_query(query, system_prompt) -> intent\")\n",
    "print(\"\\nThis is the core app. Now let's see how prompts affect performance...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Step 3: Before/After Preview\n",
    "\n",
    "Compare a **baseline prompt** (78%) vs an **optimized prompt** (92%) on 50 test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Before/After Comparison\n",
    "#\n",
    "# Compare baseline vs optimized prompts on 50 test samples.\n",
    "# The optimized prompt was discovered by GEPA - it achieves ~92% vs ~78% baseline.\n",
    "\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "if not OPENAI_API_KEY:\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "\n",
    "        OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\") or \"\"\n",
    "    except Exception:\n",
    "        OPENAI_API_KEY = \"\"\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY not set; skipping Step 3 eval.\")\n",
    "\n",
    "BASELINE_SYSTEM_PROMPT = \"\"\"You are an expert banking assistant that classifies customer queries into banking intents. Given a customer message, respond with exactly one intent label from the provided list using the `banking77_classify` tool.\"\"\"\n",
    "\n",
    "# This optimized prompt was discovered by GEPA - it adds classification strategy and key distinctions\n",
    "OPTIMIZED_SYSTEM_PROMPT = \"\"\"You are a precise banking intent classifier. Analyze customer queries and classify them into exactly one of the 77 predefined banking intents.\n",
    "\n",
    "Classification Strategy:\n",
    "1. IDENTIFY THE PRIMARY ACTION: What does the customer want to DO? (activate, cancel, check, transfer, verify, etc.)\n",
    "2. IDENTIFY THE SUBJECT: What is it about? (card, transfer, payment, account, etc.)\n",
    "3. IDENTIFY THE STATE: Is it about something pending, failed, declined, or completed?\n",
    "\n",
    "Key Intent Distinctions:\n",
    "- \"card_arrival\" vs \"card_delivery_estimate\": Both about card delivery. Use \"card_arrival\" for \"where is my card?\" and \"card_delivery_estimate\" for \"how long will it take?\"\n",
    "- \"get_physical_card\" vs \"order_physical_card\": Use \"order_physical_card\" for placing an order, \"get_physical_card\" for asking HOW to get one\n",
    "- \"pending_*\" intents: Transaction is IN PROGRESS, not yet complete\n",
    "- \"failed_*\" or \"declined_*\" intents: Transaction was REJECTED\n",
    "- \"*_not_recognised\" intents: Customer doesn't recognize a transaction on their statement\n",
    "- \"verify_*\" intents: About verification/authentication processes\n",
    "- \"top_up_*\" intents: About adding money TO the account\n",
    "- \"transfer_*\" intents: About moving money between accounts\n",
    "\n",
    "Output the single most appropriate intent using the banking77_classify tool.\"\"\"\n",
    "\n",
    "# Test on 50 held-out samples\n",
    "if OPENAI_API_KEY:\n",
    "    TEST_INDICES = list(range(100, 150))\n",
    "\n",
    "    async def score_prompt(system_prompt: str, indices: list[int], prompt_name: str) -> float:\n",
    "        \"\"\"Score a prompt on a set of test samples.\"\"\"\n",
    "        correct = 0\n",
    "        total = len(indices)\n",
    "\n",
    "        for i, idx in enumerate(indices):\n",
    "            sample = dataset[idx]\n",
    "            query = sample[\"text\"]\n",
    "            expected = label_names[sample[\"label\"]]\n",
    "\n",
    "            predicted = await classify_banking77_query(\n",
    "                query=query,\n",
    "                system_prompt=system_prompt,\n",
    "                model=\"gpt-4o-mini\",\n",
    "            )\n",
    "\n",
    "            # Normalize for comparison\n",
    "            pred_norm = predicted.lower().replace(\"_\", \" \").strip()\n",
    "            exp_norm = expected.lower().replace(\"_\", \" \").strip()\n",
    "            is_correct = pred_norm == exp_norm\n",
    "\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(\n",
    "                    f\"  {prompt_name}: {i + 1}/{total} done, {correct}/{i + 1} correct ({correct / (i + 1):.0%})\"\n",
    "                )\n",
    "\n",
    "        accuracy = correct / total\n",
    "        return accuracy\n",
    "\n",
    "    print(\n",
    "        f\"Testing on {len(TEST_INDICES)} samples (indices {TEST_INDICES[0]}-{TEST_INDICES[-1]})...\\n\"\n",
    "    )\n",
    "\n",
    "    print(\"Scoring BASELINE prompt...\")\n",
    "    baseline_score = await score_prompt(BASELINE_SYSTEM_PROMPT, TEST_INDICES, \"Baseline\")\n",
    "\n",
    "    print(\"\\nScoring OPTIMIZED prompt...\")\n",
    "    optimized_score = await score_prompt(OPTIMIZED_SYSTEM_PROMPT, TEST_INDICES, \"Optimized\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BEFORE/AFTER COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nBASELINE PROMPT:\")\n",
    "    print(f'  \"{BASELINE_SYSTEM_PROMPT[:80]}...\"')\n",
    "    print(\n",
    "        f\"  Accuracy: {baseline_score:.0%} ({int(baseline_score * len(TEST_INDICES))}/{len(TEST_INDICES)})\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nOPTIMIZED PROMPT (from GEPA):\")\n",
    "    print(f'  \"{OPTIMIZED_SYSTEM_PROMPT[:80]}...\"')\n",
    "    print(\n",
    "        f\"  Accuracy: {optimized_score:.0%} ({int(optimized_score * len(TEST_INDICES))}/{len(TEST_INDICES)})\"\n",
    "    )\n",
    "\n",
    "    lift = optimized_score - baseline_score\n",
    "    print(f\"\\nLIFT: {lift:+.0%}\")\n",
    "\n",
    "    if lift > 0:\n",
    "        print(\"\\n>>> Better prompts = better results!\")\n",
    "        print(\">>> Now let's see how Synth finds these optimized prompts...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Step 3: Local API\n",
    "\n",
    "Expose the task via HTTP so Synth can run optimization against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synth_ai.sdk.localapi import LocalAPIConfig, create_local_api\n",
    "from synth_ai.sdk.localapi._impl.contracts import RolloutMetrics, RolloutRequest, RolloutResponse, TaskInfo\n",
    "from synth_ai.core.tunnels import TunnelBackend, TunneledLocalAPI\n",
    "\n",
    "APP_ID = \"banking77\"\n",
    "APP_NAME = \"Banking77 Intent Classification\"\n",
    "\n",
    "BASELINE_SYSTEM_PROMPT = \"\"\"You are an expert banking assistant that classifies customer queries into banking intents. Given a customer message, respond with exactly one intent label from the provided list using the `banking77_classify` tool.\"\"\"\n",
    "\n",
    "USER_PROMPT = \"Customer Query: {query}\\n\\nAvailable Intents:\\n{available_intents}\\n\\nClassify this query into one of the above banking intents using the tool call.\"\n",
    "\n",
    "\n",
    "class Banking77Dataset:\n",
    "    \"\"\"Lazy dataset loader for Banking77.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._cache = {}\n",
    "        self._label_names = None\n",
    "\n",
    "    def _load_split(self, split: str):\n",
    "        if split not in self._cache:\n",
    "            ds = load_dataset(\"banking77\", split=split, trust_remote_code=False)\n",
    "            self._cache[split] = ds\n",
    "            if self._label_names is None:\n",
    "                self._label_names = ds.features[\"label\"].names\n",
    "        return self._cache[split]\n",
    "\n",
    "    def ensure_ready(self, splits):\n",
    "        \"\"\"Ensure specified dataset splits are loaded.\"\"\"\n",
    "        for split in splits:\n",
    "            self._load_split(split)\n",
    "\n",
    "    def size(self, split: str) -> int:\n",
    "        return len(self._load_split(split))\n",
    "\n",
    "    def sample(self, *, split: str, index: int) -> dict:\n",
    "        ds = self._load_split(split)\n",
    "        idx = index % len(ds)\n",
    "        row = ds[idx]\n",
    "        label_idx = int(row.get(\"label\", 0))\n",
    "        label_text = (\n",
    "            self._label_names[label_idx]\n",
    "            if self._label_names and label_idx < len(self._label_names)\n",
    "            else f\"label_{label_idx}\"\n",
    "        )\n",
    "        return {\"index\": idx, \"split\": split, \"text\": str(row.get(\"text\", \"\")), \"label\": label_text}\n",
    "\n",
    "    @property\n",
    "    def label_names(self) -> list:\n",
    "        if self._label_names is None:\n",
    "            self._load_split(\"train\")\n",
    "        return self._label_names or []\n",
    "\n",
    "\n",
    "def create_banking77_local_api(system_prompt: str):\n",
    "    \"\"\"Create a Banking77 local API for optimization.\"\"\"\n",
    "\n",
    "    dataset = Banking77Dataset()\n",
    "    dataset.ensure_ready([\"train\", \"test\"])\n",
    "\n",
    "    async def run_rollout(request: RolloutRequest, fastapi_request) -> RolloutResponse:\n",
    "        split = request.env.config.get(\"split\", \"train\")\n",
    "        seed = request.env.seed\n",
    "        sample = dataset.sample(split=split, index=seed)\n",
    "\n",
    "        # Use model from policy config (OPENAI_API_KEY is already set in environment)\n",
    "        predicted_intent = await classify_banking77_query(\n",
    "            query=sample[\"text\"],\n",
    "            system_prompt=system_prompt,\n",
    "            model=request.policy.config.get(\"model\", \"gpt-4o-mini\"),\n",
    "        )\n",
    "\n",
    "        expected_intent = sample[\"label\"]\n",
    "        is_correct = (\n",
    "            predicted_intent.lower().replace(\"_\", \" \").strip()\n",
    "            == expected_intent.lower().replace(\"_\", \" \").strip()\n",
    "        )\n",
    "        reward = 1.0 if is_correct else 0.0\n",
    "\n",
    "        return RolloutResponse(\n",
    "            run_id=request.run_id,\n",
    "            reward_info=RolloutMetrics(outcome_reward=reward),\n",
    "            trace=None,\n",
    "        )\n",
    "\n",
    "    def provide_taskset_description():\n",
    "        return {\n",
    "            \"splits\": [\"train\", \"test\"],\n",
    "            \"sizes\": {\"train\": dataset.size(\"train\"), \"test\": dataset.size(\"test\")},\n",
    "        }\n",
    "\n",
    "    def provide_task_instances(seeds):\n",
    "        for seed in seeds:\n",
    "            sample = dataset.sample(split=\"train\", index=seed)\n",
    "            yield TaskInfo(\n",
    "                task={\"id\": APP_ID, \"name\": APP_NAME},\n",
    "                dataset={\"id\": APP_ID, \"split\": sample[\"split\"], \"index\": sample[\"index\"]},\n",
    "                inference={\"tool\": TOOL_NAME},\n",
    "                limits={\"max_turns\": 1},\n",
    "                task_metadata={\"query\": sample[\"text\"], \"expected_intent\": sample[\"label\"]},\n",
    "            )\n",
    "\n",
    "    return create_local_api(\n",
    "        LocalAPIConfig(\n",
    "            app_id=APP_ID,\n",
    "            name=APP_NAME,\n",
    "            description=f\"{APP_NAME} local API for classifying customer queries into banking intents.\",\n",
    "            provide_taskset_description=provide_taskset_description,\n",
    "            provide_task_instances=provide_task_instances,\n",
    "            rollout=run_rollout,\n",
    "            cors_origins=[\"*\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Starting local API...\")\n",
    "baseline_app = create_banking77_local_api(BASELINE_SYSTEM_PROMPT)\n",
    "\n",
    "# Create tunnel - handles server startup, health check, and tunnel creation automatically\n",
    "print(\"\\nStarting server and provisioning Cloudflare tunnel...\")\n",
    "baseline_tunnel = await TunneledLocalAPI.create_for_app(\n",
    "    app=baseline_app,\n",
    "    local_port=None,\n",
    "    backend=TunnelBackend.CloudflareQuickTunnel,\n",
    "    progress=True,\n",
    ")\n",
    "BASELINE_LOCAL_API_URL = baseline_tunnel.url\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(\"LOCAL API READY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"URL: {BASELINE_LOCAL_API_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step 4: Run GEPA\n",
    "\n",
    "GEPA evolves prompts over multiple generations, selecting the best performers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synth_ai.sdk.optimization.internal.prompt_learning import PromptLearningJob\n",
    "\n",
    "\n",
    "def run_gepa():\n",
    "    config_body = {\n",
    "        \"prompt_learning\": {\n",
    "            \"algorithm\": \"gepa\",\n",
    "            \"task_app_url\": BASELINE_LOCAL_API_URL,\n",
    "            \"env_name\": \"banking77\",\n",
    "            \"initial_prompt\": {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"order\": 0, \"pattern\": BASELINE_SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"order\": 1, \"pattern\": USER_PROMPT},\n",
    "                ],\n",
    "                \"wildcards\": {\"query\": \"REQUIRED\", \"available_intents\": \"OPTIONAL\"},\n",
    "            },\n",
    "            \"policy\": {\n",
    "                \"model\": \"gpt-4.1-nano\",\n",
    "                \"provider\": \"openai\",\n",
    "                \"inference_mode\": \"synth_hosted\",\n",
    "                \"temperature\": 0.0,\n",
    "                \"max_completion_tokens\": 256,\n",
    "            },\n",
    "            \"gepa\": {\n",
    "                \"env_name\": \"banking77\",\n",
    "                \"evaluation\": {\n",
    "                    \"seeds\": list(range(50)),  # Training seeds (used during optimization)\n",
    "                    \"validation_seeds\": list(\n",
    "                        range(50, 60)\n",
    "                    ),  # Validation seeds (held-out, checked during optimization)\n",
    "                },\n",
    "                \"rollout\": {\"budget\": 80, \"max_concurrent\": 8, \"minibatch_size\": 8},\n",
    "                \"proposer_effort\": \"MEDIUM\",  # Controls mutation model: LOW_CONTEXT, LOW, MEDIUM, HIGH\n",
    "                \"proposer_output_tokens\": \"FAST\",  # Controls mutation length: RAPID, FAST, SLOW\n",
    "                \"mutation\": {\"rate\": 0.3},  # llm_model is deprecated - use proposer_effort instead\n",
    "                \"population\": {\n",
    "                    \"initial_size\": 4,\n",
    "                    \"num_generations\": 3,\n",
    "                    \"children_per_generation\": 3,\n",
    "                },\n",
    "                \"archive\": {\"size\": 5, \"pareto_set_size\": 10},\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(f\"Creating GEPA job...\")\n",
    "\n",
    "    pl_job = PromptLearningJob.from_dict(\n",
    "        config_dict=config_body,\n",
    "        skip_health_check=True,\n",
    "    )\n",
    "\n",
    "    job_id = pl_job.submit()\n",
    "    print(f\"Job ID: {job_id}\")\n",
    "\n",
    "    result = pl_job.poll_until_complete(timeout=3600.0, interval=3.0, progress=True)\n",
    "\n",
    "    print(f\"\\nFINAL: {result.status.value}\")\n",
    "\n",
    "    if result.succeeded:\n",
    "        print(f\"BEST SCORE: {result.best_score}\")\n",
    "    elif result.failed:\n",
    "        print(f\"ERROR: {result.error}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_gepa()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate\n",
    "\n",
    "Compare baseline vs optimized prompts on held-out test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synth_ai.sdk.eval.job import EvalJob, EvalJobConfig, EvalResult\n",
    "from synth_ai.sdk.optimization.internal.learning.prompt_learning_client import PromptLearningClient\n",
    "from synth_ai.sdk.optimization.internal.learning.prompt_learning_types import PromptResults\n",
    "\n",
    "EVAL_SEEDS = list(range(100, 150))  # Held-out test samples\n",
    "\n",
    "\n",
    "def run_eval_job(local_api_url: str, seeds: list[int], mode: str) -> EvalResult:\n",
    "    \"\"\"Run an eval job and wait for completion.\"\"\"\n",
    "    config = EvalJobConfig(\n",
    "        local_api_url=local_api_url,\n",
    "        backend_url=SYNTH_API_BASE,\n",
    "        api_key=API_KEY,\n",
    "        env_name=\"banking77\",\n",
    "        seeds=seeds,\n",
    "        policy_config={\"model\": \"gpt-4.1-nano\", \"provider\": \"openai\"},\n",
    "        env_config={\"split\": \"test\"},\n",
    "        concurrency=10,\n",
    "    )\n",
    "    job = EvalJob(config)\n",
    "    job.submit()\n",
    "    return job.poll_until_complete(timeout=600.0, interval=2.0, progress=True)\n",
    "\n",
    "\n",
    "def extract_system_prompt(prompt_results: PromptResults) -> str:\n",
    "    \"\"\"Extract system prompt from the best optimized prompt.\"\"\"\n",
    "    sections = prompt_results.top_prompts[0][\"template\"][\"sections\"]\n",
    "    return next(s[\"content\"] for s in sections if s[\"role\"] == \"system\")\n",
    "\n",
    "\n",
    "if result.succeeded:\n",
    "    pl_client = PromptLearningClient()\n",
    "    prompt_results = await pl_client.get_prompts(result.job_id)\n",
    "    gepa_optimized_system = extract_system_prompt(prompt_results)\n",
    "    best_train_reward = prompt_results.best_score\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"OPTIMIZATION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nBest Train Reward: {best_train_reward:.1%}\")\n",
    "    print(f\"\\nOptimized Prompt:\")\n",
    "    print(\n",
    "        gepa_optimized_system[:400] + \"...\"\n",
    "        if len(gepa_optimized_system) > 400\n",
    "        else gepa_optimized_system\n",
    "    )\n",
    "\n",
    "    # Create optimized API and run final evaluation\n",
    "    optimized_app = create_banking77_local_api(gepa_optimized_system)\n",
    "    optimized_tunnel = await TunneledLocalAPI.create_for_app(\n",
    "        app=optimized_app,\n",
    "        local_port=None,\n",
    "        backend=TunnelBackend.CloudflareQuickTunnel,\n",
    "        progress=True,\n",
    "    )\n",
    "    OPTIMIZED_LOCAL_API_URL = optimized_tunnel.url\n",
    "\n",
    "    baseline_result = run_eval_job(BASELINE_LOCAL_API_URL, EVAL_SEEDS, \"baseline\")\n",
    "    optimized_result = run_eval_job(OPTIMIZED_LOCAL_API_URL, EVAL_SEEDS, \"optimized\")\n",
    "\n",
    "    # Final results\n",
    "    if baseline_result.succeeded and optimized_result.succeeded:\n",
    "        eval_lift = optimized_result.mean_score - baseline_result.mean_score\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"FINAL EVALUATION\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Training Score:  {best_train_reward:.1%}\")\n",
    "        print(\n",
    "            f\"Held-Out Test:   {optimized_result.mean_score:.1%} (baseline: {baseline_result.mean_score:.1%})\"\n",
    "        )\n",
    "        print(f\"Improvement:     {eval_lift:+.1%}\")\n",
    "\n",
    "        if eval_lift > 0:\n",
    "            print(\"\\n✓ Optimization generalizes to held-out data!\")\n",
    "        elif eval_lift == 0:\n",
    "            print(\"\\n= Same performance on held-out data\")\n",
    "        else:\n",
    "            print(\"\\n⚠ Possible overfitting (baseline better on held-out)\")\n",
    "else:\n",
    "    print(f\"Optimization failed: {result.error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synth_ai.core.tunnels import cleanup_all\n",
    "\n",
    "print(\"Cleaning up cloudflared processes...\")\n",
    "cleanup_all()\n",
    "print(\"Demo complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synth-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
