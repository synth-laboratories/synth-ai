check the fxns and endpoints / endpoint management for openai, synth, and gemini finetuning and     │
│   try to form a unified TrainingRunner that exposes shared endpoints and lets the user route based    │
│   on the provider   c



# Unified TrainingRunner Design for Multi-Provider Finetuning

## Analysis Summary

Based on examination of OpenAI, Modal/Synth, and Gemini finetuning implementations in the synth-ai codebase, here are the key findings for designing a unified TrainingRunner:

### Provider-Specific Analysis

#### OpenAI Fine-tuning (`kick_off_ft_oai.py`)
- **API Pattern**: Native OpenAI SDK with `client.files.create()` and `client.fine_tuning.jobs.create()`
- **Data Format**: JSONL with ChatML structure, tiktoken-based token counting
- **Auth**: API key via `OPENAI_API_KEY`
- **Status**: `validating_files`, `queued`, `running`, `succeeded`, `failed`, `cancelled`
- **Config**: Base model, suffix, polling interval
- **Management**: Synchronous polling, graceful interrupt handling

#### Modal/Synth Fine-tuning (`kick_off_ft_modal.py`) 
- **API Pattern**: OpenAI v1 compatible REST API with httpx client
- **Endpoints**: `POST /v1/files`, `POST /v1/fine_tuning/jobs`, `GET /v1/fine_tuning/jobs/{id}`
- **Data Format**: JSONL, character-based token estimation
- **Auth**: Bearer token with `MODAL_API_KEY` or `SYNTH_API_KEY`
- **Status**: Similar to OpenAI but with async/await pattern
- **Config**: Model, training_type (sft/dpo), hyperparameters (n_epochs, batch_size, learning_rate, LoRA params)
- **Management**: Async polling with timeout handling

#### Gemini/Vertex AI Fine-tuning (`kick_off_ft_gemini.py`)
- **API Pattern**: Vertex AI SDK with `GenerativeModel.tune_model()`
- **Storage**: Google Cloud Storage blob upload required
- **Data Format**: JSONL for Vertex AI, tiktoken estimation
- **Auth**: Google Cloud credentials (implicit via SDK)
- **Status**: `SUCCEEDED`, `FAILED`, `CANCELLED` via `job.state`
- **Config**: GCP project/region, GCS bucket, display_name, epochs, learning_rate_multiplier
- **Management**: Direct SDK polling with `job.refresh()`

## Unified TrainingRunner Design

### Core Interface
```python
class TrainingRunner(ABC):
    # Shared endpoints for all providers
    async def upload_training_data(file_path: Path) -> str
    async def start_training_job(file_id: str, config: TrainingConfig) -> TrainingJob  
    async def get_job_status(job_id: str) -> TrainingJob
    async def cancel_job(job_id: str) -> bool
    async def poll_until_complete(job_id: str) -> TrainingJob
    async def run_full_pipeline(file_path: Path, config: TrainingConfig) -> TrainingJob
```

### Unified Configuration
```python
@dataclass
class TrainingConfig:
    model: str
    training_type: str = "sft"  # sft, dpo, rlhf
    epochs: int = 3
    batch_size: int = 4  
    learning_rate: float = 5e-5
    hyperparameters: Dict[str, Any] = None  # Provider-specific params
    suffix: Optional[str] = None
    display_name: Optional[str] = None
    poll_interval: int = 30
    gpu_config: Any
```

### Status Normalization
```python
class TrainingStatus(Enum):
    PENDING = "pending"
    VALIDATING = "validating_files" 
    QUEUED = "queued"
    RUNNING = "running"
    SUCCEEDED = "succeeded"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass  
class TrainingJob:
    id: str
    provider: TrainingProvider
    status: TrainingStatus
    model: str
    training_file_id: Optional[str] = None
    fine_tuned_model: Optional[str] = None
    error: Optional[str] = None
    progress: Dict[str, Any] = None
```

### Factory Pattern
```python
def create_training_runner(provider: TrainingProvider, credentials: Dict[str, str]) -> TrainingRunner:
    if provider == TrainingProvider.OPENAI:
        return OpenAITrainingRunner(credentials["api_key"])
    elif provider == TrainingProvider.MODAL:
        return ModalTrainingRunner(credentials["api_key"], credentials["base_url"])
    elif provider == TrainingProvider.GEMINI:
        return GeminiTrainingRunner(credentials["project"], credentials["region"], credentials["bucket"])
```

## Key Design Features

### 1. **Shared Endpoints**
- Common interface abstracts away provider differences
- Unified method signatures for upload, start, status, cancel operations
- Built-in polling and full pipeline workflows

### 2. **Provider Abstraction** 
- Factory pattern for easy instantiation
- Provider-specific credential handling
- Automatic API endpoint/format adaptation

### 3. **Status Standardization**
- Common `TrainingStatus` enum maps all provider statuses
- Unified `TrainingJob` representation with provider-agnostic fields
- Consistent progress reporting interface

### 4. **Configuration Flexibility**
- Core hyperparameters (epochs, batch_size, learning_rate) work across providers
- Provider-specific parameters via `hyperparameters` dict
- Model naming conventions adapted per provider

### 5. **Error Handling**
- Unified exception patterns
- Provider-specific error mapping to common format
- Graceful timeout and retry logic

## Implementation Benefits

1. **Developer Experience**: Single interface for all providers
2. **Provider Switching**: Easy to compare providers or migrate between them
3. **Testing**: Mock implementations possible with common interface
4. **Monitoring**: Unified job tracking and status reporting
5. **Configuration**: Environment-based provider selection
6. **Extensibility**: New providers can be added by implementing the interface

## Usage Example

```python
# Create provider-specific runner
runner = create_training_runner(
    TrainingProvider.OPENAI, 
    {"api_key": "sk-..."}
)

# Unified configuration works across providers
config = TrainingConfig(
    model="gpt-4.1-nano-2025-04-14",
    epochs=3,
    batch_size=4, 
    learning_rate=5e-5
)

# Run complete training pipeline
job = await runner.run_full_pipeline(Path("training_data.jsonl"), config)
print(f"Training completed: {job.fine_tuned_model}")
```

This design provides a clean abstraction over the three finetuning providers while preserving their unique capabilities and maintaining compatibility with existing implementations.