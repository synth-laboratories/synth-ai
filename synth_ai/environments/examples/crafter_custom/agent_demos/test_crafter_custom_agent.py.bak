#!/usr/bin/env python3
"""
Custom Crafter Agent Evaluation with configurable world generation.

Tests ReAct agents against custom Crafter environments with different world configurations
(easy, normal, hard, peaceful) on synth service.
"""

import asyncio
import json
import uuid
import math
import argparse
import toml
import logging
from datetime import datetime
from typing import Dict, Any, Optional, List, Set
from pydantic import BaseModel, Field
from httpx import AsyncClient
import sys
import os
from pathlib import Path
from tqdm.asyncio import tqdm_asyncio
from tqdm import tqdm
import random
from collections import defaultdict

# Disable Langfuse completely to prevent hangs and warnings
os.environ["LANGFUSE_ENABLED"] = "false"
os.environ["LANGFUSE_PUBLIC_KEY"] = "dummy"  # Prevent the warning about missing key
os.environ["LANGFUSE_SECRET_KEY"] = "dummy"  # Prevent the secret key warning
# Disable all Langfuse logging
import logging
logging.getLogger("langfuse").setLevel(logging.ERROR)

# Monkey patch Langfuse to disable all warnings
import warnings
warnings.filterwarnings("ignore", message=".*Langfuse.*")

from langfuse.openai import openai
from langfuse import Langfuse

# Override Langfuse client to silence it completely
class SilentLangfuse(Langfuse):
    def __init__(self, *args, **kwargs):
        # Set dummy values to prevent warnings
        kwargs['public_key'] = kwargs.get('public_key', 'dummy')
        kwargs['secret_key'] = kwargs.get('secret_key', 'dummy')
        kwargs['enabled'] = False
        super().__init__(*args, **kwargs)
        
# Replace Langfuse with silent version
import langfuse
langfuse.Langfuse = SilentLangfuse
import numpy as np

# Import session tracer for CAIS event capture
from synth_ai.tracing_v2.session_tracer import (
    SessionTracer, SessionEventMarkovBlanketMessage, TimeRecord,
    RuntimeEvent, EnvironmentEvent, CAISEvent
)
from datetime import datetime

# Import Crafter hooks
try:
    from synth_ai.environments.examples.crafter_classic.trace_hooks import CRAFTER_HOOKS
    print(f"‚úÖ Loaded {len(CRAFTER_HOOKS)} Crafter achievement hooks (Easy, Medium, Hard)")
except ImportError:
    print("Warning: Could not import CRAFTER_HOOKS")
    CRAFTER_HOOKS = []


# Create a proper message structure with origin_system_id
def create_message(content: Any, message_type: str, origin_system_id: Any, turn: int) -> SessionEventMarkovBlanketMessage:
    """Create a message with origin system ID embedded in content."""
    return SessionEventMarkovBlanketMessage(
        content={
            "origin_system_id": str(origin_system_id),
            "payload": content
        },
        message_type=message_type,
        time_record=TimeRecord(
            event_time=datetime.now().isoformat(),
            message_time=turn
        )
    )


def compress_observation_for_trace(obs: Dict[str, Any]) -> Dict[str, Any]:
    """Compress observation data for efficient trace storage."""
    import base64
    obs_compressed = obs.copy()
    
    # Convert semantic map to text
    if "semantic_map" in obs_compressed:
        map_view = format_semantic_map_view(obs_compressed, view_size=7)
        obs_compressed["semantic_map_text"] = map_view
        del obs_compressed["semantic_map"]
    
    # Compress image/map arrays to base64
    heavy_fields = ["observation_image", "world_material_map", "rgb", "image"]
    for field in heavy_fields:
        if field in obs_compressed and isinstance(obs_compressed[field], (list, np.ndarray)):
            arr = np.array(obs_compressed[field], dtype=np.uint8)
            obs_compressed[f"{field}_base64"] = base64.b64encode(arr.tobytes()).decode('ascii')
            obs_compressed[f"{field}_shape"] = arr.shape
            del obs_compressed[field]
    
    return obs_compressed


def print_hook_legend():
    """Print the legend for hook codes."""
    print("\nüìñ Hook Legend:")
    print("  E = Easy achievement    (e.g., collect_wood, place_table)")
    print("  M = Medium achievement  (e.g., make_wood_pickaxe, collect_coal)")
    print("  H = Hard achievement    (e.g., make_iron_sword, defeat_zombie)")
    print("  X = Invalid action      (action had no effect)")
    print("  # = Regular step")
    print("")  # Add blank line to separate from progress bars


def create_progress_bar(episode_num: int, steps: List[str], max_steps: int) -> str:
    """Create a progress bar string with hook codes."""
    # Pad with spaces if fewer steps than max
    padded_steps = steps + [' '] * (max_steps - len(steps))
    bar = ''.join(padded_steps[:max_steps])
    return f"Episode {episode_num:2d}: [{bar}] {len(steps)}/{max_steps}"


def update_progress_display(episode_bars: Dict[int, List[str]], max_steps: int):
    """Update the progress display in place."""
    # Clear previous lines
    num_episodes = len(episode_bars)
    if num_episodes > 0:
        # Move cursor up to overwrite previous display
        print(f"\033[{num_episodes}A", end='')
    
    # Print all episode progress bars
    for episode_num in sorted(episode_bars.keys()):
        steps = episode_bars[episode_num]
        print(create_progress_bar(episode_num, steps, max_steps))
    
    # Ensure we don't leave cursor in wrong position
    sys.stdout.flush()


def print_achievements_table(all_achievements: Dict[str, int], num_episodes: int):
    """Print a beautiful table of achievements across all episodes."""
    if not all_achievements:
        return
        
    print("\n" + "=" * 80)
    print("üèÜ ACHIEVEMENTS SUMMARY")
    print("=" * 80)
    print(f"{'Achievement':<30} {'Count':<10} {'Percentage':<15}")
    print("-" * 55)
    
    # Sort achievements by count (descending) then by name
    sorted_achievements = sorted(all_achievements.items(), key=lambda x: (-x[1], x[0]))
    
    for achievement, count in sorted_achievements:
        percentage = (count / num_episodes) * 100
        print(f"{achievement:<30} {count:<10} {percentage:>6.1f}%")
    
    print("-" * 55)
    print(f"{'TOTAL UNIQUE':<30} {len(all_achievements):<10}")
    print("=" * 80)


def print_invalid_actions_table(invalid_actions: Dict[str, int], total_actions: Dict[str, int] = None):
    """Print a table of invalid actions by type with failure rates."""
    if not invalid_actions:
        return
    
    print("\n" + "=" * 90)
    print("‚ùå INVALID ACTIONS SUMMARY")
    print("=" * 90)
    print(f"{'Action Type':<20} {'Invalid/Total':<15} {'Failure %':<12} {'Description':<35}")
    print("-" * 90)
    
    # Sort by count (descending) then by name
    sorted_actions = sorted(invalid_actions.items(), key=lambda x: (-x[1], x[0]))
    
    action_descriptions = {
        'move_left': 'Movement blocked (wall/edge)',
        'move_right': 'Movement blocked (wall/edge)',
        'move_up': 'Movement blocked (wall/edge)',
        'move_down': 'Movement blocked (wall/edge)',
        'do': 'Nothing to collect/attack',
        'sleep': 'Energy already full or conditions not met',
        'place_stone': 'No stone or invalid location',
        'place_table': 'No wood or invalid location',
        'place_furnace': 'No stone or invalid location',
        'place_plant': 'No sapling or invalid location',
        'make_wood_pickaxe': 'Missing materials or no table',
        'make_stone_pickaxe': 'Missing materials or no table',
        'make_iron_pickaxe': 'Missing materials or no furnace',
        'make_wood_sword': 'Missing materials or no table',
        'make_stone_sword': 'Missing materials or no table',
        'make_iron_sword': 'Missing materials or no furnace'
    }
    
    total_invalid = 0
    for action, invalid_count in sorted_actions:
        total_count = total_actions.get(action, invalid_count) if total_actions else invalid_count
        failure_rate = (invalid_count / total_count * 100) if total_count > 0 else 0
        fraction = f"{invalid_count}/{total_count}"
        description = action_descriptions.get(action, 'Unknown reason')
        print(f"{action:<20} {fraction:<15} {failure_rate:>6.1f}%     {description:<35}")
        total_invalid += invalid_count
    
    print("-" * 90)
    total_all_actions = sum(total_actions.values()) if total_actions else total_invalid
    total_failure_rate = (total_invalid / total_all_actions * 100) if total_all_actions > 0 else 0
    print(f"{'TOTAL':<20} {total_invalid}/{total_all_actions:<15} {total_failure_rate:>6.1f}%")
    print("=" * 90)


def print_termination_breakdown(termination_reasons: List[str]):
    """Print episode termination breakdown."""
    print("\n" + "=" * 80)
    print("üèÅ EPISODE TERMINATION BREAKDOWN")
    print("=" * 80)
    
    if not termination_reasons:
        print("No termination data available.")
        return
    
    # Count termination reasons
    reason_counts = {}
    for reason in termination_reasons:
        reason_counts[reason] = reason_counts.get(reason, 0) + 1
    
    # Sort by count descending
    sorted_reasons = sorted(reason_counts.items(), key=lambda x: x[1], reverse=True)
    
    total_episodes = len(termination_reasons)
    
    print(f"{'Termination Reason':<40} {'Count':<10} {'Percentage':<12} {'Description'}")
    print("-" * 80)
    
    # Descriptions for different termination types
    descriptions = {
        "max_turns_reached": "Episode completed all turns",
        "death": "Agent died (health <= 0)",
        "environment_terminated": "Environment ended episode",
        "no_actions_provided": "Agent failed to provide actions"
    }
    
    for reason, count in sorted_reasons:
        percentage = (count / total_episodes * 100) if total_episodes > 0 else 0
        
        # Parse complex reasons
        display_reason = reason
        description = "Other termination reason"
        
        if reason == "max_turns_reached":
            description = descriptions.get(reason, "Episode completed all turns")
        elif reason == "death":
            description = descriptions.get(reason, "Agent died (health <= 0)")
        elif reason == "environment_terminated":
            description = descriptions.get(reason, "Environment ended episode")
        elif reason.startswith("agent_terminate:"):
            display_reason = "agent_terminate"
            description = f"Agent chose to quit: {reason.split(':', 1)[1][:30]}"
        elif reason.startswith("http_error:"):
            display_reason = "http_error"
            description = f"API request failed: {reason.split(':', 1)[1]}"
        elif reason.startswith("exception:"):
            display_reason = "exception"
            description = f"Runtime error: {reason.split(':', 1)[1][:30]}"
        elif reason.startswith("outer_exception:"):
            display_reason = "outer_exception"
            description = f"Fatal error: {reason.split(':', 1)[1][:30]}"
        elif reason.startswith("gather_exception:"):
            display_reason = "gather_exception"
            description = f"Async gather failed: {reason.split(':', 1)[1][:30]}"
        elif reason == "no_actions_provided":
            description = descriptions.get(reason, "Agent failed to provide actions")
        
        print(f"{display_reason:<40} {count:<10} {percentage:<11.1f}% {description}")
    
    print("-" * 80)
    print(f"{'TOTAL':<40} {total_episodes:<10} {'100.0%':<11}")
    print("=" * 80)


def analyze_trace_file(trace_file: Path):
    """Analyze a trace file and print detailed step-by-step information."""
    import time
    
    start_time = time.time()
    print(f"\nüìÑ Analyzing trace: {trace_file.name}")
    print("=" * 80)
    
    try:
        with open(trace_file, 'r') as f:
            trace_data = json.load(f)
    except Exception as e:
        print(f"[ERROR] Failed to load trace file: {e}")
        return
    
    # Get events and messages
    events = trace_data.get('event_history', [])
    messages = trace_data.get('message_history', [])
    
    # Group CAISEvents by turn
    cais_events = [e for e in events if e.get('system_instance_id', '').startswith('crafter-react-agent')]
    
    # Process each turn
    for i, cais_event in enumerate(cais_events):
        print(f"\nüéÆ Step {i + 1}")
        print("-" * 40)
        
        # Token usage
        tokens = {
            'prompt': cais_event.get('prompt_tokens', None),
            'completion': cais_event.get('completion_tokens', None),
            'total': cais_event.get('total_tokens', None)
        }
        if any(t is not None for t in tokens.values()):
            print(f"ü™ô Tokens: Prompt={tokens['prompt']}, Completion={tokens['completion']}, Total={tokens['total']}")
        else:
            # Try to get from llm_call_records
            llm_records = cais_event.get('llm_call_records', [])
            if llm_records and isinstance(llm_records[0], dict):
                response = llm_records[0].get('response', {})
                if response:
                    usage = response.get('usage', {})
                    if usage:
                        print(f"ü™ô Tokens: Prompt={usage.get('prompt_tokens')}, Completion={usage.get('completion_tokens')}, Total={usage.get('total_tokens')}")
        
        # Tool calls from LLM records
        llm_records = cais_event.get('llm_call_records', [])
        if llm_records and isinstance(llm_records[0], dict):
            # First check the response for tool calls
            response = llm_records[0].get('response', {})
            if response and 'choices' in response:
                choices = response.get('choices', [])
                if choices and isinstance(choices[0], dict):
                    tool_calls = choices[0].get('message', {}).get('tool_calls', [])
                    if tool_calls:
                        for tc in tool_calls:
                            tool_name = tc.get('function', {}).get('name', 'unknown')
                            args = json.loads(tc.get('function', {}).get('arguments', '{}'))
                            actions = args.get('actions', [])
                            reasoning = args.get('reasoning', '')
                            print(f"üîß Tool: {tool_name}")
                            if actions:
                                print(f"   Actions: {', '.join(actions)}")
                            if reasoning:
                                print(f"   Reasoning: {reasoning[:60]}...")
        
        # Show hooks that fired for this CAISEvent
        if cais_event.get('event_metadata'):
            print("üéØ Agent Hooks Fired:")
            for meta in cais_event['event_metadata']:
                print(f"   - {meta['hook_name']}: {meta['description']}")
        
        # Find corresponding observations to track achievements and inventory
        turn_time = cais_event.get('time_record', {}).get('message_time', i)
        
        # Get observations for this turn and next turn
        turn_observations = [m for m in messages 
                           if m.get('message_type') == 'observation' 
                           and m.get('time_record', {}).get('message_time', -1) in [turn_time, turn_time + 1]]
        
        if len(turn_observations) >= 2:
            before_obs = turn_observations[0].get('content', {}).get('payload', {})
            after_obs = turn_observations[1].get('content', {}).get('payload', {})
            
            # Achievement changes
            before_achievements = before_obs.get('achievements_status', {})
            after_achievements = after_obs.get('achievements_status', {})
            
            new_achievements = []
            for ach_name, ach_status in after_achievements.items():
                if ach_status and not before_achievements.get(ach_name, False):
                    new_achievements.append(ach_name)
            
            if new_achievements:
                print(f"üèÜ New Achievements: {', '.join(new_achievements)}")
            
            # Inventory changes
            before_inv = before_obs.get('inventory', {})
            after_inv = after_obs.get('inventory', {})
            
            inv_changes = []
            for item, count in after_inv.items():
                before_count = before_inv.get(item, 0)
                if count > before_count:
                    inv_changes.append(f"{item}: {before_count} ‚Üí {count} (+{count - before_count})")
                elif count < before_count:
                    inv_changes.append(f"{item}: {before_count} ‚Üí {count} ({count - before_count})")
            
            if inv_changes:
                print(f"üì¶ Inventory Changes: {', '.join(inv_changes)}")
            
            # Position change
            before_pos = before_obs.get('player_position', [0, 0])
            after_pos = after_obs.get('player_position', [0, 0])
            if before_pos != after_pos:
                print(f"üìç Position: {before_pos} ‚Üí {after_pos}")
        
        # Find corresponding environment event and show its hooks
        env_events = [e for e in events 
                     if not e.get('system_instance_id', '').startswith('crafter-react-agent')
                     and e.get('time_record', {}).get('message_time', -1) == turn_time
                     and 'reward' in e]
        
        if env_events and env_events[0].get('event_metadata'):
            print("üåç Environment Hooks Fired:")
            for meta in env_events[0]['event_metadata']:
                print(f"   - {meta['hook_name']}: {meta['description']}")
    
    print("\n" + "=" * 80)
    
    end_time = time.time()
    print(f"[DEBUG] Trace analysis completed in {end_time - start_time:.2f} seconds")


# --- Configuration Class ---
class CrafterConfig:
    """Configuration for Crafter evaluation."""

    def __init__(self, config_path: Optional[str] = None):
        # Default values
        self.model_name: Optional[str] = None  # Must be provided via config or CLI
        self.num_instances = 1  # Changed from 3 to 1
        self.max_turns = 2  # Changed to just 2 steps
        self.difficulty = "easy"
        self.service_base_url = "http://localhost:8901"
        self.service_timeout = 30.0
        self.seed = 42
        self.save_traces = True
        self.save_detailed_results = True
        self.verbose = False
        self.analyze_traces = False  # Whether to analyze traces after evaluation
        
        # World configuration settings
        self.world_config = "normal"  # Default world config
        self.world_config_path = None  # Optional path to custom JSON config

        # Custom OpenAI endpoint support
        self.custom_openai_base_url = None  # e.g., "https://lora-inference-service-xyz.modal.run"
        self.custom_openai_api_key = "dummy"  # Default dummy key for custom endpoints

        # Load from TOML if provided
        if config_path and os.path.exists(config_path):
            self.load_from_toml(config_path)

        # Fail fast if no model name provided
        # Configure custom OpenAI endpoint if specified
        self._configure_custom_openai()

    def load_from_toml(self, config_path: str):
        """Load configuration from TOML file."""
        config = toml.load(config_path)

        # Extract eval settings
        eval_config = config.get("eval", {})
        self.model_name = eval_config.get("model_name", self.model_name)
        self.num_instances = eval_config.get("episodes", self.num_instances)
        self.max_turns = eval_config.get("max_steps", self.max_turns)
        self.difficulty = eval_config.get("difficulty", self.difficulty)
        self.seed = eval_config.get("seed", self.seed)
        
        # Extract world configuration settings
        self.world_config = eval_config.get("world_config", config.get("world_config", self.world_config))
        self.world_config_path = eval_config.get("world_config_path", config.get("world_config_path", self.world_config_path))

        # Extract service settings
        service_config = config.get("service", {})
        self.service_base_url = service_config.get("base_url", self.service_base_url)
        self.service_timeout = service_config.get("timeout", self.service_timeout)

        # Extract output settings
        output_config = config.get("output", {})
        self.save_traces = output_config.get("save_traces", self.save_traces)
        self.save_detailed_results = output_config.get(
            "save_detailed_results", self.save_detailed_results
        )

        # Extract custom OpenAI endpoint settings
        openai_config = config.get("openai", {})
        self.custom_openai_base_url = openai_config.get("base_url", self.custom_openai_base_url)
        self.custom_openai_api_key = openai_config.get("api_key", self.custom_openai_api_key)

    def _configure_custom_openai(self):
        """Configure environment variables for custom OpenAI endpoint if specified."""
        if self.custom_openai_base_url:
            # Ensure the base URL ends with /v1 for OpenAI compatibility
            base_url = self.custom_openai_base_url.rstrip("/")
            if not base_url.endswith("/v1"):
                base_url += "/v1"

            # Set environment variables for OpenAI SDK
            os.environ["OPENAI_BASE_URL"] = base_url
            os.environ["OPENAI_API_KEY"] = self.custom_openai_api_key

            print(f"üîß Configured custom OpenAI endpoint: {base_url}")
            print(f"   API Key: {self.custom_openai_api_key}")

            # Auto-detect if this looks like a fine-tuned model and add ft: regex support
            if self.model_name and (
                self.model_name.startswith("ft:") or "lora" in self.model_name.lower()
            ):
                self._add_ft_regex_support()

    def _add_ft_regex_support(self):
        """Add ft: regex pattern to OpenAI naming regexes if not already present."""
        try:
            import re
            from synth_ai.lm.core import vendor_clients

            # Check if ft: pattern already exists
            ft_pattern = re.compile(r"^ft:.*$")
            if not any(
                pattern.pattern == ft_pattern.pattern
                for pattern in vendor_clients.openai_naming_regexes
            ):
                # Add ft: pattern at the beginning to catch all fine-tuned models
                vendor_clients.openai_naming_regexes.insert(0, ft_pattern)
                print(f"‚úÖ Added ft:* regex pattern for fine-tuned model support")
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: Could not add ft: regex pattern: {e}")

    def set_custom_endpoint(self, base_url: str, api_key: str = "dummy"):
        """Programmatically set custom OpenAI endpoint."""
        self.custom_openai_base_url = base_url
        self.custom_openai_api_key = api_key
        self._configure_custom_openai()


# --- Global Config ---
config = CrafterConfig()


# --- Helper to build crafter semantic mapping ---
def get_crafter_semantic_mapping():
    """Build the crafter semantic ID to item name mapping."""
    try:
        import crafter
        import itertools

        # Create a dummy env to get ID mappings
        dummyenv = None
        try:
            dummyenv = crafter.Env()
            max_id = (
                max(
                    max(dummyenv._world._mat_ids.values()),
                    max(dummyenv._sem_view._obj_ids.values()),
                )
                + 1
            )
            id_to_item = ["void"] * max_id
            for name, ind in itertools.chain(
                dummyenv._world._mat_ids.items(), dummyenv._sem_view._obj_ids.items()
            ):
                if name is None:
                    clean = "none"
                elif hasattr(name, "__name__"):
                    clean = name.__name__
                else:
                    clean = str(name)
                id_to_item[ind] = clean.lower()
            player_idx = id_to_item.index("player")
            return id_to_item, player_idx
        finally:
            if dummyenv:
                try:
                    dummyenv.close()
                except Exception:
                    pass
            del dummyenv
    except ImportError:
        # Fallback if crafter is not available
        return None, None


def format_semantic_map_view(obs_data: Dict[str, Any], view_size: int = 7) -> str:
    """Format a semantic map view around the player (ASCII)."""
    try:
        # Get mapping list
        id_to_item, _ = get_crafter_semantic_mapping()
        if id_to_item is None:
            return "Map view unavailable (crafter not installed)"

        semantic_map = obs_data.get("semantic_map")
        player_position = obs_data.get("player_position", [0, 0])

        if semantic_map is None:
            return "Map view unavailable (no semantic map data)"

        # Ensure numpy array with 2 dimensions
        sem_arr = np.asarray(semantic_map)
        if sem_arr.ndim == 1:
            # Probably flattened; try to infer square size
            size = int(np.sqrt(sem_arr.size))
            sem_arr = sem_arr.reshape(size, size)
        elif sem_arr.ndim != 2:
            return "Map view unavailable (invalid map dimensionality)"

        px, py = map(int, player_position)
        half = view_size // 2

        rows = []
        visible = set()
        for dy in range(-half, half + 1):
            row_tokens = []
            for dx in range(-half, half + 1):
                x, y = px + dx, py + dy
                if 0 <= x < sem_arr.shape[0] and 0 <= y < sem_arr.shape[1]:
                    if dx == 0 and dy == 0:
                        token = "player"
                    else:
                        idx = int(sem_arr[x, y])
                        token = id_to_item[idx] if idx < len(id_to_item) else "?"
                else:
                    token = "void"
                row_tokens.append(token)
                if token not in {"void", "player"}:
                    visible.add(token)
            rows.append(" ".join(row_tokens))

        map_view = f"\nLocal Map View ({view_size}x{view_size}):\n" + "\n".join(rows)
        if visible:
            map_view += "\nVisible items: " + ", ".join(sorted(visible))
        else:
            map_view += "\nNo special items visible (mostly grass/empty)"
        return map_view
    except Exception as e:
        return f"Map view error: {e}"


# --- Shaped Reward Configuration ---
# K-values for shaped reward calculation: reward = sum(K * log(count)) for each achievement
ACHIEVEMENT_K_VALUES = {
    "collect_coal": 3.0,
    "collect_diamond": 100.0,
    "collect_drink": 0.1,
    "collect_iron": 10.0,
    "collect_sapling": 0.1,
    "collect_stone": 1.0,
    "collect_wood": 1.0,
    "defeat_skeleton": 1.0,
    "defeat_zombie": 1.0,
    "eat_cow": 1.0,
    "eat_plant": 0.1,
    "make_iron_pickaxe": 30.0,
    "make_iron_sword": 30.0,
    "make_stone_pickaxe": 10.0,
    "make_stone_sword": 10.0,
    "make_wood_pickaxe": 3.0,
    "make_wood_sword": 3.0,
    "place_furnace": 10.0,
    "place_plant": 0.1,
    "place_stone": 1.0,
    "place_table": 3.0,
    "wake_up": 0.1,
}


# --- Tool Definitions ---
def get_openai_tools():
    """Get OpenAI-compatible tool definitions."""
    return [
        {
            "type": "function",
            "function": {
                "name": "interact",
                "description": "Perform 1-5 actions in sequence in the Crafter environment.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "actions": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "List of 1-5 action names to execute in sequence (e.g., ['move_up', 'do', 'mine_down'])"
                        },
                        "reasoning": {
                            "type": "string",
                            "description": "Brief explanation of why these actions were chosen"
                        }
                    },
                    "required": ["actions", "reasoning"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "terminate",
                "description": "End the episode when finished or no progress can be made.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "reason": {
                            "type": "string",
                            "description": "Reason for termination"
                        }
                    },
                    "required": ["reason"]
                }
            }
        }
    ]


# --- Shaped Reward Helper ---
def calculate_shaped_reward(achievement_counts: Dict[str, int]) -> Dict[str, Any]:
    """Calculate shaped reward using K * log(count) for each achievement."""
    total_reward = 0.0
    reward_breakdown = {}

    for achievement, count in achievement_counts.items():
        if count > 0 and achievement in ACHIEVEMENT_K_VALUES:
            k_value = ACHIEVEMENT_K_VALUES[achievement]
            # Use log(count + 1) to handle count=0 case gracefully
            reward_contribution = k_value * math.log(count + 1)
            total_reward += reward_contribution
            reward_breakdown[achievement] = {
                "count": count,
                "k_value": k_value,
                "contribution": reward_contribution,
            }

    return {"total_shaped_reward": total_reward, "breakdown": reward_breakdown}


# --- Base ReAct Agent ---
class BaseReActAgent:
    """Base ReAct agent for environment interaction."""

    def __init__(self, model_name: str, max_turns: int = 20, verbose: bool = False, tracer: Optional[SessionTracer] = None):
        self.model_name = model_name
        self.max_turns = max_turns
        self.verbose = verbose
        self.history = []
        self.system_name = "base-react-agent"
        self.tools = get_openai_tools()
        self.tracer = tracer
        # Unique system ID for this agent instance
        import uuid
        self.system_id = uuid.uuid4()
        
        # Agent state tracking
        self.agent_state = {
            "message_history": [],  # LLM conversation history
            "steps_taken": 0,
            "steps_remaining": max_turns,
            "total_tokens_used": 0,
            "tool_calls_made": 0,
            "current_turn": 0
        }

    def decide(self, obs: str, system_message: str, turn: int) -> Dict[str, Any]:
        """Get agent decision based on observation."""
        # Update agent state
        self.agent_state["current_turn"] = turn
        self.agent_state["steps_taken"] = turn
        self.agent_state["steps_remaining"] = self.max_turns - turn
        
        # Create conversation context
        context = f"Turn {turn + 1}/{self.max_turns}\n\n{obs}"
        messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": context}
        ]
        
        # Only keep the last N messages to avoid huge state
        max_history_length = 20
        if len(self.agent_state["message_history"]) > max_history_length:
            # Keep system message and last N-1 messages
            self.agent_state["message_history"] = (
                [self.agent_state["message_history"][0]] +  # Keep first system message
                self.agent_state["message_history"][-(max_history_length-1):]
            )
        
        # Add current messages to history
        self.agent_state["message_history"].append({"role": "system", "content": system_message})
        self.agent_state["message_history"].append({"role": "user", "content": context})
        
        # Capture system state before LLM call (compress message history)
        system_state_before = self.agent_state.copy()
        # Truncate message history in the saved state to save space
        if "message_history" in system_state_before and len(system_state_before["message_history"]) > 4:
            system_state_before["message_history"] = (
                system_state_before["message_history"][:2] +  # First 2 messages
                ["... truncated ..."] +
                system_state_before["message_history"][-2:]  # Last 2 messages
            )
        
        # Use langfuse generation (langfuse 3.x API - not context manager)
        # Suppress all stdout during Langfuse operations
        import sys
        import io
        old_stdout = sys.stdout
        old_stderr = sys.stderr
        sys.stdout = io.StringIO()
        sys.stderr = io.StringIO()
        try:
            langfuse_client = Langfuse()
        finally:
            sys.stdout = old_stdout
            sys.stderr = old_stderr
        
        # Create generation (not as context manager)
        generation = langfuse_client.generation(
            name=f"crafter_agent_turn_{turn}",
            model=self.model_name,
            input=messages,
            metadata={
                "turn": turn,
                "agent_type": self.system_name,
                "tools_available": len(self.tools)
            }
        )
        
        # Store langfuse client for cleanup
        self._langfuse_client = langfuse_client
        
        try:
            # Generate response using OpenAI client (v1.0+ API)
            response = openai.chat.completions.create(
                model=self.model_name,
                messages=messages,
                tools=self.tools,
                tool_choice="auto",
                temperature=0.0
            )
            
            # Update generation with output
            generation.update(
                output=response.choices[0].message.model_dump() if response.choices else None,
                usage={
                    "prompt_tokens": response.usage.prompt_tokens if response.usage else None,
                    "completion_tokens": response.usage.completion_tokens if response.usage else None,
                    "total_tokens": response.usage.total_tokens if response.usage else None
                }
            )
            
            # End the generation
            generation.end()

            # Extract tool calls from response
            tool_calls = response.choices[0].message.tool_calls

            # Handle case where tool_calls is None or empty (graceful fallback)
            if not tool_calls:
                if self.verbose:
                    print(f"[WARNING] No tool calls returned by LLM, using default action")
                decision = {
                    "name": "interact",
                    "parameters": {
                        "actions": ["do"],
                        "reasoning": "Default action - no tool call received",
                    },
                }
            else:
                tool_call_data = tool_calls[0]
                tool_name = tool_call_data.function.name
                tool_arguments = json.loads(tool_call_data.function.arguments)
                decision = {"name": tool_name, "parameters": tool_arguments}
            
            # Update agent state with response
            if response.usage:
                self.agent_state["total_tokens_used"] += response.usage.total_tokens
            self.agent_state["tool_calls_made"] += 1
            
            # Add assistant message to history
            assistant_message = {
                "role": "assistant",
                "content": response.choices[0].message.content if response.choices else None,
                "tool_calls": [
                    {
                        "id": tc.id,
                        "function": {
                            "name": tc.function.name,
                            "arguments": tc.function.arguments
                        },
                        "type": tc.type
                    } for tc in tool_calls
                ] if tool_calls else []
            }
            self.agent_state["message_history"].append(assistant_message)

            # Capture system state after LLM call (compress message history)
            system_state_after = self.agent_state.copy()
            # Truncate message history in the saved state to save space
            if "message_history" in system_state_after and len(system_state_after["message_history"]) > 4:
                system_state_after["message_history"] = (
                    system_state_after["message_history"][:2] +  # First 2 messages
                    ["... truncated ..."] +
                    system_state_after["message_history"][-2:]  # Last 2 messages
                )
            
            # Record LLM call as a CAISEvent (internal to agent, NOT a message)
            if self.tracer:
                try:
                    # Create LLM call record with all info needed to reproduce
                    llm_call_record = {
                        "model": self.model_name,
                        "messages": messages,
                        "tools": self.tools,
                        "tool_choice": "auto",
                        "temperature": 0.0,
                        "response": {
                            "id": response.id if response else None,
                            "choices": [{
                                "index": 0,
                                "message": {
                                    "role": "assistant",
                                    "content": response.choices[0].message.content if response.choices else None,
                                    "tool_calls": [
                                        {
                                            "id": tc.id,
                                            "function": {
                                                "name": tc.function.name,
                                                "arguments": tc.function.arguments
                                            },
                                            "type": tc.type
                                        } for tc in tool_calls
                                    ] if tool_calls else None
                                },
                                "finish_reason": response.choices[0].finish_reason if response.choices else None
                            }] if response.choices else [],
                            "usage": {
                                "prompt_tokens": response.usage.prompt_tokens,
                                "completion_tokens": response.usage.completion_tokens,
                                "total_tokens": response.usage.total_tokens
                            } if response.usage else None
                        }
                    }
                    
                    llm_event = CAISEvent(
                        time_record=TimeRecord(
                            event_time=datetime.now().isoformat(),
                            message_time=turn
                        ),
                        system_instance_id=f"{self.system_name}_{self.model_name}",
                        system_state_before=system_state_before,
                        system_state_after=system_state_after,
                        llm_call_records=[llm_call_record],  # Include the LLM call record
                        metadata={
                            "model_name": self.model_name,
                            "prompt_tokens": response.usage.prompt_tokens if response.usage else None,
                            "completion_tokens": response.usage.completion_tokens if response.usage else None,
                            "total_tokens": response.usage.total_tokens if response.usage else None,
                            "turn": turn
                        }
                    )
                    
                    #print(f"ü§ñ Created CAISEvent: {llm_event.system_instance_id}, has tracer: {self.tracer is not None}")
                    
                    if hasattr(self.tracer, 'current_session') and self.tracer.current_session:
                        self.tracer.record_event(llm_event)
                        # Store the last event for progress tracking
                        self.last_cais_event = llm_event
                        if self.verbose:
                            print(f"‚úÖ Added CAISEvent for turn {turn}")
                    else:
                        if self.verbose:
                            print(f"‚ö†Ô∏è No current_session in tracer")
                        
                except Exception as e:
                    if self.verbose:
                        print(f"Warning: Failed to capture LLM event: {e}")
                        import traceback
                        traceback.print_exc()
                    # Always print errors for debugging
                    print(f"‚ùå Error adding CAISEvent: {e}")
                    import traceback
                    traceback.print_exc()
        
        except Exception as e:
            raise e

        return decision


# --- Crafter ReAct Agent ---
class CrafterReActAgent(BaseReActAgent):
    """ReAct agent for Crafter environment."""

    def __init__(self, model_name: str, max_turns: int = 20, verbose: bool = False, tracer: Optional[SessionTracer] = None):
        super().__init__(model_name, max_turns, verbose, tracer)
        self.system_name = "crafter-react-agent"

    def get_system_message(self) -> str:
        return """You are CrafterAgent playing Crafter survival environment. Your goal is to unlock as many achievements as possible while staying alive.

You will see a semantic map view showing your surroundings. Use this to navigate toward resources.

Key mechanics:
‚Ä¢ 'do' action: collect wood from trees, stone from deposits, food from cows/plants
‚Ä¢ 'do' does nothing on grass/water - move to find resources first
‚Ä¢ Craft progression: wood ‚Üí table ‚Üí wood_pickaxe ‚Üí stone ‚Üí stone_pickaxe ‚Üí iron tools
‚Ä¢ Sleep when energy low to restore and unlock wake_up achievement
‚Ä¢ Use semantic map view to navigate toward resources you can see

Available actions: move_left, move_right, move_up, move_down, do, sleep, place_stone, place_table, place_furnace, place_plant, make_wood_pickaxe, make_stone_pickaxe, make_iron_pickaxe, make_wood_sword, make_stone_sword, make_iron_sword, noop

Strategy:
1. Look at the semantic map to see what's around you
2. Move toward trees to collect wood with 'do'
3. Once you have wood, place a table to enable crafting
4. Make a wood pickaxe to collect stone more efficiently
5. Progress to stone pickaxe, then iron tools
6. Eat food when health is low, sleep when energy is low

You should provide 1-5 actions in sequence for efficient gameplay. Use the semantic map view to navigate toward visible resources.

Example good action sequences:
- ['move_right', 'move_right', 'do'] (move to tree and collect wood)
- ['place_table', 'make_wood_pickaxe'] (craft progression)
- ['move_up', 'do', 'move_down', 'do'] (collect from multiple resources)

Be strategic and use the map view to find resources! Focus on unlocking achievements."""

    def format_observation(self, obs: Dict[str, Any]) -> str:
        """Format observation for Crafter with rich context."""
        # Extract key information from observation
        health = obs.get("health", 0)
        inventory = obs.get("inventory", {})

        # Extract health from inventory if not in main obs
        if health == 0 and "health" in inventory:
            health = inventory["health"]

        # Format inventory items (exclude health since we show it separately)
        inventory_items = []
        for item, count in inventory.items():
            if count > 0 and item != "health":
                inventory_items.append(f"{item}: {count}")

        inventory_str = ", ".join(inventory_items) if inventory_items else "empty"

        # Get achievements
        achievements = obs.get("achievements") or obs.get("achievements_status", {})
        unlocked_achievements = [name for name, unlocked in achievements.items() if unlocked]
        achievements_str = ", ".join(unlocked_achievements) if unlocked_achievements else "none"

        # Get position and other state
        position = obs.get("position", [0, 0])
        player_position = obs.get("player_position", position)
        player_direction = obs.get("player_direction", [0, 1])
        num_steps = obs.get("num_steps_taken", 0)

        # Check termination status
        terminated = obs.get("terminated", False)

        # Get semantic map view
        map_view = format_semantic_map_view(obs, view_size=5)

        return (
            f"Crafter Game State:\n"
            f"Step: {num_steps}\n"
            f"Health: {health}\n"
            f"Position: {player_position}\n"
            f"Direction: {player_direction}\n"
            f"Inventory: {inventory_str}\n"
            f"Achievements: {achievements_str}\n"
            f"Terminated: {terminated}\n"
            f"{map_view}\n\n"
            f"Available actions: move_left, move_right, move_up, move_down, do, sleep, place_stone, place_table, place_furnace, place_plant, make_wood_pickaxe, make_stone_pickaxe, make_iron_pickaxe, make_wood_sword, make_stone_sword, make_iron_sword, noop\n\n"
            f"Key mechanics:\n"
            f"‚Ä¢ 'do' action: collect wood from trees, stone from deposits, food from cows/plants\n"
            f"‚Ä¢ 'do' does nothing on grass/water - move to find resources\n"
            f"‚Ä¢ Craft progression: wood ‚Üí table ‚Üí wood_pickaxe ‚Üí stone ‚Üí stone_pickaxe ‚Üí iron tools\n"
            f"‚Ä¢ Sleep when energy low to restore and unlock wake_up achievement\n\n"
            f"Choose 1-5 actions to execute in sequence. Focus on exploring to find resources and crafting tools to unlock achievements."
        )


# --- Episode Runner ---
async def run_single_episode(
    client: AsyncClient, agent: CrafterReActAgent, task_instance, instance_num: int, traces_dir: str = "traces", 
    config=None, episode_progress_bars=None, all_achievements=None, all_invalid_actions=None, all_total_actions=None
) -> Dict[str, Any]:
    """Run a single Crafter episode and return episode metrics."""
    # Create session tracer for this episode with hooks
    tracer = SessionTracer(traces_dir, hooks=CRAFTER_HOOKS)
    session_id = f"crafter_episode_{instance_num}_{task_instance.id}"
    tracer.start_session(session_id)
    
    # Progress bars already initialized in batch processing
    
    # Create system IDs
    import uuid
    runtime_id = uuid.uuid4()  # Runtime converts tool calls to actions
    environment_id = task_instance.id  # Use task instance ID for environment
    
    # Add episode metadata
    tracer.add_session_metadata("episode_config", {
        "instance_num": instance_num,
        "task_instance_id": str(task_instance.id),
        "difficulty": task_instance.metadata.difficulty,
        "max_turns": agent.max_turns,
        "model_name": agent.model_name,
        "agent_type": agent.system_name
    })
    
    # Add system ID mapping
    tracer.add_session_metadata("system_ids", {
        "agent": str(agent.system_id),
        "runtime": str(runtime_id),
        "environment": str(environment_id)
    })
    
    # Update agent with tracer
    agent.tracer = tracer
    
    try:
        # Create environment using the task instance
        create_resp = await client.post(
            f"/env/CrafterCustom/initialize",
            json={"task_instance": await task_instance.serialize()},
        )

        if create_resp.status_code != 200:
            print(f"  ‚ùå Failed to create environment: {create_resp.status_code}")
            print(f"  Response: {create_resp.text}")
            return {
                "eval_metric": 0.0,
                "rubric": {},
                "total_reward": 0.0,
                "num_achievements": 0,
                "terminated": False,
                "error": True,
            }

        env_id = create_resp.json()["env_id"]
        
        # Get and verify environment metadata
        metadata_resp = await client.get(
            f"/env/CrafterCustom/metadata",
            params={"env_id": env_id}
        )
        if metadata_resp.status_code == 200:
            metadata = metadata_resp.json()
            if config.verbose:
                print(f"\n  üìä Environment metadata for instance {instance_num}:")
                print(f"     World config: {metadata.get('world_config', 'N/A')}")
                print(f"     Seed: {metadata.get('engine_seed', 'N/A')}")
                if 'world_object_counts' in metadata:
                    print(f"     Objects: {metadata['world_object_counts']}")

        # Get initial observation
        obs = create_resp.json()["observation"]
        formatted_obs = agent.format_observation(obs)

        # DEBUG: Print initial state (minimal)
        # print(
        #     f"\n  Instance {instance_num}: Starting Crafter survival ({task_instance.metadata.difficulty}, {agent.max_turns} turns max)"
        # )

        # Track episode metrics
        total_reward = 0.0
        final_achievements = {}
        num_achievements = 0
        terminated = False
        rollout_length = 0
        termination_reason = "max_turns_reached"  # Default assumption

        # Run episode without tqdm progress bar (we use custom progress display)
        # Disable tqdm to avoid conflicts with custom progress bars
        episode_progress = range(agent.max_turns)
        
        for turn in episode_progress:
            try:
                # Create timestep for this turn (needed for record_event to work)
                if tracer:
                    tracer.start_timestep(f"turn_{turn}")
                
                # Record observation message (Environment ‚Üí Runtime ‚Üí Agent)
                obs_for_trace = compress_observation_for_trace(obs)
                
                obs_message = create_message(
                    content=obs_for_trace,  # Compressed observation
                    message_type="observation",
                    origin_system_id=environment_id,  # From environment
                    turn=turn
                )
                # Add message directly to session history
                if tracer.current_session:
                    tracer.current_session.add_message(obs_message)
                
                # Get agent decision
                action = agent.decide(formatted_obs, agent.get_system_message(), turn)
                # print(f"  ‚úÖ Agent decision received: {action}")

                # Record tool call message (Agent ‚Üí Runtime)
                tool_call_message = create_message(
                    content=[{
                        "tool": action["name"],
                        "args": action["parameters"]
                    }],
                    message_type="tool_call",
                    origin_system_id=agent.system_id,  # From agent
                    turn=turn
                )
                # Add message directly to session history
                if tracer.current_session:
                    tracer.current_session.add_message(tool_call_message)

                # # DEBUG: Print agent decision with safer access
                # try:
                #     actions = action.get('parameters', {}).get('actions', action.get('arguments', {}).get('actions', []))
                #     reasoning = action.get('parameters', {}).get('reasoning', action.get('arguments', {}).get('reasoning', 'no reasoning'))
                #     #print(f"  Turn {turn+1}: Agent chose {actions} - {reasoning}")
                # except Exception as e:
                #     print(f"  Turn {turn+1}: Agent action structure: {action}")
                #     print(f"  Error parsing action: {e}")

                # Check for termination
                if action["name"] == "terminate":
                    reason = action.get("parameters", {}).get(
                        "reason", action.get("arguments", {}).get("reason", "no reason given")
                    )
                    termination_reason = f"agent_terminate: {reason}"
                    # Don't update tqdm since we're using custom progress display
                    pass
                    break

                # Execute actions in environment with safer access
                action_sequence = action.get("parameters", {}).get(
                    "actions", action.get("arguments", {}).get("actions", [])
                )
                if not action_sequence:
                    print(f"  ‚ö†Ô∏è  No actions found in: {action}")
                    termination_reason = "no_actions_provided"
                    break

                # Convert action names to integers using the proper action map
                # Define the proper Crafter action mapping
                CRAFTER_ACTION_MAP = {
                    "noop": 0,
                    "move_left": 1,
                    "move_right": 2,
                    "move_up": 3,
                    "move_down": 4,
                    "do": 5,
                    "sleep": 6,
                    "place_stone": 7,
                    "place_table": 8,
                    "place_furnace": 9,
                    "place_plant": 10,
                    "make_wood_pickaxe": 11,
                    "make_stone_pickaxe": 12,
                    "make_iron_pickaxe": 13,
                    "make_wood_sword": 14,
                    "make_stone_sword": 15,
                    "make_iron_sword": 16,
                }

                action_ints = []
                for action_name in action_sequence:
                    if action_name in CRAFTER_ACTION_MAP:
                        action_int = CRAFTER_ACTION_MAP[action_name]
                    else:
                        action_int = 0  # Default to noop
                    action_ints.append(action_int)

                # Record runtime event before executing actions
                prev_obs = compress_observation_for_trace(obs)
                
                runtime_event = RuntimeEvent(
                    system_state_before={"observation": prev_obs},
                    actions=action_ints,
                    metadata={
                        "action_names": action_sequence,
                        "action_reasoning": action.get("parameters", {}).get("reasoning", ""),
                        "turn": turn
                    }
                )

                # Record action messages (Runtime ‚Üí Environment)
                for action_int in action_ints:
                    action_message = create_message(
                        content={
                            "action": action_int,
                            "action_type": "crafter_action"
                        },
                        message_type="action",
                        origin_system_id=runtime_id,  # From runtime
                        turn=turn
                    )
                    if tracer.current_session:
                        tracer.current_session.add_message(action_message)
                
                # Execute each action individually (Crafter expects single actions)
                for i, action_int in enumerate(action_ints):
                    step_resp = await client.post(
                        f"/env/CrafterCustom/step",
                        json={
                            "env_id": env_id,
                            "request_id": str(uuid.uuid4()),
                            "action": {
                                "tool_calls": [{"tool": "interact", "args": {"action": action_int}}]
                            },
                        },
                    )

                    if step_resp.status_code != 200:
                        print(
                            f"    ‚ùå Action {i + 1} failed: {step_resp.status_code}: {step_resp.text}"
                        )
                        termination_reason = f"http_error: {step_resp.status_code}"
                        break

                    # Update observation after each action
                    obs = step_resp.json()["observation"]

                # Check final response status
                if step_resp.status_code != 200:
                    termination_reason = f"http_error: {step_resp.status_code}"
                    break

                # Convert observation to compressed format before saving
                obs_for_trace = compress_observation_for_trace(obs)
                
                # Record runtime event now that we have the final state
                runtime_event.system_state_after = {"observation": obs_for_trace}
                if tracer.current_session:
                    tracer.record_event(runtime_event)

                # Record environment event for the state transition
                # Extract public and private state from observations
                prev_public_state = {
                    k: v for k, v in prev_obs.items() 
                    if k not in ["reward_last_step", "total_reward_episode", "terminated", "truncated", "tool_error"]
                }
                prev_private_state = {
                    "reward_last_step": prev_obs.get("reward_last_step", 0.0),
                    "total_reward_episode": prev_obs.get("total_reward_episode", 0.0),
                    "terminated": prev_obs.get("terminated", False),
                    "truncated": prev_obs.get("truncated", False)
                }
                
                new_public_state = {
                    k: v for k, v in obs_for_trace.items() 
                    if k not in ["reward_last_step", "total_reward_episode", "terminated", "truncated", "tool_error"]
                }
                new_private_state = {
                    "reward_last_step": obs_for_trace.get("reward_last_step", 0.0),
                    "total_reward_episode": obs_for_trace.get("total_reward_episode", 0.0),
                    "terminated": obs_for_trace.get("terminated", False),
                    "truncated": obs_for_trace.get("truncated", False)
                }
                
                env_event = EnvironmentEvent(
                    time_record=TimeRecord(
                        event_time=datetime.now().isoformat(),
                        message_time=turn
                    ),
                    system_instance_id=str(environment_id),
                    system_state_before={
                        "public_state": prev_public_state,
                        "private_state": prev_private_state
                    },
                    system_state_after={
                        "public_state": new_public_state,
                        "private_state": new_private_state
                    },
                    reward=obs.get("reward_last_step", 0.0),
                    terminated=obs.get("terminated", False),
                    metadata={
                        "actions_executed": action_sequence,
                        "turn": turn
                    }
                )
                if tracer.current_session:
                    tracer.record_event(env_event)

                # Show final state after all actions
                formatted_obs = agent.format_observation(obs)
                step_count = obs.get("num_steps_taken", 0)
                rollout_length = step_count
                position = obs.get("player_position", [0, 0])
                # print(f"  Turn {turn+1}: Actions completed - Step: {step_count}, Position: {position}")
                
                # Track progress for this turn
                if episode_progress_bars is not None:
                    # Check what hooks fired by looking at the events we just created
                    step_code = '#'  # Default for regular step
                    
                    # Collect all hooks that fired with their priorities
                    all_hooks = []
                    
                    # Check environment event hooks
                    if env_event and env_event.event_metadata:
                        for meta in env_event.event_metadata:
                            if 'code' in meta:
                                all_hooks.append((meta.get('priority', 0), meta.get('code', '?')))
                    
                    # Check agent hooks from the agent's recorded event
                    if hasattr(agent, 'last_cais_event') and agent.last_cais_event and agent.last_cais_event.event_metadata:
                        for meta in agent.last_cais_event.event_metadata:
                            if 'code' in meta:
                                all_hooks.append((meta.get('priority', 0), meta.get('code', '?')))
                    
                    # Select the hook with highest priority
                    if all_hooks:
                        all_hooks.sort(key=lambda x: x[0], reverse=True)  # Sort by priority descending
                        step_code = all_hooks[0][1]  # Get code of highest priority hook
                    
                    episode_progress_bars[instance_num].append(step_code)
                    
                    # Update the display
                    if hasattr(config, 'max_turns'):
                        update_progress_display(episode_progress_bars, config.max_turns)

                # Update history with safer access
                reasoning = action.get("parameters", {}).get(
                    "reasoning", action.get("arguments", {}).get("reasoning", "")
                )
                agent.history.append(f"{', '.join(action_sequence)}: {reasoning[:50]}")

                # Track episode progress - Use the FINAL observation from the last action
                terminated = obs.get("terminated", False)
                step_reward = obs.get("reward", 0.0)
                total_reward += step_reward
                achievements = obs.get("achievements") or obs.get("achievements_status", {})

                # ALWAYS update final_achievements with the latest observation
                final_achievements = achievements

                num_achievements = sum(1 for v in achievements.values() if v) if achievements else 0

                # Update progress bar description with achievements
                # Don't update tqdm since we're using custom progress display
                pass

                # No need to advance turn - we're tracking via message_time

                if terminated:
                    # Check if it's death or other environment termination
                    health = obs.get("health", 9)  # Default health value
                    if health <= 0:
                        termination_reason = "death"
                    else:
                        termination_reason = "environment_terminated"
                    break

            except Exception as e:
                # Error occurred
                termination_reason = f"exception: {str(e)[:50]}"
                break
        
        # No need to close progress bar since we're not using tqdm
        pass  # Already closed

        # Cleanup
        await client.post(f"/env/CrafterClassic/terminate", json={"env_id": env_id})

        # Add final episode metadata to trace
        tracer.add_session_metadata("episode_results", {
            "total_reward": total_reward,
            "num_achievements": num_achievements,
            "achievements": final_achievements,
            "rollout_length": rollout_length,
            "terminated": terminated
        })
        
        # Capture session before ending it
        session_to_save = tracer.current_session
        
        # End session but don't save yet - we'll clean it up first
        tracer.end_session(save=False)
        
        # Clean up the trace to remove session_time_steps and save manually
        if session_to_save:
            trace_dict = session_to_save.to_dict()
            # Remove session_time_steps as we're not using them
            trace_dict.pop('session_time_steps', None)
            
            # Save the cleaned trace manually
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"session_{session_id}_{timestamp}.json"
            filepath = tracer.traces_dir / filename
            
            with open(filepath, 'w') as f:
                json.dump(trace_dict, f, indent=2, default=str)
            
            #print(f"üìÅ Saved session trace: {filepath}")
        
        # Clear the tracer reference to allow garbage collection
        agent.tracer = None

        # Track achievements globally
        if all_achievements is not None and final_achievements:
            for achievement, unlocked in final_achievements.items():
                if unlocked:
                    all_achievements[achievement] += 1
        
        # Track invalid actions globally
        if all_invalid_actions is not None and tracer.hooks:
            # Find the InvalidActionHook and get its tracked actions
            for hook in tracer.hooks:
                if hasattr(hook, '__class__') and hook.__class__.__name__ == 'InvalidActionHook':
                    if hasattr(hook, 'invalid_actions'):
                        for action, count in hook.invalid_actions.items():
                            all_invalid_actions[action] += count
                    if hasattr(hook, 'total_actions') and all_total_actions is not None:
                        for action, count in hook.total_actions.items():
                            all_total_actions[action] += count
        
        # Calculate K-weighted achievement reward
        achievement_reward = 0.0
        if final_achievements:
            for achievement, unlocked in final_achievements.items():
                if unlocked and achievement in ACHIEVEMENT_K_VALUES:
                    k_value = ACHIEVEMENT_K_VALUES[achievement]
                    achievement_reward += k_value * math.log(2)  # log(1+1) for single achievement

        # Use achievement reward as the total reward
        total_reward = achievement_reward

        # Calculate eval metric and rubric
        eval_metric = float(num_achievements)  # Simple metric: number of achievements

        # Create rubric with specific achievement checks
        rubric = {}
        if final_achievements:
            rubric = {
                "collect_wood": 1.0 if final_achievements.get("collect_wood", False) else 0.0,
                "collect_stone": 1.0 if final_achievements.get("collect_stone", False) else 0.0,
                "collect_coal": 1.0 if final_achievements.get("collect_coal", False) else 0.0,
                "collect_iron": 1.0 if final_achievements.get("collect_iron", False) else 0.0,
                "collect_diamond": 1.0 if final_achievements.get("collect_diamond", False) else 0.0,
                "place_table": 1.0 if final_achievements.get("place_table", False) else 0.0,
                "place_furnace": 1.0 if final_achievements.get("place_furnace", False) else 0.0,
                "make_wood_pickaxe": 1.0
                if final_achievements.get("make_wood_pickaxe", False)
                else 0.0,
                "make_stone_pickaxe": 1.0
                if final_achievements.get("make_stone_pickaxe", False)
                else 0.0,
                "make_iron_pickaxe": 1.0
                if final_achievements.get("make_iron_pickaxe", False)
                else 0.0,
                "make_wood_sword": 1.0 if final_achievements.get("make_wood_sword", False) else 0.0,
                "make_stone_sword": 1.0
                if final_achievements.get("make_stone_sword", False)
                else 0.0,
                "make_iron_sword": 1.0 if final_achievements.get("make_iron_sword", False) else 0.0,
                "defeat_skeleton": 1.0 if final_achievements.get("defeat_skeleton", False) else 0.0,
                "defeat_zombie": 1.0 if final_achievements.get("defeat_zombie", False) else 0.0,
                "wake_up": 1.0 if final_achievements.get("wake_up", False) else 0.0,
                "eat_cow": 1.0 if final_achievements.get("eat_cow", False) else 0.0,
                "eat_plant": 1.0 if final_achievements.get("eat_plant", False) else 0.0,
            }
        else:
            # Default rubric with all zeros
            rubric = {
                "collect_wood": 0.0,
                "collect_stone": 0.0,
                "collect_coal": 0.0,
                "collect_iron": 0.0,
                "collect_diamond": 0.0,
                "place_table": 0.0,
                "place_furnace": 0.0,
                "make_wood_pickaxe": 0.0,
                "make_stone_pickaxe": 0.0,
                "make_iron_pickaxe": 0.0,
                "make_wood_sword": 0.0,
                "make_stone_sword": 0.0,
                "make_iron_sword": 0.0,
                "defeat_skeleton": 0.0,
                "defeat_zombie": 0.0,
                "wake_up": 0.0,
                "eat_cow": 0.0,
                "eat_plant": 0.0,
            }

        return {
            "eval_metric": eval_metric,
            "rubric": rubric,
            "total_reward": total_reward,
            "num_achievements": num_achievements,
            "achievements": final_achievements,
            "rollout_length": rollout_length,
            "terminated": terminated,
            "termination_reason": termination_reason,
            "error": False,
        }

    except Exception as e:
        # Save trace even on error
        try:
            tracer.add_session_metadata("error", {"error_message": str(e)})
            tracer.end_session(save=True)
        except:
            pass  # Don't let trace saving errors mask the original error
            
        return {
            "eval_metric": 0.0,
            "rubric": {},
            "total_reward": 0.0,
            "num_achievements": 0,
            "terminated": False,
            "termination_reason": f"outer_exception: {str(e)[:50]}",
            "error": True,
        }


# --- Batch Evaluation ---
async def evaluate_crafter_batch() -> Dict[str, Any]:
    """Evaluate Crafter agent on multiple easy instances."""
    print(f"üéØ Evaluating Crafter on {config.num_instances} {config.difficulty} instances...")
    
    # Create traces directory in the same directory as the script
    script_dir = Path(__file__).parent
    traces_dir = script_dir / "traces"
    traces_dir.mkdir(exist_ok=True)
    print(f"  üìÅ Traces will be saved to: {traces_dir}")
    
    # Initialize progress tracking
    episode_progress_bars = {}  # Store progress info for each episode
    all_achievements = defaultdict(int)  # Track all achievements across episodes
    all_invalid_actions = defaultdict(int)  # Track all invalid actions across episodes
    all_total_actions = defaultdict(int)  # Track all actions across episodes
    
    # Print hook legend
    print_hook_legend()
    
    # Track trace files created during this run
    initial_trace_files = set(traces_dir.glob("*.json"))
    
    # Initialize all episode progress bars upfront
    for i in range(config.num_instances):
        episode_progress_bars[i + 1] = []
    
    # Display all empty progress bars at once
    update_progress_display(episode_progress_bars, config.max_turns)

    # Load task instances from dataset
    from synth_ai.environments.examples.crafter_classic.taskset import CrafterTaskInstance
    
    dataset_path = Path(__file__).parent.parent / "dataset" / "instances.json"
    if not dataset_path.exists():
        print(f"  ‚ùå Dataset not found: {dataset_path}")
        return
    
    # Load all instances
    with open(dataset_path, 'r') as f:
        instances_data = json.load(f)
    
    # Filter instances based on configuration
    filtered_instances = []
    for instance_data in instances_data:
        metadata = instance_data.get('metadata', {})
        
        # Filter by difficulty if specified
        if config.difficulty and metadata.get('difficulty') != config.difficulty:
            continue
            
        # Filter by world config if specified
        if config.world_config != "normal":
            # Skip instances that don't match the requested world config
            if metadata.get('world_config') != config.world_config:
                continue
        
        # Filter by custom world config path if specified
        if config.world_config_path:
            if metadata.get('world_config_path') != config.world_config_path:
                continue
        
        # Deserialize the instance
        try:
            task_instance = await CrafterTaskInstance.deserialize(instance_data)
            
            # Override world config if specified via command line
            if config.world_config and config.world_config != "normal":
                task_instance.metadata.world_config = config.world_config
            if config.world_config_path:
                task_instance.metadata.world_config_path = config.world_config_path
                
            filtered_instances.append(task_instance)
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Failed to deserialize instance {instance_data.get('id')}: {e}")
            continue
    
    # Limit to requested number of instances
    easy_task_instances = filtered_instances[:config.num_instances]
    
    print(f"  üìù Found {len(filtered_instances)} matching instances (filtered from {len(instances_data)} total)")
    
    if not easy_task_instances:
        print(f"  ‚ùå No instances found matching filters (difficulty={config.difficulty}, world_config={config.world_config})")
        return
    
    # print(f"  üìù Generated {len(easy_task_instances)} {config.difficulty} task instances")

    async with AsyncClient(
        base_url=config.service_base_url, timeout=config.service_timeout
    ) as client:
        # Run all episodes with semaphore-controlled concurrency
        max_concurrent = 15
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def run_episode_with_concurrency(client, agent, task_instance, episode_num, traces_dir, config, progress_bars, achievements, invalid_actions, total_actions):
            async with semaphore:
                return await run_single_episode(client, agent, task_instance, episode_num, traces_dir, config, progress_bars, achievements, invalid_actions, total_actions)
        
        # Create ALL tasks at once for maximum parallelism
        all_tasks = []
        for i, task_instance in enumerate(easy_task_instances):
            agent = CrafterReActAgent(config.model_name, max_turns=config.max_turns, verbose=False)
            all_tasks.append(
                run_episode_with_concurrency(client, agent, task_instance, i + 1, str(traces_dir), 
                                            config, episode_progress_bars, all_achievements, all_invalid_actions, all_total_actions)
            )

        # Single gather call for optimal async performance
        results = await asyncio.gather(*all_tasks, return_exceptions=True)

        # Filter out error results and exceptions
        valid_results = []
        all_termination_reasons = []
        for r in results:
            if isinstance(r, Exception):
                print(f"  ‚ùå Episode failed with exception: {r}")
                all_termination_reasons.append(f"gather_exception: {str(r)[:50]}")
                continue
            if r.get("error", False):
                print(f"  ‚ùå Episode failed with error: {r.get('error')}")
                all_termination_reasons.append(r.get("termination_reason", "unknown_error"))
                continue
            valid_results.append(r)
            all_termination_reasons.append(r.get("termination_reason", "unknown"))

        if not valid_results:
            return {
                "eval_metrics": [],
                "mean_eval_metric": 0.0,
                "mean_rubric": {},
                "num_episodes": 0,
            }

        # Extract eval metrics and rubrics
        eval_metrics = [r["eval_metric"] for r in valid_results]
        mean_eval_metric = sum(eval_metrics) / len(eval_metrics)

        # --- Rollout length statistics ---
        rollout_lengths = [r["rollout_length"] for r in valid_results]
        sorted_lengths = sorted(rollout_lengths)
        n_lengths = len(sorted_lengths)
        # Median (Q2)
        if n_lengths % 2 == 1:
            q2_rollout = sorted_lengths[n_lengths // 2]
        else:
            q2_rollout = (sorted_lengths[n_lengths // 2 - 1] + sorted_lengths[n_lengths // 2]) / 2
        # 90th percentile (P90)
        p90_index = int(0.9 * (n_lengths - 1))
        p90_rollout = sorted_lengths[p90_index]
        max_rollout = sorted_lengths[-1]

        # Calculate mean rubric values
        all_rubric_keys = set()
        for r in valid_results:
            all_rubric_keys.update(r["rubric"].keys())

        mean_rubric = {}
        for key in all_rubric_keys:
            values = [r["rubric"].get(key, 0.0) for r in valid_results]
            mean_rubric[key] = sum(values) / len(values)

        # Calculate shaped reward (training rubric)
        # Count total achievements across all episodes
        achievement_counts = {}
        unique_achievements_per_trajectory = []
        all_unique_achievements = set()

        for result in valid_results:
            achievements = result.get("achievements", {})
            trajectory_achievements = set()
            for achievement, unlocked in achievements.items():
                if unlocked:
                    achievement_counts[achievement] = achievement_counts.get(achievement, 0) + 1
                    trajectory_achievements.add(achievement)
                    all_unique_achievements.add(achievement)
            unique_achievements_per_trajectory.append(trajectory_achievements)

        # Calculate shaped reward using the counts
        shaped_reward_data = calculate_shaped_reward(achievement_counts)

        # Calculate unique achievements by N trajectories
        unique_achievements_by_n = {}
        for n in range(1, len(valid_results) + 1):
            unique_at_n = set()
            for i in range(n):
                unique_at_n.update(unique_achievements_per_trajectory[i])
            unique_achievements_by_n[n] = len(unique_at_n)

        # Create training rubric (normalized shaped reward components)
        training_rubric = {}
        total_episodes = len(valid_results)
        if shaped_reward_data["breakdown"]:
            for achievement, data in shaped_reward_data["breakdown"].items():
                # Normalize by number of episodes for comparison
                training_rubric[achievement] = data["contribution"] / total_episodes

        # Get newly created trace files
        final_trace_files = set(traces_dir.glob("*.json"))
        new_trace_files = sorted(list(final_trace_files - initial_trace_files))
        
        return {
            "eval_metrics": eval_metrics,
            "mean_eval_metric": mean_eval_metric,
            "mean_rubric": mean_rubric,
            "achievement_counts": achievement_counts,
            "shaped_reward_data": shaped_reward_data,
            "training_rubric": training_rubric,
            "unique_achievements_per_trajectory": unique_achievements_per_trajectory,
            "all_unique_achievements": all_unique_achievements,
            "unique_achievements_by_n": unique_achievements_by_n,
            "num_episodes": len(valid_results),
            "q2_rollout": q2_rollout,
            "p90_rollout": p90_rollout,
            "max_rollout": max_rollout,
            "new_trace_files": new_trace_files,
            "all_achievements": dict(all_achievements),  # Add the tracked achievements
            "all_invalid_actions": dict(all_invalid_actions),  # Add the tracked invalid actions
            "all_total_actions": dict(all_total_actions),  # Add the total actions
            "termination_reasons": all_termination_reasons,  # Add termination reasons
        }


async def main():
    """Run Crafter evaluation."""
    # Record start time for trace filtering
    import time
    config._run_start_time = time.time()
    
    # Configure logging to reduce verbosity
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("google_genai").setLevel(logging.ERROR)
    logging.getLogger("google.generativeai").setLevel(logging.ERROR)
    logging.getLogger("google_genai.models").setLevel(logging.ERROR)
    logging.getLogger("google_genai.types").setLevel(logging.ERROR)
    logging.getLogger("google_genai._api_client").setLevel(logging.ERROR)
    
    # Disable synth_ai LM debug logs
    logging.getLogger("synth_ai.lm.provider_support.openai").setLevel(logging.WARNING)
    logging.getLogger("synth_ai.lm").setLevel(logging.WARNING)
    
    # Suppress worker/trajectory logs from other systems
    logging.getLogger("synth_ai").setLevel(logging.WARNING)
    logging.getLogger("synth_ai.environments").setLevel(logging.WARNING)
    logging.getLogger("asyncio").setLevel(logging.WARNING)
    
    # Suppress Langfuse client warnings
    logging.getLogger("langfuse").setLevel(logging.ERROR)
    logging.getLogger("langfuse.client").setLevel(logging.ERROR)
    
    # Set root logger to WARNING to suppress debug prints
    logging.getLogger().setLevel(logging.WARNING)

    print(f"üéÆ Custom Crafter ReAct Agent Evaluation")
    print(f"Model: {config.model_name}")
    print(f"Service: {config.service_base_url}")
    print(f"Instances: {config.num_instances}")
    print(f"Max Turns: {config.max_turns}")
    print(f"Difficulty: {config.difficulty}")
    print(f"World Config: {config.world_config}")
    if config.world_config_path:
        print(f"Custom Config: {config.world_config_path}")
    print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 50)

    # Test service health
    async with AsyncClient(base_url=config.service_base_url, timeout=10.0) as client:
        try:
            health_resp = await client.get("/health")
            health_data = health_resp.json()

            if "CrafterClassic" not in health_data.get("supported_environments", []):
                print("‚ùå CrafterClassic not available on service")
                return

            print("‚úÖ Service health check passed")

        except Exception as e:
            print(f"‚ùå Service health check failed: {e}")
            return

    # Run evaluation
    try:
        results = await evaluate_crafter_batch()

        print("\n" + "=" * 80)
        print("üèÜ CRAFTER EVALUATION RESULTS")
        print("=" * 80)

        # Calculate key metrics
        achievements_per_trajectory = [
            len(achievements)
            for achievements in results.get("unique_achievements_per_trajectory", [])
        ]
        avg_achievements = (
            sum(achievements_per_trajectory) / len(achievements_per_trajectory)
            if achievements_per_trajectory
            else 0.0
        )
        total_unique = len(results.get("all_unique_achievements", set()))
        shaped_reward = results.get("shaped_reward_data", {}).get("total_shaped_reward", 0.0)
        mean_k_score = (
            shaped_reward / results["num_episodes"] if results["num_episodes"] > 0 else 0.0
        )
        achievement_counts = results.get("achievement_counts", {})
        
        # Assessment
        if results["mean_eval_metric"] >= 3.0:
            assessment = "üéâ Excellent"
        elif results["mean_eval_metric"] >= 1.0:
            assessment = "‚úÖ Good"
        elif results["mean_eval_metric"] >= 0.5:
            assessment = "‚ö†Ô∏è  Moderate"
        else:
            assessment = "üìà Learning"

        # Create dense results table
        print(f"‚îÇ Metric                    ‚îÇ Value                                     ‚îÇ")
        print(f"‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§")
        print(f"‚îÇ Model                     ‚îÇ {config.model_name:<41} ‚îÇ")
        print(f"‚îÇ Episodes                  ‚îÇ {results['num_episodes']:<41} ‚îÇ")
        print(f"‚îÇ Mean Score                ‚îÇ {results['mean_eval_metric']:<41.2f} ‚îÇ")
        print(f"‚îÇ Assessment                ‚îÇ {assessment:<41} ‚îÇ")
        print(f"‚îÇ Avg Achievements/Episode  ‚îÇ {avg_achievements:<41.2f} ‚îÇ")
        print(f"‚îÇ Unique Achievements       ‚îÇ {total_unique:<41} ‚îÇ")
        print(f"‚îÇ Shaped Reward (Total)     ‚îÇ {shaped_reward:<41.3f} ‚îÇ")
        print(f"‚îÇ Mean K-Score/Episode      ‚îÇ {mean_k_score:<41.3f} ‚îÇ")
        print(f"‚îÇ Q2 Rollout Length         ‚îÇ {results.get('q2_rollout', 0):<41} ‚îÇ")
        print(f"‚îÇ Max Rollout Length        ‚îÇ {results.get('max_rollout', 0):<41} ‚îÇ")
        
        # Show unlocked achievements
        all_unique = results.get("all_unique_achievements", set())
        if all_unique:
            achievements_str = ', '.join(sorted(all_unique))
            if len(achievements_str) > 41:
                achievements_str = achievements_str[:38] + "..."
            print(f"‚îÇ Unlocked Achievements     ‚îÇ {achievements_str:<41} ‚îÇ")
        
        print(f"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò")
        
        # Print trace file sizes for newly created files only
        new_trace_files = results.get("new_trace_files", [])
        if new_trace_files:
            print("\nüìä Trace File Sizes:")
            total_size_mb = 0
            for trace_file in new_trace_files:
                size_bytes = trace_file.stat().st_size
                size_mb = size_bytes / (1024 * 1024)
                total_size_mb += size_mb
                print(f"  {trace_file.name}: {size_mb:.2f} MB")
            
            print(f"  Total: {total_size_mb:.2f} MB")
            print(f"  Average: {total_size_mb/len(new_trace_files):.2f} MB per trace")
            
            # Only analyze traces if explicitly requested (not by default)
            if config.analyze_traces and hasattr(config, 'verbose_analysis') and config.verbose_analysis:
                print("\nüîç Analyzing Traces...")
                for i, trace_file in enumerate(new_trace_files):
                    print(f"[DEBUG] Analyzing trace {i+1}/{len(new_trace_files)}")
                    analyze_trace_file(trace_file)
                print("[DEBUG] All trace analyses complete")
        
        # Display achievements table
        all_achievements = results.get("all_achievements", {})
        if all_achievements:
            print_achievements_table(all_achievements, results['num_episodes'])
        
        # Display invalid actions table
        invalid_actions = results.get("all_invalid_actions", {})
        total_actions = results.get("all_total_actions", {})
        if invalid_actions:
            print_invalid_actions_table(invalid_actions, total_actions)
        
        # Display termination breakdown
        termination_reasons = results.get("termination_reasons", [])
        if termination_reasons:
            print_termination_breakdown(termination_reasons)
        
    except Exception as e:
        print(f"‚ùå Evaluation failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Run Crafter ReAct Agent Evaluation")
    parser.add_argument("--config", "-c", type=str, help="Path to TOML configuration file")
    parser.add_argument("--model", "-m", type=str, help="Model name (overrides config)")
    parser.add_argument("--episodes", "-e", type=int, help="Number of episodes (overrides config)")
    parser.add_argument(
        "--max-turns", "-t", type=int, help="Maximum turns per episode (overrides config)"
    )
    parser.add_argument("--difficulty", "-d", type=str, help="Difficulty level (overrides config)")
    parser.add_argument("--analyze-traces", action="store_true", help="Analyze trace files after evaluation")
    parser.add_argument("--evaluate-traces", action="store_true", help="Run trace evaluation scoring after episodes")
    
    # World configuration arguments
    parser.add_argument("--world-config", "-w", type=str, help="World configuration: easy, normal, hard, peaceful (overrides config)")
    parser.add_argument("--world-config-path", type=str, help="Path to custom world configuration JSON file")

    # Custom OpenAI endpoint support
    parser.add_argument(
        "--openai-base-url",
        type=str,
        help="Custom OpenAI-compatible base URL (e.g., https://lora-service.modal.run)",
    )
    parser.add_argument(
        "--openai-api-key",
        type=str,
        default="dummy",
        help="API key for custom endpoint (default: 'dummy')",
    )

    args = parser.parse_args()

    # Load configuration
    if args.config:
        config = CrafterConfig(args.config)
    else:
        # Try to load default config
        default_config_path = (
            Path(__file__).parent.parent.parent.parent / "evals" / "configs" / "crafter.toml"
        )
        if default_config_path.exists():
            config = CrafterConfig(str(default_config_path))
        else:
            config = CrafterConfig()

    # Override with command line arguments
    if args.model:
        config.model_name = args.model
    if args.episodes:
        config.num_instances = args.episodes
    if args.max_turns:
        config.max_turns = args.max_turns
    if args.difficulty:
        config.difficulty = args.difficulty
    if args.analyze_traces:
        config.analyze_traces = True
    if args.evaluate_traces:
        config.evaluate_traces = True
    if args.world_config:
        config.world_config = args.world_config
    if args.world_config_path:
        config.world_config_path = args.world_config_path

    # Configure custom OpenAI endpoint if provided
    if args.openai_base_url:
        config.set_custom_endpoint(args.openai_base_url, args.openai_api_key)

    # Fail fast if model_name still missing
    if not config.model_name:
        raise ValueError(
            "CrafterConfig: 'model_name' must be specified in the TOML config or via --model CLI argument; no fallback default."
        )

    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nInterrupted by user")
    except Exception as e:
        print(f"‚ùå Error: {e}")
        sys.exit(1)
    
    # Run trace evaluation if requested
    if config.analyze_traces or hasattr(config, 'evaluate_traces') and config.evaluate_traces:
        print("\n" + "=" * 80)
        print("üìä TRACE EVALUATION")
        print("=" * 80)
        
        try:
            # Add current directory to Python path to import trace_eval
            import sys
            from pathlib import Path
            current_dir = Path(__file__).parent
            if str(current_dir) not in sys.path:
                sys.path.insert(0, str(current_dir))
            
            from trace_eval import evaluate_all_traces, print_evaluation_summary
            
            trace_dir = current_dir / "traces"
            if trace_dir.exists():
                # Find trace files created during this run
                recent_traces = []
                
                # Use run start time if available, otherwise fall back to last 60 seconds
                import time
                if hasattr(config, '_run_start_time'):
                    start_time = config._run_start_time
                else:
                    start_time = time.time() - 60  # Only traces from last minute
                
                for trace_file in trace_dir.glob("*.json"):
                    # Check if file was created after start time
                    if trace_file.stat().st_mtime >= start_time:
                        recent_traces.append(trace_file)
                
                if recent_traces:
                    print(f"Evaluating {len(recent_traces)} trace files from this run...")
                    results = []
                    for trace_file in recent_traces:
                        from trace_eval import evaluate_trace, print_trace_evaluation
                        result = evaluate_trace(trace_file)
                        results.append(result)
                    
                    # Sort by score
                    results.sort(key=lambda x: x['total_score'], reverse=True)
                    
                    # Show detailed evaluation only if not too many traces
                    if len(results) <= 5:
                        for result in results:
                            print_trace_evaluation(result)
                    
                    # Always show summary
                    print_evaluation_summary(results)
                    
                    # Also save to file for debugging
                    eval_file = current_dir / "last_evaluation.txt"
                    with open(eval_file, 'w') as f:
                        f.write(f"Evaluation of {len(results)} traces\n")
                        f.write("="*60 + "\n")
                        for result in results:
                            f.write(f"\nTrace: {result['trace_file']}\n")
                            f.write(f"Score: {result['total_score']:.2f}\n")
                            f.write(f"Trajectory: {result['trajectory']}\n")
                    print(f"\nüìù Evaluation saved to: {eval_file}")
                else:
                    print("No recent trace files found to evaluate.")
            else:
                print(f"Trace directory not found: {trace_dir}")
                
        except ImportError:
            print("‚ö†Ô∏è  trace_eval module not found. Skipping trace evaluation.")
        except Exception as e:
            print(f"‚ö†Ô∏è  Error during trace evaluation: {e}")
    
    # Normal exit (allow cleanup and final output)
    sys.exit(0)
