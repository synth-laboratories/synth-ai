[algorithm]
type = "online"
method = "policy_gradient"
variety = "gspo"

[services]
task_url = "https://YOUR-MODAL-TASK-APP.modal.run"

[compute]
gpu_type = "H200"
gpu_count = 2

[topology]
type = "single_node_split"
gpus_for_vllm = 1
gpus_for_training = 1
gpus_for_ref = 0
tensor_parallel = 1

[vllm]
tensor_parallel_size = 1
max_model_len = 4096

[reference]
placement = "none"

[model]
base = "Qwen/Qwen3-VL-4B-Instruct"
trainer_mode = "lora"
label = "crafter-rl-vision-qwen3vl4b"
supports_vision = true

[lora]
r = 16
alpha = 32
dropout = 0.05
target_modules = [ "all-linear",]

[rollout]
env_name = "crafter"
max_turns = 10
episodes_per_batch = 2
policy_name = "crafter-react"
max_concurrent_rollouts = 4
batches_per_step = 2
ops = [ "agent", "env",]

[evaluation]
instances = 8
every_n_iters = 5
seeds = [ 0, 1, 2, 3, 4, 5, 6, 7,]

[training]
num_epochs = 1
iterations_per_epoch = 3
gradient_accumulation_steps = 2
max_accumulated_minibatch = 1
max_turns = 10
batch_size = 2
group_size = 2
learning_rate = 5e-5
log_interval = 1
weight_sync_interval = 1
event_rewards_kind = "unique"
async_semaphore_max = 2
step_rewards_enabled = true
step_rewards_mode = "decision_stepwise"
step_rewards_indicator_lambda = 1.0
step_rewards_beta = 0.0
step_rewards_strategy = "consistent"
max_images_per_message = 1
supports_vision = true

[tags]
experiment = "crafter_rl_vision_qwen3vl4b"
task = "crafter_agent_vision"
model_size = "4b"
vision_enabled = true
image_only = true

[vllm.limit_mm_per_prompt]
image = 1

[rollout.env_config]
difficulty = "easy"

[rollout.policy_config]
use_vision = true
image_only_mode = true
temperature = 0.6
top_p = 0.95
max_tokens = 512
max_llm_calls = 10

[training.weight_sync]
enable = true
targets = [ "policy",]
mode = "direct"
direct = true
verify_every_k = 0

[judge.options]
timeout_s = 30

[rollout.env_config.step_rewards]
enabled = true
mode = "decision_stepwise"
strategy = "consistent"
indicator_lambda = 1.0
step_beta = 0.0
