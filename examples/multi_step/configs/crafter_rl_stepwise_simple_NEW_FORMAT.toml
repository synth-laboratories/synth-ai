# Crafter RL experiment â€“ simple stepwise rewards (1 point per new achievement)
# This config uses the NEW unified [policy] section format

[algorithm]
type = "online"
method = "policy_gradient"
variety = "gspo"

[services]
# Replace with the Modal URL printed by `uvx synth-ai deploy --runtime modal --modal-mode serve grpo-crafter`
task_url = "https://YOUR-MODAL-TASK-APP.modal.run"

[compute]
gpu_type = "H200"
gpu_count = 2

  [compute.topology]  # Nested: topology is part of compute
  type = "single_node_split"
  gpus_for_vllm = 1
  gpus_for_training = 1
  gpus_for_ref = 0
  tensor_parallel = 1
  reference_placement = "none"  # Reference model placement

[vllm]
tensor_parallel_size = 1
max_model_len = 8192

[judge]
enabled = false  # Set to true to enable judge/rubric scoring

# Uncomment to enable judge-based reward blending:
# enabled = true
# timeout_s = 45
#
#  [judge.reward_blend]  # How to blend env/event/outcome reward sources
#  env = 0.2
#  event = 0.4
#  outcome = 0.4
#
#  [judge.options]
#  provider = "openai"
#  model = "openai/gpt-oss-120b"
#  event = true
#  outcome = true
#  max_concurrency = 6

# NEW: Unified [policy] section - single source of truth for model and sampling
[policy]
model_name = "Qwen/Qwen3-4B"
trainer_mode = "lora"
label = "crafter-rl-stepwise-simple"

# Sampling parameters for rollouts
max_tokens = 512
temperature = 0.6
top_p = 0.95

[rollout]
env_name = "crafter"
max_turns = 10
episodes_per_batch = 4
policy_name = "crafter-react"
max_concurrent_rollouts = 8
batches_per_step = 2
ops = ["agent", "env"]

[evaluation]
instances = 10
every_n_iters = 10
seeds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

[training]
num_epochs = 1
iterations_per_epoch = 10
gradient_accumulation_steps = 1
max_accumulated_minibatch = 1
max_turns = 10
batch_size = 4
group_size = 4
learning_rate = 5e-5
log_interval = 1
weight_sync_interval = 1

  [training.rewards]  # Nested: Reward config under training
  step_rewards_enabled = true
  step_rewards_mode = "decision_stepwise"
  step_rewards_indicator_lambda = 1.0
  step_rewards_beta = 0.0
  step_rewards_strategy = "consistent"
  event_rewards_kind = "unique"

  [training.lora]  # Nested: LoRA config under training
  r = 16
  alpha = 32
  dropout = 0.05
  target_modules = ["all-linear"]

  [training.weight_sync]
  enable = true
  targets = ["policy"]
  mode = "direct"
  direct = true
  verify_every_k = 0

