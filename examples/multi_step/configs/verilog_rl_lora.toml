# Verilog RL experiment – LoRA training on Qwen3-0.6B
#
# This configuration adapts the Crafter RL setup for Verilog spec-to-RTL tasks.
# Uses the same proven pipeline but optimized for 0.6B model and Verilog domain.

[algorithm]
type = "online"
method = "policy_gradient"
variety = "gspo"

[services]
# Replace with the Modal URL printed by `uvx synth-ai modal-serve grpo-verilog`
task_url = "https://synth-laboratories--grpo-verilog-task-app-fastapi-app-dev.modal.run"
# Point at the Synth backend (or compatible service) that exposes /api/judge/v1/*
judge_url = "https://synth-backend-dev-docker.onrender.com/api"

[compute]
gpu_type = "H200"  # ✅ 8B model needs H200 for larger context window
gpu_count = 2      # ✅ Minimum 2x GPUs (1 for vLLM inference + 1 for training)
nodes = 1

[topology]
type = "single_node_split"
gpus_for_vllm = 1      # ✅ vLLM for inference
gpus_for_training = 1  # ✅ Training GPU (8B LoRA fits well)
gpus_for_ref = 0
tensor_parallel = 1

[vllm]
tensor_parallel_size = 1
max_model_len = 24576  # ✅ Increased to 24K to accommodate long Verilog prompts (16K + 8K buffer for testbenches + history)

[reference]
placement = "none"

[model]
base = "Qwen/Qwen3-8B"  # ✅ 8B model for RL training with good balance of speed and capability
trainer_mode = "lora"
label = "verilog-rl-lora-qwen8b"

[lora]
r = 16
alpha = 32
dropout = 0.05
target_modules = ["all-linear"]

[rollout]
env_name = "verilog"  # ✅ Changed from "crafter" to "verilog"
max_turns = 6        # ✅ More steps for compilation chains vs Crafter's 10
episodes_per_batch = 4  # ✅ Good batch size for 8B model
policy_name = "verilog-designer"
max_concurrent_rollouts = 8
batches_per_step = 2
ops = ["agent", "env"]

  [rollout.env_config]
  # Verilog-specific environment settings
  difficulty = "medium"  # Can be "easy", "medium", or "hard"

    [rollout.env_config.step_rewards]
    enabled = true
    mode = "decision_stepwise"
    strategy = "consistent"
    indicator_lambda = 0.5  # ✅ Reduced from Crafter (sparser rewards)
    step_beta = 0.0

  [rollout.policy_config]
  provider = "openai"
  model = "Qwen/Qwen3-8B"  # ✅ Use the model being trained (8B) for rollouts
  temperature = 0.2
  max_tokens = 4096  # ✅ Balanced for Verilog generation while leaving room for long input prompts (testbenches + history)

[evaluation]
instances = 16
every_n_iters = 10
seeds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]

[training]
num_epochs = 1
iterations_per_epoch = 5
gradient_accumulation_steps = 1
max_accumulated_minibatch = 1
max_turns = 15
batch_size = 4          # ✅ Same as Crafter (works well for 8B LoRA)
group_size = 4
learning_rate = 5e-5    # ✅ Same as Crafter
log_interval = 1
weight_sync_interval = 1
event_rewards_kind = "unique"
async_semaphore_max = 20  # Max concurrent rollouts in streaming pipeline

# Enable dense decision rewards in the trainer
step_rewards_enabled = true
step_rewards_mode = "decision_stepwise"
step_rewards_indicator_lambda = 0.5  # ✅ Reduced for Verilog's sparser rewards
step_rewards_beta = 0.0
step_rewards_strategy = "consistent"

[training.weight_sync]
enable = true
targets = ["policy"]
mode = "direct"
direct = true
verify_every_k = 0

[rubric]
enabled = true
model = "openai/gpt-oss-120b"
api_base = "https://synth-backend-dev-docker.onrender.com/api/judge"
api_key_env = "OPENAI_API_KEY"

# Blend the hosted judge scores with environment returns
[rubric.weights]
env = 0.3     # ✅ Higher weight on env rewards for Verilog (vs Crafter's 0.2)
event = 0.3   # ✅ Adjusted for Verilog's different reward structure
outcome = 0.4

[rubric.event]
# Verilog-specific event rubric for process efficiency
rubric_id = "verilog/event@v1"
criteria = [
  { key = "process.compilation_success", weight = 0.7, description = "Return 1.0 when compilation succeeds, 0.5 for partial success, 0.0 for failure", aggregation = "weighted_sum" },
  { key = "process.design_iterations", weight = 0.3, description = "Reward efficient design iterations without unnecessary recompilation", aggregation = "weighted_sum" },
]

[rubric.outcome]
# Verilog-specific outcome rubric for final results
rubric_id = "verilog/outcome@v1"
criteria = [
  { key = "outcome.tests_passed", weight = 0.8, description = "Full credit when all tests pass, partial for some tests", aggregation = "weighted_sum" },
  { key = "outcome.design_quality", weight = 0.2, description = "Code quality, documentation, and design efficiency", aggregation = "weighted_sum" },
]

[judge]
type = "groq"
timeout_s = 45

  [judge.options]
  event = true
  outcome = true
  provider = "openai"
  model = "openai/gpt-oss-120b"
  rubric_id = "verilog/bundle@v1"
  max_concurrency = 6
  tracks = ["process", "reasoning", "progress", "outcome"]

    [judge.options.rubric_overrides]

      [judge.options.rubric_overrides.event]
      goal_text = """
      Evaluate each Verilog design decision for compilation success and process efficiency.
      High scores for successful compilation and strategic tool usage.
      Penalize unnecessary operations and compilation failures."""
      aggregation = "weighted_sum"

        [[judge.options.rubric_overrides.event.criteria]]
        id = "process.compilation_success"
        weight = 0.7
        scale = "bounded"
        description = "Return 1.0 when compilation succeeds cleanly, 0.5 for warnings, 0.0 for errors"

        [[judge.options.rubric_overrides.event.criteria]]
        id = "process.design_iterations"
        weight = 0.3
        scale = "bounded"
        description = "Reward efficient write→compile→simulate workflow, penalize redundant operations"

      [judge.options.rubric_overrides.outcome]
      goal_text = """
      Evaluate the final Verilog implementation for correctness and quality.
      High scores for working designs that pass all tests with good code quality."""
      aggregation = "weighted_sum"

        [[judge.options.rubric_overrides.outcome.criteria]]
        id = "outcome.tests_passed"
        weight = 0.8
        scale = "binary"
        description = "Full credit when all tests pass, partial credit for some tests passing"

        [[judge.options.rubric_overrides.outcome.criteria]]
        id = "outcome.design_quality"
        weight = 0.2
        scale = "bounded"
        description = "Code clarity, proper documentation, and efficient design patterns"

    [judge.options.weights]
    process = 0.1
    reasoning = 0.2
    progress = 0.3
    outcome = 0.4
