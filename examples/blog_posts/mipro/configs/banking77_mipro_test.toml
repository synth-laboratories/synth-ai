# MIPROv2 Prompt Learning for Banking77
# Test configuration with reduced iterations for faster testing

[prompt_learning]
algorithm = "mipro"
task_app_url = "https://synth-laboratories-dev--synth-banking77-web-web.modal.run"
task_app_id = "banking77"

# Initial prompt pattern (pattern-based mode)
[prompt_learning.initial_prompt]
id = "banking77_pattern"
name = "Banking77 Classification Pattern"

[[prompt_learning.initial_prompt.messages]]
role = "system"
pattern = "You are an expert banking assistant that classifies customer queries into banking intents. Given a customer message, respond with exactly one intent label from the provided list using the `banking77_classify` tool."
order = 0

[[prompt_learning.initial_prompt.messages]]
role = "user"
pattern = "Customer Query: {query}\n\nClassify this query into one of the banking intents using the tool call."
order = 1

[prompt_learning.initial_prompt.wildcards]
query = "REQUIRED"

# Policy configuration
[prompt_learning.policy]
inference_mode = "synth_hosted"
model = "openai/gpt-oss-20b"
provider = "groq"
inference_url = "https://api.groq.com/openai/v1"
temperature = 0.0
max_completion_tokens = 128
policy_name = "banking77-mipro"

# Training split config
[prompt_learning.env_config]
pool = "train"

# MIPROv2-specific configuration (reduced for testing)
[prompt_learning.mipro]
env_name = "banking77"
num_iterations = 10  # Reduced for testing (default: 16)
num_evaluations_per_iteration = 4  # Reduced for testing (default: 6)
batch_size = 4
max_concurrent = 10
meta_model = "gpt-4o-mini"
meta_model_provider = "openai"
meta_model_inference_url = "https://api.openai.com/v1"
few_shot_score_threshold = 0.80  # Lower threshold for testing (default: 0.85)

# Seed pools for different phases
bootstrap_train_seeds = [0, 1, 2, 3, 4]  # 5 seeds for bootstrap phase
online_pool = [5, 6, 7, 8, 9]  # 5 seeds for online evaluation
test_pool = [10, 11, 12, 13, 14]  # 5 seeds for final test

# TPE hyperparameters
[prompt_learning.mipro.tpe]
gamma = 0.25              # Quantile for splitting good/bad observations
n_candidates = 16         # Number of candidates to evaluate Expected Improvement (reduced for testing)
n_startup_trials = 5      # Uniform random trials before TPE kicks in
epsilon = 0.1             # Exploration probability (uniform sample)
categorical_alpha = 1.0   # Laplace smoothing for categorical variables
prior_weight = 0.05       # Prior mixing weight to avoid zero-densities

# Demo set selection hyperparameters
[prompt_learning.mipro.demo]
max_few_shot_examples = 5  # Maximum number of few-shot examples to include
sets_per_size = 3          # Number of demo sets to generate per size (reduced for testing)
include_empty = true       # Whether to include empty demo set (zero-shot)

# Grounding configuration for instruction proposals
[prompt_learning.mipro.grounding]
n = 8                      # Number of instruction proposals to generate (reduced for testing)
temperature = 0.7          # Temperature for LLM proposal generation
max_tokens = 600           # Max tokens for proposal generation

# Meta-update configuration
[prompt_learning.mipro.meta_update]
enabled = true             # Enable periodic meta-updates
every_iterations = 3       # Regenerate instructions every N iterations
topk_success = 3           # Number of top success examples to use for grounding (reduced for testing)
topk_failure = 3           # Number of top failure examples to use for grounding (reduced for testing)
validate_on_batch = 16     # Batch size for proposal validation (reduced for testing)
keep_k = 8                 # Maximum number of instruction variants to keep (reduced for testing)
dedup_token_overlap = 0.8  # Token overlap threshold for deduplication
regen_temperature_decay = 0.95  # Temperature decay factor per meta-update

