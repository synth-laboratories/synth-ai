# # from dev.testing.hendryks import HendryksMathBenchmark, TrivialHendryksMathAgent
# import asyncio
# import json
# import logging
# import os
# import re
# import sys
# import time
# import uuid
# from typing import List, Dict
# from pydantic import BaseModel
# import pytest
# from dotenv import load_dotenv
# from synth_sdk.tracing.abstractions import (
#     Dataset,
#     RewardSignal,
#     TrainingQuestion,
# )
# from synth_sdk.tracing.client_manager import ClientManager
# from synth_sdk.tracing.decorators import get_tracing_config, trace_system_async
# from synth_sdk.tracing.upload import upload
# from synth_sdk.tracing.utils import get_system_id

# from datasets import load_dataset
# from zyk import LM


# class HendryksMathBenchmark:
#     def __init__(self):
#         self.name = "hendryks_math"
#         self.temp_dir = "temp"
#         os.makedirs(self.temp_dir, exist_ok=True)
#         os.makedirs("datasets/competition_math", exist_ok=True)

#     def load_data(self):
#         cache_path = "datasets/competition_math/dataset.json"

#         # Try to load from cache first
#         if os.path.exists(cache_path):
#             with open(cache_path, "r") as f:
#                 dataset = json.load(f)
#                 problems = []
#                 for item in dataset["train"]:  # Using train split for consistency
#                     problem = {
#                         "question": item["problem"],
#                         "answer": item["solution"],
#                         "subject": item.get("type", "unknown"),
#                         "level": "competition",  # All problems are competition level
#                     }
#                     problems.append(problem)
#                 return problems

#         # If not cached, load from HF and cache
#         dataset = load_dataset("competition_math", "main")
#         with open(cache_path, "w") as f:
#             json.dump(
#                 {"train": list(dataset["train"]), "test": list(dataset["test"])}, f
#             )

#         # Convert to our format
#         problems = []
#         for item in dataset["train"]:
#             problem = {
#                 "question": item["problem"],
#                 "answer": item["solution"],
#                 "subject": item.get("type", "unknown"),
#                 "level": "competition",
#             }
#             problems.append(problem)

#         return problems

#     def get_problems(self):
#         temp_path = os.path.join(self.temp_dir, "hendryks_math.json")

#         # Load from temp file if it exists
#         if os.path.exists(temp_path):
#             with open(temp_path, "r") as f:
#                 return json.load(f)

#         # Otherwise load from dataset and save
#         problems = self.load_data()
#         with open(temp_path, "w") as f:
#             json.dump(problems, f)
#         return problems

#     def score_answer(self, question: str, proposed_answer: str) -> bool:
#         """Score a proposed answer against the correct answer for a given question."""
#         # Find the problem that matches the question
#         problems = self.get_problems()
#         matching_problem = next(
#             (p for p in problems if p["question"] == question), None
#         )

#         if not matching_problem:
#             raise ValueError("Question not found in benchmark")

#         # Extract answer from proposed solution's \boxed{} format
#         proposed_match = re.search(r"\\boxed{((?:[^{}]|{[^{}]*})*)}", proposed_answer)
#         if not proposed_match:
#             return False

#         # Extract answer from correct solution's \boxed{} format
#         correct_match = re.search(
#             r"\\boxed{((?:[^{}]|{[^{}]*})*)}", matching_problem["answer"]
#         )
#         if not correct_match:
#             return False

#         extracted_proposed = proposed_match.group(1).strip()
#         extracted_correct = correct_match.group(1).strip()

#         # print(f"Proposed answer: {extracted_proposed}")
#         # print(f"Correct answer: {extracted_correct}")

#         return extracted_proposed == extracted_correct


# class TrivialHendryksMathAgent:
#     def __init__(self):
#         self.lm = LM(  # gemini-1.5-flash
#             model_name="gpt-4o-mini",
#             formatting_model_name="gpt-4o-mini",
#             temperature=0.1,
#             synth_logging=True,
#         )
#         self.system_name = "HendryksMathAgent"
#         self.system_id = get_system_id(self.system_name)
#         self.system_instance_id = str(uuid.uuid4())

#     @trace_system_async(
#         origin="agent",
#         event_type="plan",
#         manage_event="create_and_end",
#         increment_partition=True,
#         verbose=True,
#     )
#     async def plan(self, math_question: str) -> str:
#         logger.debug("Starting plan method with trace decorator")
#         try:
#             class Plan(BaseModel):
#                 content: str
#             response = await self.lm.respond_async(
#                 system_message="""You are an AI assisting a colleague in completing a mathematics problem.
# You will be given a mathematics problem statement. Your task is to create a detailed plan to solve the problem, 
# breaking it down into clear, logical steps.""",
#                 user_message=f"""Please provide a detailed, step-by-step plan to solve this math problem:
# {math_question}

# Your plan should include:
# 1. A clear statement of the given information and problem to be solved
# 2. Identification of relevant mathematical concepts and techniques
# 3. Definition of variables and known relationships
# 4. A step-by-step approach to solving the problem
# 5. Explanation of the reasoning behind each step""",
#                 response_model=Plan
#             )
#             logger.debug("Successfully got response from LM in plan method")
#             return response.content
#         except Exception as e:
#             logger.error(f"Error in plan method: {str(e)}", exc_info=True)
#             raise

#     @trace_system_async(
#         origin="agent",
#         event_type="execute",
#         manage_event="create_and_end",
#         increment_partition=True,
#         verbose=True,
#     )
#     async def execute(self, plan: str) -> str:
#         logger.debug("Starting execute method with trace decorator")
#         try:
#             class Solution(BaseModel):
#                 content: str
#             response = await self.lm.respond_async(
#                 system_message="""You are an AI mathematical problem-solving assistant.
# You will be given a solution plan. Your task is to implement this plan,
# showing all work and verifying correctness at each step.""",
#                 user_message=f"""
# Plan:
# {plan}

# Please solve this problem by carefully following the provided plan. Show all your work and calculations.
# Leave your final answer at the very end in the format \\boxed{{answer}}.""",
#                 response_model=Solution,
#             )
#             logger.debug("Successfully got response from LM in execute method")
#             return response.content
#         except Exception as e:
#             logger.error(f"Error in execute method: {str(e)}", exc_info=True)
#             raise

#     async def run(self, math_question: str) -> str:
#         logger.debug("Starting run method")
#         plan = await self.plan(math_question)
#         logger.debug("Completed plan method")
#         solution = await self.execute(plan)
#         logger.debug("Completed execute method")
#         return solution


# # Configure logging
# logging.basicConfig(
#     level=logging.ERROR, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
# )
# logger = logging.getLogger(__name__)

# # Add logging for trace decorator
# trace_logger = logging.getLogger("synth_sdk.tracing.decorators")
# trace_logger.setLevel(logging.ERROR)

# # Add logging for client manager
# client_logger = logging.getLogger("synth_sdk.tracing.client_manager")
# client_logger.setLevel(logging.ERROR)

# load_dotenv()


# async def setup_synth_config():
#     """Setup synth configuration for deferred logging."""
#     logger.info("Setting up synth configuration for deferred logging")
#     os.environ["SYNTH_LOGGING_MODE"] = "deferred"
#     os.environ["SYNTH_ENDPOINT_OVERRIDE"] = "https://agent-learning.onrender.com"
#     config = get_tracing_config()
#     ClientManager.initialize(config)
#     logger.info("Synth config:")
#     logger.info(f"  Mode: {config.mode}")
#     logger.info(f"  API Key present: {bool(config.api_key)}")
#     logger.info(f"  Base URL: {config.base_url}")


# @pytest.mark.asyncio
# async def test_deferred_logging():
#     """Test deferred logging with both pytest and regular assertions."""
#     logger.info("=== STARTING DEFERRED LOGGING TEST ===")
#     start_time = time.time()
#     logger.info(f"Test start time: {start_time}")

#     # Determine if running under pytest
#     is_pytest = "pytest" in sys.modules

#     try:
#         await setup_synth_config()

#         # Initialize and run agent
#         benchmark = HendryksMathBenchmark()
#         agent = TrivialHendryksMathAgent()
#         logger.info(f"Agent system ID: {agent.system_id}")
#         logger.info(f"Agent system instance ID: {agent.system_instance_id}")

#         problems = benchmark.get_problems()
#         test_problem = problems[0]["question"]
#         logger.info(f"Using test problem: {test_problem}")

#         # Run the agent
#         logger.info("Running agent...")
#         solution = await agent.run(test_problem)
#         logger.info(f"Agent solution: {solution}")

#         # Create dataset and upload results
#         logger.info("Creating dataset and uploading results...")
#         dataset = Dataset(
#             questions=[
#                 TrainingQuestion(
#                     id="q0",
#                     intent="Test math problem",
#                     criteria="Testing deferred tracing and upload functionality",
#                 )
#             ],
#             reward_signals=[
#                 RewardSignal(
#                     question_id="q0",
#                     system_instance_id=agent.system_instance_id,
#                     reward=1.0,
#                     annotation="Test reward",
#                 )
#             ],
#         )

#         # Upload the dataset and traces
#         logger.info("Starting upload process...")
#         upload_id, questions_json, reward_signals_json, traces_json = upload(
#             dataset=dataset
#         )

#         logger.info(f"Upload completed with ID: {upload_id}")
#         logger.debug(f"Number of traces: {len(traces_json)}")
#         print(traces_json)

#         # Verify upload results
#         if is_pytest:
#             assert upload_id
#             assert questions_json
#             assert reward_signals_json
#             assert traces_json
#         else:
#             assert upload_id, "Upload ID should not be empty"
#             assert questions_json, "Questions JSON should not be empty"
#             assert reward_signals_json, "Reward signals JSON should not be empty"
#             assert traces_json, "Traces JSON should not be empty"

#         # Verify trace content
#         for i, trace in enumerate(traces_json):
#             logger.debug(f"Verifying trace {i}:")
#             verify_trace_content(trace, is_pytest)

#         logger.info("All traces verified successfully!")
#         return True

#     except AssertionError as e:
#         logger.error(f"Test failed: {str(e)}")
#         if is_pytest:
#             raise
#         return False
#     except Exception as e:
#         logger.error(f"Unexpected error: {str(e)}", exc_info=True)
#         if is_pytest:
#             raise
#         return False


# def verify_trace_content(trace: dict, is_pytest: bool = False) -> None:
#     """Verify the content of a trace."""
#     if is_pytest:
#         assert trace["system_instance_id"]
#     else:
#         assert trace["system_instance_id"], "Trace missing system_instance_id"

#     # Verify events were captured
#     has_events = False
#     for partition in trace["partition"]:
#         if len(partition["events"]) > 0:
#             has_events = True
#             for event in partition["events"]:
#                 logger.debug(f"Checking event: {json.dumps(event, indent=2)}")
#                 if "agent_compute_step" in event:
#                     step = event["agent_compute_step"]
#                     logger.debug(f"Checking compute step: {json.dumps(step, indent=2)}")
#                     if is_pytest:
#                         assert step.get("model_name") is not None
#                         assert step.get("model_name") != ""
#                     else:
#                         assert (
#                             step.get("model_name") is not None
#                         ), "Model name is missing"
#                         assert step.get("model_name") != "", "Model name is empty"

#                     if step.get("compute_input"):
#                         for input_item in step["compute_input"]:
#                             if is_pytest:
#                                 assert "messages" in input_item, input_item.keys()
#                             else:
#                                 assert "messages" in input_item, (
#                                     f"Input must have 'messages' key, but found keys: {list(input_item.keys())}"
#                                     f"\nFull input: {json.dumps(input_item, indent=2)}"
#                                 )
#                             messages = input_item["messages"]
#                             if is_pytest:
#                                 assert isinstance(messages, list)
#                                 assert len(messages) == 2
#                             else:
#                                 assert isinstance(
#                                     messages, list
#                                 ), "Messages must be a list"
#                                 assert len(messages) == 2, (
#                                     f"Expected exactly 2 messages (system and user), but found {len(messages)}"
#                                     f"\nMessages: {json.dumps(messages, indent=2)}"
#                                 )
#             break

#     if is_pytest:
#         assert has_events
#     else:
#         assert (
#             has_events
#         ), f"At least one partition should contain events - {trace['partition']}"


# if __name__ == "__main__":
#     # Remove the pytest check so the test always runs
#     success = asyncio.run(test_deferred_logging())
#     print("âœ… All tests passed!" if success else "âŒ Tests failed!")
#     exit(0 if success else 1)
