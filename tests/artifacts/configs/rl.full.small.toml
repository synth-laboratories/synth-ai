# Minimal RL config for full-parameter smoke tests (no LoRA)

[algorithm]
type = "online"
method = "policy_gradient"
variety = "gspo"

[model]
base = "Qwen/Qwen3-1.7B"
trainer_mode = "full"

[compute]
gpu_type = "H100"
gpu_count = 2

[topology]
type = "single_node_split"
gpus_for_vllm = 1
gpus_for_training = 1
gpus_for_ref = 0
tensor_parallel = 1

[vllm]
tensor_parallel_size = 1
max_model_len = 4096

[reference]
placement = "none"

[rollout]
env_name = "crafter"
policy_name = "crafter-react"
max_turns = 10
episodes_per_batch = 16
max_concurrent_rollouts = 4
batches_per_step = 2
ops = [
  "agent", "env",
  "agent", "env",
  "agent", "env",
  "agent", "env",
  "agent", "env",
  "agent", "env",
  "agent", "env",
  "agent", "env",
  "agent", "env",
  "agent", "env",
]

[rollout.policy_config]
max_llm_calls = 10
max_tokens = 512
max_completion_tokens = 512
temperature = 0.2
top_p = 0.95

[training]
num_epochs = 1
gradient_accumulation_steps = 1
max_accumulated_minibatch = 1
max_turns = 10
batch_size = 4
group_size = 4
learning_rate = 5e-5
log_interval = 1
weight_sync_interval = 1
iterations_per_epoch = 10
weight_sync_verify_checksums = false

[training.weight_sync]
enable = true
targets = ["policy"]
mode = "full"
direct = true
verify_every_k = 0
chunk_bytes = 0

[evaluation]
instances = 2
every_n_iters = 1
seeds = [0, 1]

[tags]
experiment = "ci_rl_full_small"

