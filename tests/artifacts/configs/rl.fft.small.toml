# Minimal RL config for FFT smoke tests

[algorithm]
type = "online"
method = "policy_gradient"
variety = "gspo"

[model]
base = "Qwen/Qwen3-1.7B"
trainer_mode = "full"

[compute]
gpu_type = "H100"
gpu_count = 2

[topology]
type = "single_node_split"
gpus_for_vllm = 1
gpus_for_training = 1
gpus_for_ref = 0
tensor_parallel = 1

[vllm]
tensor_parallel_size = 1
max_model_len = 4096

[reference]
placement = "none"

[rollout]
env_name = "math"
policy_name = "math"
max_turns = 1
episodes_per_batch = 2
max_concurrent_rollouts = 2
batches_per_step = 1
task_app_origin_rewards_only = true

[training]
num_epochs = 1
gradient_accumulation_steps = 1
max_accumulated_minibatch = 1
max_turns = 1
batch_size = 1
group_size = 2
learning_rate = 5e-6
log_interval = 1
weight_sync_interval = 1
iterations_per_epoch = 1
weight_sync_verify_checksums = false

[training.weight_sync]
enable = true
targets = ["policy"]
mode = "full"
direct = true
verify_every_k = 0
chunk_bytes = 0

[evaluation]
instances = 2
every_n_iters = 1
seeds = [0, 1]

[tags]
experiment = "ci_rl_fft_small"


